"""
ML Pipeline DAG - End-to-End Data Lakehouse ML Workflow
========================================================

ì´ DAG(Directed Acyclic Graph)ë°©í–¥ì„± ë¹„ìˆœí™˜ ê·¸ë˜í”„ ëŠ” ë‹¤ìŒ ë‹¨ê³„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤:
1. RAW â†’ Bronze: ì›ì‹œ ë°ì´í„° ìˆ˜ì§‘
2. Bronze â†’ Silver: ë°ì´í„° ì •ì œ ë° ë³€í™˜
3. Silver â†’ Gold: ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì ìš©
4. Feature Engineering: ML í”¼ì²˜ ìƒì„±
5. Model Training: MLflow ê¸°ë°˜ ëª¨ë¸ í•™ìŠµ
6. Model Evaluation: ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
7. Model Registry: MLflowì— ëª¨ë¸ ë“±ë¡

ì—°ë™:
- Trino: ë°ì´í„° ì¿¼ë¦¬
- MLflow: ì‹¤í—˜ ì¶”ì  ë° ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬
- SeaweedFS S3: ë°ì´í„° ì €ì¥ì†Œ
"""

from datetime import datetime, timedelta
from airflow import DAG
try:
    from airflow.providers.standard.operators.python import PythonOperator
except ModuleNotFoundError:
    from airflow.operators.python import PythonOperator
import os

# MLflow ì„¤ì •
MLFLOW_TRACKING_URI = os.getenv('MLFLOW_TRACKING_URI', 'http://mlflow:5000')

# ê¸°ë³¸ DAG ì¸ì
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=1),
}

# DAG ì •ì˜
dag = DAG(
    'ml_pipeline_end_to_end',
    default_args=default_args,
    description='End-to-End ML Pipeline with MLflow',
    schedule=timedelta(days=1),
    start_date=datetime(2025, 12, 25),
    catchup=False,
    tags=['ml', 'mlflow', 'lakehouse'],
)


# ============================================================================
# Task 1: RAW â†’ Bronze (ë°ì´í„° ìˆ˜ì§‘)
# ============================================================================
def raw_to_bronze(**context):
    """
    ì›ì‹œ ë°ì´í„°ë¥¼ Bronze í…Œì´ë¸”ì— ìˆ˜ì§‘
    """
    import mlflow

    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

    with mlflow.start_run(run_name="raw_to_bronze"):
        mlflow.log_param("layer", "bronze")
        mlflow.log_param("source", "raw_data")

        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì—¬ê¸°ì„œ Spark/Trino ì‘ì—… ì‹¤í–‰
        print("ğŸ“¥ RAW â†’ Bronze: ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")

        # ì˜ˆì‹œ ë©”íŠ¸ë¦­
        rows_ingested = 10000
        mlflow.log_metric("rows_ingested", rows_ingested)

        print(f"âœ… Bronze ë ˆì´ì–´ì— {rows_ingested}ê°œ í–‰ ìˆ˜ì§‘ ì™„ë£Œ")

        return {"rows_ingested": rows_ingested}


task_raw_to_bronze = PythonOperator(
    task_id='raw_to_bronze',
    python_callable=raw_to_bronze,
    dag=dag,
)


# ============================================================================
# Task 2: Bronze â†’ Silver (ë°ì´í„° ì •ì œ)
# ============================================================================
def bronze_to_silver(**context):
    """
    Bronze ë°ì´í„° ì •ì œ ë° Silver í…Œì´ë¸” ìƒì„±
    """
    import mlflow

    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

    with mlflow.start_run(run_name="bronze_to_silver"):
        mlflow.log_param("layer", "silver")
        mlflow.log_param("transformation", "cleansing")

        print("ğŸ§¹ Bronze â†’ Silver: ë°ì´í„° ì •ì œ ì¤‘...")

        # ì˜ˆì‹œ ë©”íŠ¸ë¦­
        rows_cleaned = 9500
        rows_filtered = 500
        quality_score = 0.95

        mlflow.log_metric("rows_cleaned", rows_cleaned)
        mlflow.log_metric("rows_filtered", rows_filtered)
        mlflow.log_metric("quality_score", quality_score)

        print(f"âœ… Silver ë ˆì´ì–´ì— {rows_cleaned}ê°œ ì •ì œëœ í–‰ ìƒì„± (í’ˆì§ˆ ì ìˆ˜: {quality_score})")

        return {"rows_cleaned": rows_cleaned, "quality_score": quality_score}


task_bronze_to_silver = PythonOperator(
    task_id='bronze_to_silver',
    python_callable=bronze_to_silver,
    dag=dag,
)


# ============================================================================
# Task 3: Silver â†’ Gold (ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§)
# ============================================================================
def silver_to_gold(**context):
    """
    Silver ë°ì´í„°ì— ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì ìš©í•˜ì—¬ Gold í…Œì´ë¸” ìƒì„±
    """
    import mlflow

    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

    with mlflow.start_run(run_name="silver_to_gold"):
        mlflow.log_param("layer", "gold")
        mlflow.log_param("aggregation", "daily")

        print("ğŸ’ Silver â†’ Gold: ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì ìš© ì¤‘...")

        # ì˜ˆì‹œ ë©”íŠ¸ë¦­
        rows_aggregated = 1000

        mlflow.log_metric("rows_aggregated", rows_aggregated)

        print(f"âœ… Gold ë ˆì´ì–´ì— {rows_aggregated}ê°œ ì§‘ê³„ í–‰ ìƒì„±")

        return {"rows_aggregated": rows_aggregated}


task_silver_to_gold = PythonOperator(
    task_id='silver_to_gold',
    python_callable=silver_to_gold,
    dag=dag,
)


# ============================================================================
# Task 4: Feature Engineering (í”¼ì²˜ ìƒì„±)
# ============================================================================
def feature_engineering(**context):
    """
    ML ëª¨ë¸ìš© í”¼ì²˜ ìƒì„±
    """
    import mlflow
    import numpy as np

    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

    with mlflow.start_run(run_name="feature_engineering"):
        mlflow.log_param("layer", "features")
        mlflow.log_param("feature_count", 20)

        print("ğŸ”§ Feature Engineering: í”¼ì²˜ ìƒì„± ì¤‘...")

        # ì˜ˆì‹œ í”¼ì²˜ í†µê³„
        feature_stats = {
            "mean": np.random.rand(),
            "std": np.random.rand(),
            "min": np.random.rand(),
            "max": np.random.rand(),
        }

        for key, value in feature_stats.items():
            mlflow.log_metric(f"feature_{key}", value)

        print(f"âœ… 20ê°œ í”¼ì²˜ ìƒì„± ì™„ë£Œ")

        return {"feature_count": 20}


task_feature_engineering = PythonOperator(
    task_id='feature_engineering',
    python_callable=feature_engineering,
    dag=dag,
)


# ============================================================================
# Task 5: Model Training (ëª¨ë¸ í•™ìŠµ)
# ============================================================================
def model_training(**context):
    """
    MLflow ê¸°ë°˜ ëª¨ë¸ í•™ìŠµ
    """
    import mlflow
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.datasets import make_classification
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, f1_score
    import pickle
    import os

    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
    mlflow.set_experiment("lakehouse_ml_pipeline")

    with mlflow.start_run(run_name="model_training"):
        # íŒŒë¼ë¯¸í„° ë¡œê¹…
        mlflow.log_param("model_type", "RandomForest")
        mlflow.log_param("n_estimators", 100)
        mlflow.log_param("max_depth", 10)

        print("ğŸ§  Model Training: ëª¨ë¸ í•™ìŠµ ì¤‘...")

        # ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì‹¤ì œë¡œëŠ” Gold í…Œì´ë¸”ì—ì„œ ë¡œë“œ)
        X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # ëª¨ë¸ í•™ìŠµ
        model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)
        model.fit(X_train, y_train)

        # ì˜ˆì¸¡ ë° í‰ê°€
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)

        # ë©”íŠ¸ë¦­ ë¡œê¹…
        mlflow.log_metric("accuracy", accuracy)
        mlflow.log_metric("f1_score", f1)

        # ëª¨ë¸ì„ pickleë¡œ ì €ì¥ (mlflow.sklearn.log_model ëŒ€ì‹  ì‚¬ìš©)
        model_path = "/tmp/model.pkl"
        with open(model_path, "wb") as f:
            pickle.dump(model, f)
        mlflow.log_artifact(model_path, artifact_path="model")

        print(f"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ (Accuracy: {accuracy:.4f}, F1: {f1:.4f})")

        # Run ID ì €ì¥ (ë‹¤ìŒ íƒœìŠ¤í¬ì—ì„œ ì‚¬ìš©)
        run_id = mlflow.active_run().info.run_id
        context['task_instance'].xcom_push(key='model_run_id', value=run_id)

        return {"accuracy": accuracy, "f1_score": f1, "run_id": run_id}


task_model_training = PythonOperator(
    task_id='model_training',
    python_callable=model_training,
    dag=dag,
)


# ============================================================================
# Task 6: Model Evaluation (ëª¨ë¸ í‰ê°€)
# ============================================================================
def model_evaluation(**context):
    """
    í•™ìŠµëœ ëª¨ë¸ í‰ê°€ ë° ì„±ëŠ¥ ê²€ì¦
    """
    import mlflow

    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

    # ì´ì „ íƒœìŠ¤í¬ì—ì„œ Run ID ê°€ì ¸ì˜¤ê¸°
    run_id = context['task_instance'].xcom_pull(task_ids='model_training', key='model_run_id')

    with mlflow.start_run(run_id=run_id):
        print("ğŸ“Š Model Evaluation: ëª¨ë¸ í‰ê°€ ì¤‘...")

        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì—¬ê¸°ì„œ ì¶”ê°€ ê²€ì¦ ìˆ˜í–‰
        # - êµì°¨ ê²€ì¦
        # - A/B í…ŒìŠ¤íŠ¸
        # - ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ ê²€ì¦

        mlflow.log_metric("validation_passed", 1)

        print("âœ… ëª¨ë¸ í‰ê°€ ì™„ë£Œ - ê²€ì¦ í†µê³¼")

        return {"validation_passed": True}


task_model_evaluation = PythonOperator(
    task_id='model_evaluation',
    python_callable=model_evaluation,
    dag=dag,
)


# ============================================================================
# Task 7: Model Registry (ëª¨ë¸ ë“±ë¡)
# ============================================================================
def model_registry(**context):
    """
    ê²€ì¦ëœ ëª¨ë¸ì„ MLflow Model Registryì— ë“±ë¡
    """
    import mlflow
    from mlflow.tracking import MlflowClient

    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
    client = MlflowClient()

    # ì´ì „ íƒœìŠ¤í¬ì—ì„œ Run ID ê°€ì ¸ì˜¤ê¸°
    run_id = context['task_instance'].xcom_pull(task_ids='model_training', key='model_run_id')

    print("ğŸ“¦ Model Registry: ëª¨ë¸ ë“±ë¡ ì¤‘...")

    # ëª¨ë¸ URI
    model_uri = f"runs:/{run_id}/model"
    model_name = "lakehouse_ml_model"

    # ëª¨ë¸ ë“±ë¡
    try:
        # ìƒˆ ë²„ì „ ë“±ë¡
        model_version = mlflow.register_model(model_uri, model_name)

        # Productionìœ¼ë¡œ ì „í™˜ (ì‹¤ì œë¡œëŠ” ìŠ¹ì¸ í”„ë¡œì„¸ìŠ¤ í•„ìš”)
        client.transition_model_version_stage(
            name=model_name,
            version=model_version.version,
            stage="Production"
        )

        print(f"âœ… ëª¨ë¸ ë“±ë¡ ì™„ë£Œ: {model_name} v{model_version.version} (Production)")

        return {
            "model_name": model_name,
            "model_version": model_version.version,
            "stage": "Production"
        }
    except Exception as e:
        print(f"âš ï¸  ëª¨ë¸ ë“±ë¡ ì¤‘ ì˜¤ë¥˜: {e}")
        # ì²« ë“±ë¡ì´ ì•„ë‹Œ ê²½ìš° ë²„ì „ë§Œ ì—…ë°ì´íŠ¸
        return {"status": "registered"}


task_model_registry = PythonOperator(
    task_id='model_registry',
    python_callable=model_registry,
    dag=dag,
)


# ============================================================================
# DAG íë¦„ ì •ì˜
# ============================================================================

# ë°ì´í„° íŒŒì´í”„ë¼ì¸: RAW â†’ Bronze â†’ Silver â†’ Gold
task_raw_to_bronze >> task_bronze_to_silver >> task_silver_to_gold

# ML íŒŒì´í”„ë¼ì¸: Feature Engineering â†’ Training â†’ Evaluation â†’ Registry
task_silver_to_gold >> task_feature_engineering >> task_model_training
task_model_training >> task_model_evaluation >> task_model_registry

"""
DAG ì‹¤í–‰ íë¦„:
==============

raw_to_bronze
      â†“
bronze_to_silver
      â†“
silver_to_gold
      â†“
feature_engineering
      â†“
model_training
      â†“
model_evaluation
      â†“
model_registry

ê° ë‹¨ê³„ëŠ” MLflowì— ì‹¤í—˜ ê²°ê³¼ë¥¼ ê¸°ë¡í•˜ë©°,
ìµœì¢… ëª¨ë¸ì€ MLflow Model Registryì— ë“±ë¡ë©ë‹ˆë‹¤.

ì ‘ì† URL:
---------
- Airflow UI: http://localhost:8082
- MLflow UI: http://localhost:5000

ë¡œê·¸ì¸:
------
- Airflow: admin / admin
"""
