from pyspark.sql import SparkSession
from datetime import datetime
from pyspark.sql import Row
import os

# MinIO 인증 (docker 기본값)
os.environ['AWS_ACCESS_KEY_ID'] = 'minioadmin'
os.environ['AWS_SECRET_ACCESS_KEY'] = 'minioadmin'
os.environ['AWS_REGION'] = 'us-east-1'  # 무시됨 but 필요

spark = SparkSession.builder \
    .appName("IcebergWindowsConnect") \
    .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
    .config("spark.sql.catalog.hive_prod", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.hive_prod.type", "hive") \
    .config("spark.sql.catalog.hive_prod.uri", "thrift://localhost:9083") \
    .config("spark.sql.catalog.hive_prod.warehouse", "s3://warehouse/") \
    .config("spark.sql.catalog.hive_prod.io-impl", "org.apache.iceberg.aws.s3.S3FileIO") \
    .config("spark.sql.catalog.hive_prod.s3.endpoint", "http://localhost:9000") \
    .config("spark.sql.catalog.hive_prod.s3.path-style-access", "true") \
    .config("spark.jars.packages", \
            "org.apache.iceberg:iceberg-spark-runtime-4.0_2.13:1.10.0,"  # Spark 4.0 + Scala 2.13 build (available on Maven Central)
            "software.amazon.awssdk:bundle:2.25.23," \
            "software.amazon.awssdk:url-connection-client:2.25.23") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider") \
    .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
    .config("spark.hadoop.fs.s3a.secret.key", "minioadmin") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://localhost:9000") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
    .getOrCreate()

print("Spark 버전:", spark.version)
print("연결 성공! Spark UI: http://localhost:4040 (또는 4041)")

# 테스트: 테이블 목록 확인
spark.sql("SHOW DATABASES IN hive_prod").show()
