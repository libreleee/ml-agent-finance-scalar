# Airflow DAG ì™„ì „ ì˜ˆì œ

## ğŸ“‹ ëª©ì°¨

1. [XGBoost DAG](#xgboost-dag)
2. [LightGBM DAG](#lightgbm-dag)
3. [TensorFlow MNIST MLP DAG](#tensorflow-mnist-mlp-dag)
4. [PyTorch MNIST MLP DAG](#pytorch-mnist-mlp-dag)

---

## XGBoost DAG

íŒŒì¼ëª…: `dags/xgboost_pipeline_dag.py`

```python
"""
XGBoost Classification Pipeline with MLflow
============================================

ì´ DAGëŠ” XGBoost ë¶„ë¥˜ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  MLflowì— ë“±ë¡í•©ë‹ˆë‹¤.
- ë°ì´í„°: scikit-learn make_classificationìœ¼ë¡œ ìƒì„±
- ëª¨ë¸: XGBoost ì´ì§„ ë¶„ë¥˜
- ì¶”ì : MLflow í†µí•©
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.standard.operators.python import PythonOperator
import os

# MLflow ì„¤ì •
MLFLOW_TRACKING_URI = os.getenv('MLFLOW_TRACKING_URI', 'http://mlflow:5000')

# ê¸°ë³¸ ì¸ì
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=2),
}

# DAG ì •ì˜
dag = DAG(
    'xgboost_classification_pipeline',
    default_args=default_args,
    description='XGBoost Classification with MLflow',
    schedule_interval=timedelta(days=1),
    start_date=datetime(2025, 12, 27),
    catchup=False,
    tags=['ml', 'xgboost', 'mlflow'],
)


def load_data(**context):
    """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
    import mlflow
    from sklearn.datasets import make_classification
    from sklearn.model_selection import train_test_split

    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

    with mlflow.start_run(run_name="load_data"):
        # ë°ì´í„° ìƒì„±
        X, y = make_classification(
            n_samples=10000,
            n_features=20,
            n_informative=15,
            n_redundant=5,
            random_state=42
        )
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        # ë©”íƒ€ë°ì´í„° ë¡œê¹…
        mlflow.log_param("n_samples", 10000)
        mlflow.log_param("n_features", 20)
        mlflow.log_param("train_size", len(X_train))
        mlflow.log_param("test_size", len(X_test))

        print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: Train={len(X_train)}, Test={len(X_test)}")

        return {
            'train_size': len(X_train),
            'test_size': len(X_test)
        }


def train_xgboost(**context):
    """XGBoost ëª¨ë¸ í•™ìŠµ"""
    import mlflow
    import mlflow.xgboost
    import xgboost as xgb
    from sklearn.datasets import make_classification
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
    mlflow.xgboost.autolog()

    with mlflow.start_run(run_name="train_xgboost"):
        # ë°ì´í„° ë¡œë“œ
        X, y = make_classification(
            n_samples=10000,
            n_features=20,
            n_informative=15,
            n_redundant=5,
            random_state=42
        )
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        # DMatrix ìƒì„±
        dtrain = xgb.DMatrix(X_train, label=y_train)
        dtest = xgb.DMatrix(X_test, label=y_test)

        # í•˜ì´í¼íŒŒë¼ë¯¸í„°
        params = {
            'max_depth': 6,
            'eta': 0.3,
            'objective': 'binary:logistic',
            'eval_metric': 'logloss',
            'subsample': 0.8,
            'colsample_bytree': 0.8,
        }

        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œê¹…
        for key, value in params.items():
            mlflow.log_param(key, value)

        # í•™ìŠµ
        evals = [(dtrain, 'train'), (dtest, 'test')]
        model = xgb.train(
            params,
            dtrain,
            num_boost_round=100,
            evals=evals,
            early_stopping_rounds=10,
            verbose_eval=False
        )

        # í‰ê°€
        y_pred_proba = model.predict(dtest)
        y_pred = (y_pred_proba > 0.5).astype(int)

        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)

        # ë©”íŠ¸ë¦­ ë¡œê¹…
        mlflow.log_metric("accuracy", accuracy)
        mlflow.log_metric("precision", precision)
        mlflow.log_metric("recall", recall)
        mlflow.log_metric("f1_score", f1)

        print(f"âœ… XGBoost í•™ìŠµ ì™„ë£Œ")
        print(f"   Accuracy: {accuracy:.4f}")
        print(f"   Precision: {precision:.4f}")
        print(f"   Recall: {recall:.4f}")
        print(f"   F1-Score: {f1:.4f}")

        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1
        }


def register_model(**context):
    """MLflow ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡"""
    import mlflow

    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

    # ìµœì‹  run ê°€ì ¸ì˜¤ê¸°
    experiment = mlflow.get_experiment_by_name("Default")
    if experiment is None:
        print("âŒ Default experimentë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return

    runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])
    if runs.empty:
        print("âŒ ì‹¤í–‰ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤")
        return

    latest_run = runs.iloc[0]
    latest_run_id = latest_run['run_id']

    # ëª¨ë¸ ë“±ë¡
    model_name = "xgboost_classifier"
    model_uri = f"runs:/{latest_run_id}/model"

    try:
        mlflow.register_model(model_uri, model_name)
        print(f"âœ… ëª¨ë¸ ë“±ë¡ ì™„ë£Œ: {model_name}")
    except Exception as e:
        print(f"âš ï¸ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨: {str(e)}")
        print(f"   (ëª¨ë¸ì´ ì´ë¯¸ ë“±ë¡ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")


# Task ì •ì˜
load_data_task = PythonOperator(
    task_id='load_data',
    python_callable=load_data,
    dag=dag,
)

train_task = PythonOperator(
    task_id='train_xgboost',
    python_callable=train_xgboost,
    dag=dag,
)

register_task = PythonOperator(
    task_id='register_model',
    python_callable=register_model,
    dag=dag,
)

# Task ì˜ì¡´ì„±
load_data_task >> train_task >> register_task
```

---

## LightGBM DAG

íŒŒì¼ëª…: `dags/lightgbm_pipeline_dag.py`

XGBoost DAGê³¼ ê±°ì˜ ë™ì¼í•˜ë©°, ì£¼ìš” ì°¨ì´ì ì€:

```python
def train_lightgbm(**context):
    """LightGBM ëª¨ë¸ í•™ìŠµ"""
    import mlflow
    import mlflow.lightgbm
    import lightgbm as lgb
    from sklearn.datasets import make_classification
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, f1_score

    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
    mlflow.lightgbm.autolog()

    with mlflow.start_run(run_name="train_lightgbm"):
        # ë°ì´í„° ë¡œë“œ
        X, y = make_classification(
            n_samples=10000,
            n_features=20,
            random_state=42
        )
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        # Dataset ìƒì„±
        dtrain = lgb.Dataset(X_train, label=y_train)
        dtest = lgb.Dataset(X_test, label=y_test, reference=dtrain)

        # íŒŒë¼ë¯¸í„°
        params = {
            'num_leaves': 31,
            'learning_rate': 0.05,
            'objective': 'binary',
            'metric': 'binary_logloss',
            'subsample': 0.8,
            'feature_fraction': 0.8,
        }

        # í•™ìŠµ
        model = lgb.train(
            params,
            dtrain,
            num_boost_round=100,
            valid_sets=[dtest],
            early_stopping_rounds=10,
            verbose_eval=False
        )

        # í‰ê°€
        y_pred_proba = model.predict(X_test)
        y_pred = (y_pred_proba > 0.5).astype(int)

        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)

        mlflow.log_metric("accuracy", accuracy)
        mlflow.log_metric("f1_score", f1)

        print(f"âœ… LightGBM í•™ìŠµ ì™„ë£Œ: Accuracy={accuracy:.4f}, F1={f1:.4f}")

        return {'accuracy': accuracy, 'f1_score': f1}
```

---

## TensorFlow MNIST MLP DAG

íŒŒì¼ëª…: `dags/tensorflow_mnist_mlp_dag.py`

```python
"""
TensorFlow MNIST MLP Pipeline with MLflow
==========================================

ì´ DAGëŠ” MNIST ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•œ MLPë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.standard.operators.python import PythonOperator
import os

MLFLOW_TRACKING_URI = os.getenv('MLFLOW_TRACKING_URI', 'http://mlflow:5000')

default_args = {
    'owner': 'airflow',
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=2),
}

dag = DAG(
    'tensorflow_mnist_mlp_pipeline',
    default_args=default_args,
    description='TensorFlow MNIST MLP with MLflow',
    schedule_interval=timedelta(days=1),
    start_date=datetime(2025, 12, 27),
    catchup=False,
    tags=['dl', 'tensorflow', 'mnist', 'mlp'],
)


def download_mnist(**context):
    """MNIST ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ"""
    import mlflow
    import tensorflow as tf

    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

    with mlflow.start_run(run_name="download_mnist"):
        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

        mlflow.log_param("train_samples", len(x_train))
        mlflow.log_param("test_samples", len(x_test))

        print(f"âœ… MNIST ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
        print(f"   Train: {len(x_train)}, Test: {len(x_test)}")

        return {
            'train_samples': len(x_train),
            'test_samples': len(x_test)
        }


def train_mlp(**context):
    """MNIST MLP ëª¨ë¸ í•™ìŠµ"""
    import mlflow
    import mlflow.tensorflow
    import tensorflow as tf
    import numpy as np

    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
    mlflow.tensorflow.autolog()

    with mlflow.start_run(run_name="train_mnist_mlp"):
        # ë°ì´í„° ë¡œë“œ
        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

        # ì „ì²˜ë¦¬
        x_train = x_train / 255.0
        x_test = x_test / 255.0

        # ëª¨ë¸ ì •ì˜
        model = tf.keras.Sequential([
            tf.keras.layers.Flatten(input_shape=(28, 28)),
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dropout(0.2),
            tf.keras.layers.Dense(10, activation='softmax')
        ])

        # ì»´íŒŒì¼
        model.compile(
            optimizer='adam',
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )

        # í•™ìŠµ
        history = model.fit(
            x_train, y_train,
            epochs=10,
            batch_size=128,
            validation_split=0.1,
            verbose=1
        )

        # í‰ê°€
        test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)

        mlflow.log_metric("test_accuracy", test_acc)
        mlflow.log_metric("test_loss", test_loss)
        mlflow.log_metric("final_train_loss", history.history['loss'][-1])
        mlflow.log_metric("final_train_accuracy", history.history['accuracy'][-1])

        print(f"âœ… MLP í•™ìŠµ ì™„ë£Œ")
        print(f"   Test Accuracy: {test_acc:.4f}")
        print(f"   Test Loss: {test_loss:.4f}")

        return {
            'test_accuracy': float(test_acc),
            'test_loss': float(test_loss)
        }


# Task ì •ì˜
download_task = PythonOperator(
    task_id='download_mnist',
    python_callable=download_mnist,
    dag=dag,
)

train_task = PythonOperator(
    task_id='train_mlp',
    python_callable=train_mlp,
    dag=dag,
)

# Task ì˜ì¡´ì„±
download_task >> train_task
```

---

## PyTorch MNIST MLP DAG

íŒŒì¼ëª…: `dags/pytorch_mnist_mlp_dag.py`

```python
"""
PyTorch MNIST MLP Pipeline with MLflow
=======================================

ì´ DAGëŠ” MNIST ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ PyTorch MLPë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.standard.operators.python import PythonOperator
import os

MLFLOW_TRACKING_URI = os.getenv('MLFLOW_TRACKING_URI', 'http://mlflow:5000')

default_args = {
    'owner': 'airflow',
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=2),
}

dag = DAG(
    'pytorch_mnist_mlp_pipeline',
    default_args=default_args,
    description='PyTorch MNIST MLP with MLflow',
    schedule_interval=timedelta(days=1),
    start_date=datetime(2025, 12, 27),
    catchup=False,
    tags=['dl', 'pytorch', 'mnist', 'mlp'],
)


def train_pytorch_mlp(**context):
    """PyTorch MNIST MLP ëª¨ë¸ í•™ìŠµ"""
    import mlflow
    import mlflow.pytorch
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torchvision import datasets, transforms
    from torch.utils.data import DataLoader

    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
    mlflow.pytorch.autolog()

    with mlflow.start_run(run_name="train_pytorch_mlp"):
        # í•˜ì´í¼íŒŒë¼ë¯¸í„°
        epochs = 10
        batch_size = 128
        learning_rate = 0.001

        mlflow.log_param("epochs", epochs)
        mlflow.log_param("batch_size", batch_size)
        mlflow.log_param("learning_rate", learning_rate)

        # ë°ì´í„° ë¡œë“œ
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])

        train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
        test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)

        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

        # ëª¨ë¸ ì •ì˜
        class MLP(nn.Module):
            def __init__(self):
                super().__init__()
                self.flatten = nn.Flatten()
                self.fc1 = nn.Linear(28*28, 128)
                self.relu = nn.ReLU()
                self.dropout = nn.Dropout(0.2)
                self.fc2 = nn.Linear(128, 10)

            def forward(self, x):
                x = self.flatten(x)
                x = self.fc1(x)
                x = self.relu(x)
                x = self.dropout(x)
                x = self.fc2(x)
                return x

        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = MLP().to(device)

        # ì†ì‹¤í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì €
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)

        # í•™ìŠµ
        for epoch in range(epochs):
            model.train()
            train_loss = 0.0

            for images, labels in train_loader:
                images, labels = images.to(device), labels.to(device)

                optimizer.zero_grad()
                outputs = model(images)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()

                train_loss += loss.item()

            mlflow.log_metric("train_loss", train_loss / len(train_loader), step=epoch)

        # í‰ê°€
        model.eval()
        correct = 0
        total = 0

        with torch.no_grad():
            for images, labels in test_loader:
                images, labels = images.to(device), labels.to(device)

                outputs = model(images)
                _, predicted = torch.max(outputs.data, 1)

                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        test_accuracy = 100 * correct / total
        mlflow.log_metric("test_accuracy", test_accuracy)

        print(f"âœ… PyTorch MLP í•™ìŠµ ì™„ë£Œ")
        print(f"   Test Accuracy: {test_accuracy:.2f}%")

        return {'test_accuracy': test_accuracy}


# Task ì •ì˜
train_task = PythonOperator(
    task_id='train_pytorch_mlp',
    python_callable=train_pytorch_mlp,
    dag=dag,
)
```

---

## MLflow í†µí•© íŒ¨í„´

### Autologging í™œì„±í™”

ê° í”„ë ˆì„ì›Œí¬ ìë™ ë¡œê¹… ì„¤ì •:

```python
# XGBoost
import mlflow.xgboost
mlflow.xgboost.autolog()

# LightGBM
import mlflow.lightgbm
mlflow.lightgbm.autolog()

# TensorFlow
import mlflow.tensorflow
mlflow.tensorflow.autolog()

# PyTorch
import mlflow.pytorch
mlflow.pytorch.autolog()
```

### Manual Logging íŒ¨í„´

```python
with mlflow.start_run(run_name="experiment_name"):
    # íŒŒë¼ë¯¸í„° ë¡œê¹…
    mlflow.log_param("learning_rate", 0.01)
    mlflow.log_param("batch_size", 128)

    # í•™ìŠµ ì½”ë“œ
    for epoch in range(epochs):
        loss = train_step()
        mlflow.log_metric("train_loss", loss, step=epoch)

    # ìµœì¢… ë©”íŠ¸ë¦­
    mlflow.log_metric("final_accuracy", accuracy)
```

---

**ë‹¤ìŒ**: [ì‹¤í–‰ ê³„íš ì½ê¸° â†’](05-EXECUTION_PLAN.md)
