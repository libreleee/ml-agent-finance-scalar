# ML/DL ëª¨ë¸ ìƒì„¸ ì‚¬ì–‘

## ğŸ“‹ ëª©ì°¨

1. [XGBoost ì‚¬ì–‘](#xgboost-ì‚¬ì–‘)
2. [LightGBM ì‚¬ì–‘](#lightgbm-ì‚¬ì–‘)
3. [TensorFlow/Keras ì‚¬ì–‘](#tensorflowkeras-ì‚¬ì–‘)
4. [PyTorch ì‚¬ì–‘](#pytorch-ì‚¬ì–‘)
5. [ì„±ëŠ¥ ë¹„êµ](#ì„±ëŠ¥-ë¹„êµ)

---

## XGBoost ì‚¬ì–‘

### ë¼ì´ë¸ŒëŸ¬ë¦¬ ì •ë³´

- **ë²„ì „**: 2.0.3 ì´ìƒ
- **ê³µì‹ ë¬¸ì„œ**: https://xgboost.readthedocs.io/
- **GitHub**: https://github.com/dmlc/xgboost

### ê¸°ë³¸ íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ì„¤ëª… | ì¶”ì²œê°’ |
|---------|------|-------|
| `max_depth` | íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´ | 6 |
| `eta` (learning_rate) | í•™ìŠµë¥  | 0.3 |
| `objective` | ì†ì‹¤ í•¨ìˆ˜ | binary:logistic (ì´ì§„) / multi:softmax (ë‹¤ì¤‘) |
| `num_boost_round` | ë¶€ìŠ¤íŒ… ë¼ìš´ë“œ | 100 |
| `subsample` | ìƒ˜í”Œë§ ë¹„ìœ¨ | 0.8 |
| `colsample_bytree` | í”¼ì²˜ ìƒ˜í”Œë§ ë¹„ìœ¨ | 0.8 |

### ëª¨ë¸ êµ¬ì¡°

```python
import xgboost as xgb
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

# ë°ì´í„° ì¤€ë¹„
X, y = make_classification(n_samples=10000, n_features=20, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# DMatrix ìƒì„±
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# íŒŒë¼ë¯¸í„° ì„¤ì •
params = {
    'max_depth': 6,
    'eta': 0.3,
    'objective': 'binary:logistic',
    'eval_metric': 'logloss',
    'subsample': 0.8,
    'colsample_bytree': 0.8,
}

# ëª¨ë¸ í•™ìŠµ
model = xgb.train(
    params,
    dtrain,
    num_boost_round=100,
    evals=[(dtrain, 'train'), (dtest, 'test')],
    early_stopping_rounds=10,
    verbose_eval=10
)

# ì˜ˆì¸¡
predictions = model.predict(dtest)
```

### ì˜ˆìƒ ì„±ëŠ¥

- **í•™ìŠµ ì‹œê°„**: 2GB ë°ì´í„° ê¸°ì¤€ ~ 30ì´ˆ
- **ë©”ëª¨ë¦¬ ì‚¬ìš©**: ì…ë ¥ ë°ì´í„° í¬ê¸°ì˜ ì•½ 2-3ë°°
- **ì •í™•ë„**: íŠœë‹ ì‹œ 85-95%

### MLflow í†µí•©

```python
import mlflow
import mlflow.xgboost

mlflow.xgboost.autolog()

with mlflow.start_run():
    # ëª¨ë¸ í•™ìŠµ
    model = xgb.train(params, dtrain, num_boost_round=100)
    # ìë™ìœ¼ë¡œ íŒŒë¼ë¯¸í„°, ë©”íŠ¸ë¦­, ëª¨ë¸ì´ ë¡œê¹…ë¨
```

---

## LightGBM ì‚¬ì–‘

### ë¼ì´ë¸ŒëŸ¬ë¦¬ ì •ë³´

- **ë²„ì „**: 4.1.0 ì´ìƒ
- **ê³µì‹ ë¬¸ì„œ**: https://lightgbm.readthedocs.io/
- **GitHub**: https://github.com/microsoft/LightGBM

### ê¸°ë³¸ íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ì„¤ëª… | ì¶”ì²œê°’ |
|---------|------|-------|
| `num_leaves` | ë¦¬í”„ ë…¸ë“œ ìˆ˜ | 31 |
| `learning_rate` | í•™ìŠµë¥  | 0.05 |
| `objective` | ì†ì‹¤ í•¨ìˆ˜ | binary / multiclass |
| `num_boost_round` | ë¶€ìŠ¤íŒ… ë¼ìš´ë“œ | 100 |
| `subsample` | ìƒ˜í”Œë§ ë¹„ìœ¨ | 0.8 |
| `feature_fraction` | í”¼ì²˜ ìƒ˜í”Œë§ ë¹„ìœ¨ | 0.8 |

### ëª¨ë¸ êµ¬ì¡°

```python
import lightgbm as lgb
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

# ë°ì´í„° ì¤€ë¹„
X, y = make_classification(n_samples=10000, n_features=20, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Dataset ìƒì„±
dtrain = lgb.Dataset(X_train, label=y_train)
dtest = lgb.Dataset(X_test, label=y_test, reference=dtrain)

# íŒŒë¼ë¯¸í„° ì„¤ì •
params = {
    'num_leaves': 31,
    'learning_rate': 0.05,
    'objective': 'binary',
    'metric': 'binary_logloss',
    'subsample': 0.8,
    'feature_fraction': 0.8,
}

# ëª¨ë¸ í•™ìŠµ
model = lgb.train(
    params,
    dtrain,
    num_boost_round=100,
    valid_sets=[dtest],
    early_stopping_rounds=10,
    verbose_eval=10
)

# ì˜ˆì¸¡
predictions = model.predict(X_test)
```

### ì˜ˆìƒ ì„±ëŠ¥

- **í•™ìŠµ ì‹œê°„**: 2GB ë°ì´í„° ê¸°ì¤€ ~ 15ì´ˆ (XGBoost ëŒ€ë¹„ 2ë°° ë¹ ë¦„)
- **ë©”ëª¨ë¦¬ ì‚¬ìš©**: XGBoost ëŒ€ë¹„ 20-30% ì ìŒ
- **ì •í™•ë„**: XGBoostì™€ ë™ë“± ë˜ëŠ” ì•½ê°„ ë†’ìŒ

### MLflow í†µí•©

```python
import mlflow
import mlflow.lightgbm

mlflow.lightgbm.autolog()

with mlflow.start_run():
    # ëª¨ë¸ í•™ìŠµ
    model = lgb.train(params, dtrain, num_boost_round=100)
    # ìë™ìœ¼ë¡œ íŒŒë¼ë¯¸í„°, ë©”íŠ¸ë¦­, ëª¨ë¸ì´ ë¡œê¹…ë¨
```

---

## TensorFlow/Keras ì‚¬ì–‘

### ë¼ì´ë¸ŒëŸ¬ë¦¬ ì •ë³´

- **TensorFlow ë²„ì „**: 2.15.0 ì´ìƒ
- **Keras ë²„ì „**: 3.0.0 ì´ìƒ
- **ê³µì‹ ë¬¸ì„œ**: https://tensorflow.org/
- **GitHub**: https://github.com/tensorflow/tensorflow

### MNIST MLP ì‚¬ì–‘

**ì•„í‚¤í…ì²˜**:
```
Input (784) â†’ Dense (128, relu) â†’ Dropout (0.2) â†’ Dense (10, softmax)
```

**íŒŒë¼ë¯¸í„° ìˆ˜**: ~100K

**ì½”ë“œ ì˜ˆì œ**:
```python
import tensorflow as tf
from tensorflow import keras

# ëª¨ë¸ ì •ì˜
model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10, activation='softmax')
])

# ì»´íŒŒì¼
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# í•™ìŠµ
history = model.fit(
    x_train, y_train,
    epochs=10,
    batch_size=128,
    validation_split=0.1,
    verbose=1
)

# í‰ê°€
test_loss, test_acc = model.evaluate(x_test, y_test)
```

**ì˜ˆìƒ ì„±ëŠ¥**:
- í•™ìŠµ ì‹œê°„: CPU ê¸°ì¤€ 5-10ë¶„ (10 epochs)
- ì •í™•ë„: ~97%
- ë©”ëª¨ë¦¬: ì•½ 2-3GB

### MNIST CNN ì‚¬ì–‘

**ì•„í‚¤í…ì²˜**:
```
Input (28, 28, 1)
â†’ Conv2D (32, 3Ã—3, relu) â†’ MaxPool (2Ã—2)
â†’ Conv2D (64, 3Ã—3, relu) â†’ MaxPool (2Ã—2)
â†’ Flatten
â†’ Dense (64, relu)
â†’ Dense (10, softmax)
```

**íŒŒë¼ë¯¸í„° ìˆ˜**: ~100K

**ì½”ë“œ ì˜ˆì œ**:
```python
model = keras.Sequential([
    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Conv2D(64, (3, 3), activation='relu'),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Flatten(),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(10, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

history = model.fit(
    x_train.reshape(-1, 28, 28, 1), y_train,
    epochs=10,
    batch_size=128,
    validation_split=0.1,
    verbose=1
)
```

**ì˜ˆìƒ ì„±ëŠ¥**:
- í•™ìŠµ ì‹œê°„: CPU ê¸°ì¤€ 20-30ë¶„ (10 epochs)
- ì •í™•ë„: ~99%
- ë©”ëª¨ë¦¬: ì•½ 3-4GB

### MLflow í†µí•©

```python
import mlflow
import mlflow.tensorflow

mlflow.tensorflow.autolog()

with mlflow.start_run():
    # ëª¨ë¸ í•™ìŠµ
    history = model.fit(x_train, y_train, epochs=10)
    # ìë™ìœ¼ë¡œ íŒŒë¼ë¯¸í„°, ë©”íŠ¸ë¦­ì´ ë¡œê¹…ë¨
```

---

## PyTorch ì‚¬ì–‘

### ë¼ì´ë¸ŒëŸ¬ë¦¬ ì •ë³´

- **PyTorch ë²„ì „**: 2.1.0 ì´ìƒ
- **TorchVision ë²„ì „**: 0.16.0 ì´ìƒ
- **ê³µì‹ ë¬¸ì„œ**: https://pytorch.org/
- **GitHub**: https://github.com/pytorch/pytorch

### MNIST MLP ì‚¬ì–‘

**ì•„í‚¤í…ì²˜**:
```python
class MLP(nn.Module):
    Input (784) â†’ Linear (128) â†’ ReLU â†’ Dropout (0.2) â†’ Linear (10)
```

**íŒŒë¼ë¯¸í„° ìˆ˜**: ~100K

**ì½”ë“œ ì˜ˆì œ**:
```python
import torch
import torch.nn as nn
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(28*28, 128)
        self.dropout = nn.Dropout(0.2)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.flatten(x)
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

# ëª¨ë¸, ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €
model = MLP()
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())

# í•™ìŠµ
for epoch in range(10):
    for images, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

# í‰ê°€
correct = 0
total = 0
with torch.no_grad():
    for images, labels in test_loader:
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = 100 * correct / total
```

**ì˜ˆìƒ ì„±ëŠ¥**:
- í•™ìŠµ ì‹œê°„: CPU ê¸°ì¤€ 5-10ë¶„
- ì •í™•ë„: ~97%
- ë©”ëª¨ë¦¬: ì•½ 2-3GB

### MNIST CNN ì‚¬ì–‘

**ì•„í‚¤í…ì²˜**:
```python
class CNN(nn.Module):
    Input (1, 28, 28)
    â†’ Conv2D (32, 3Ã—3) â†’ ReLU â†’ MaxPool (2Ã—2)
    â†’ Conv2D (64, 3Ã—3) â†’ ReLU â†’ MaxPool (2Ã—2)
    â†’ Flatten
    â†’ Linear (64*5*5, 64) â†’ ReLU
    â†’ Linear (64, 10)
```

**íŒŒë¼ë¯¸í„° ìˆ˜**: ~100K

**ì½”ë“œ ì˜ˆì œ**:
```python
class CNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(32, 64, 3)
        self.fc1 = nn.Linear(64 * 5 * 5, 64)
        self.fc2 = nn.Linear(64, 10)

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = x.view(-1, 64 * 5 * 5)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x
```

**ì˜ˆìƒ ì„±ëŠ¥**:
- í•™ìŠµ ì‹œê°„: CPU ê¸°ì¤€ 20-30ë¶„
- ì •í™•ë„: ~99%
- ë©”ëª¨ë¦¬: ì•½ 3-4GB

### MLflow í†µí•©

```python
import mlflow
import mlflow.pytorch

mlflow.pytorch.autolog()

with mlflow.start_run():
    # ëª¨ë¸ í•™ìŠµ
    for epoch in range(10):
        # í•™ìŠµ ì½”ë“œ
        pass
    # ìë™ìœ¼ë¡œ ë©”íŠ¸ë¦­ì´ ë¡œê¹…ë¨
```

---

## ì„±ëŠ¥ ë¹„êµ

### í•™ìŠµ ì‹œê°„ ë¹„êµ (CPU ê¸°ì¤€)

| ëª¨ë¸ | í•™ìŠµ ì‹œê°„ | ìƒëŒ€ê°’ |
|------|----------|-------|
| XGBoost (2GB ë°ì´í„°) | ~30ì´ˆ | 1x |
| LightGBM (2GB ë°ì´í„°) | ~15ì´ˆ | 0.5x |
| TensorFlow MLP (10 epochs) | 5-10ë¶„ | 10-20x |
| TensorFlow CNN (10 epochs) | 20-30ë¶„ | 40-60x |
| PyTorch MLP (10 epochs) | 5-10ë¶„ | 10-20x |
| PyTorch CNN (10 epochs) | 20-30ë¶„ | 40-60x |

### ì •í™•ë„ ë¹„êµ

| ëª¨ë¸ | MNIST ì •í™•ë„ | ì„¤ëª… |
|------|-----------|------|
| XGBoost | ~85% | Tabular ë°ì´í„°ìš© (MNISTì—ëŠ” ë¶€ìµœì ) |
| LightGBM | ~87% | Tabular ë°ì´í„°ìš© (MNISTì—ëŠ” ë¶€ìµœì ) |
| TensorFlow MLP | ~97% | ê¸°ë³¸ MLP |
| TensorFlow CNN | ~99% | CNN (ì´ë¯¸ì§€ì— ìµœì í™”) |
| PyTorch MLP | ~97% | ê¸°ë³¸ MLP |
| PyTorch CNN | ~99% | CNN (ì´ë¯¸ì§€ì— ìµœì í™”) |

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

| ëª¨ë¸ | ë©”ëª¨ë¦¬ ì‚¬ìš© |
|------|-----------|
| XGBoost | ë°ì´í„° í¬ê¸°ì˜ 2-3ë°° |
| LightGBM | ë°ì´í„° í¬ê¸°ì˜ 1.5-2ë°° |
| TensorFlow MLP | 2-3GB |
| TensorFlow CNN | 3-4GB |
| PyTorch MLP | 2-3GB |
| PyTorch CNN | 3-4GB |

---

**ë‹¤ìŒ**: [Airflow DAG ì˜ˆì œ ì½ê¸° â†’](04-AIRFLOW_DAG_EXAMPLES.md)
