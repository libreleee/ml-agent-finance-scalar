# ML/DL ëª¨ë¸ êµ¬í˜„ ê°€ì´ë“œ

## ðŸ“‹ ëª©ì°¨

1. [ì‚¬ì „ ì¤€ë¹„](#ì‚¬ì „-ì¤€ë¹„)
2. [1ë‹¨ê³„: ì˜ì¡´ì„± ì¶”ê°€](#1ë‹¨ê³„-ì˜ì¡´ì„±-ì¶”ê°€)
3. [2ë‹¨ê³„: Docker ì„¤ì • ë³€ê²½](#2ë‹¨ê³„-docker-ì„¤ì •-ë³€ê²½)
4. [3ë‹¨ê³„: Airflow DAG ìž‘ì„±](#3ë‹¨ê³„-airflow-dag-ìž‘ì„±)
5. [4ë‹¨ê³„: MLflow í†µí•©](#4ë‹¨ê³„-mlflow-í†µí•©)
6. [5ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë° ê²€ì¦](#5ë‹¨ê³„-í…ŒìŠ¤íŠ¸-ë°-ê²€ì¦)
7. [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)

---

## ì‚¬ì „ ì¤€ë¹„

### í™•ì¸ ì‚¬í•­

```bash
# í˜„ìž¬ ë””ë ‰í† ë¦¬
cd /home/i/work/ai/lakehouse-tick

# MLOps ìŠ¤íƒ ì‹¤í–‰ í™•ì¸
docker compose -f docker-compose-mlops.yml ps

# Airflow UI: http://localhost:8082 (admin/admin)
# MLflow UI: http://localhost:5000
```

### ë°±ì—…

```bash
# ê¸°ì¡´ ì„¤ì • ë°±ì—…
cp requirements-airflow.txt requirements-airflow.txt.backup
cp docker-compose-mlops.yml docker-compose-mlops.yml.backup
```

---

## 1ë‹¨ê³„: ì˜ì¡´ì„± ì¶”ê°€

### íŒŒì¼: requirements-airflow.txt

ìœ„ì¹˜: `/home/i/work/ai/lakehouse-tick/requirements-airflow.txt`

#### Traditional ML ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€

ê¸°ì¡´ ë‚´ìš© ì•„ëž˜ì— ì¶”ê°€:

```txt
# ============ NEW: Traditional ML ============
xgboost>=2.0.3
lightgbm>=4.1.0
```

#### Deep Learning ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€

```txt
# ============ NEW: Deep Learning ============
# TensorFlow/Keras
tensorflow>=2.15.0,<2.16.0
keras>=3.0.0

# PyTorch
torch>=2.1.0,<2.3.0
torchvision>=0.16.0

# Compatibility
numpy>=1.23.0,<2.0.0
protobuf>=3.19.0,<4.0.0
```

---

## 2ë‹¨ê³„: Docker ì„¤ì • ë³€ê²½

### íŒŒì¼: docker-compose-mlops.yml

ìœ„ì¹˜: `/home/i/work/ai/lakehouse-tick/docker-compose-mlops.yml`

#### 2.1 Airflow Worker ë¦¬ì†ŒìŠ¤ ì¦ê°€

`airflow-worker` ì„¹ì…˜ì— `deploy` ë¸”ë¡ ì¶”ê°€:

```yaml
airflow-worker:
  <<: *airflow-common
  command: celery worker

  # ì•„ëž˜ ì¶”ê°€
  deploy:
    resources:
      limits:
        cpus: '6'      # CPU ì¦ê°€: 2 â†’ 6
        memory: 8G     # ë©”ëª¨ë¦¬ ì¦ê°€: 2G â†’ 8G
      reservations:
        cpus: '4'
        memory: 6G
  # ì—¬ê¸°ê¹Œì§€

  healthcheck:
    test: ["CMD-SHELL", 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"']
    interval: 30s
    timeout: 10s
    retries: 5
  environment:
    <<: *airflow-common-env
    DUMB_INIT_SETSID: "0"
  restart: always
  depends_on:
    <<: *airflow-common-depends-on
    airflow-init:
      condition: service_completed_successfully
```

#### 2.2 ë³€ê²½ ì‚¬í•­ ì ìš©

```bash
# 1. MLOps ìŠ¤íƒ ì¤‘ì§€
docker compose -f docker-compose-mlops.yml down

# 2. Airflow ì´ë¯¸ì§€ ìž¬ë¹Œë“œ (requirements-airflow.txt ë³€ê²½ ë°˜ì˜)
docker compose -f docker-compose-mlops.yml build --no-cache airflow-worker

# 3. MLOps ìŠ¤íƒ ìž¬ì‹œìž‘
docker compose -f docker-compose-mlops.yml up -d

# 4. ë¡œê·¸ í™•ì¸ (ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í™•ì¸)
docker compose -f docker-compose-mlops.yml logs -f airflow-worker
```

#### 2.3 ì„¤ì¹˜ í™•ì¸

```bash
# Worker ì»¨í…Œì´ë„ˆ ì ‘ì†
docker compose -f docker-compose-mlops.yml exec airflow-worker bash

# ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
python -c "import xgboost; print(f'XGBoost: {xgboost.__version__}')"
python -c "import lightgbm; print(f'LightGBM: {lightgbm.__version__}')"
python -c "import tensorflow as tf; print(f'TensorFlow: {tf.__version__}')"
python -c "import torch; print(f'PyTorch: {torch.__version__}')"

# ì˜ˆìƒ ì¶œë ¥:
# XGBoost: 2.0.3
# LightGBM: 4.1.0
# TensorFlow: 2.15.x
# PyTorch: 2.1.x
```

---

## 3ë‹¨ê³„: Airflow DAG ìž‘ì„±

### 3.1 DAG íŒŒì¼ ìœ„ì¹˜

ëª¨ë“  DAG íŒŒì¼ì€ ë‹¤ìŒ ë””ë ‰í† ë¦¬ì— ìƒì„±:

```
/home/i/work/ai/lakehouse-tick/dags/
```

### 3.2 ê¸°ì¡´ DAG ì°¸ì¡°

ì°¸ì¡° íŒŒì¼: `/home/i/work/ai/lakehouse-tick/dags/ml_pipeline_dag.py`

ê¸°ì¡´ êµ¬ì¡°ë¥¼ ì°¸ê³ í•˜ì—¬ ìƒˆë¡œìš´ DAG ìž‘ì„±

### 3.3 DAG ìž‘ì„± íŒ¨í„´

ëª¨ë“  DAGëŠ” ë‹¤ìŒ íŒ¨í„´ì„ ë”°ë¦…ë‹ˆë‹¤:

```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.standard.operators.python import PythonOperator
import os

# MLflow ì¶”ì  URI
MLFLOW_TRACKING_URI = os.getenv('MLFLOW_TRACKING_URI', 'http://mlflow:5000')

# ê¸°ë³¸ ì¸ìž
default_args = {
    'owner': 'airflow',
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=2),
}

# DAG ì •ì˜
dag = DAG(
    'model_name_pipeline',  # DAG ID
    default_args=default_args,
    description='Model description',
    schedule=timedelta(days=1),
    start_date=datetime(2025, 12, 27),
    catchup=False,
    tags=['ml', 'model-type', 'mlflow'],
)

# Task ì •ì˜
def task_name(**context):
    """Task description"""
    import mlflow

    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

    with mlflow.start_run(run_name="task_name"):
        # Task ë¡œì§
        mlflow.log_param("param_name", param_value)
        mlflow.log_metric("metric_name", metric_value)

        return {"result": "value"}

# Task ì¸ìŠ¤í„´ìŠ¤í™”
task = PythonOperator(
    task_id='task_name',
    python_callable=task_name,
    dag=dag,
)
```

### 3.4 ìƒì„±í•  DAG íŒŒì¼ë“¤

- `xgboost_pipeline_dag.py` - XGBoost ë¶„ë¥˜
- `lightgbm_pipeline_dag.py` - LightGBM ë¶„ë¥˜
- `tensorflow_mnist_mlp_dag.py` - TensorFlow MNIST MLP
- `tensorflow_mnist_cnn_dag.py` - TensorFlow MNIST CNN
- `pytorch_mnist_mlp_dag.py` - PyTorch MNIST MLP
- `pytorch_mnist_cnn_dag.py` - PyTorch MNIST CNN

ìžì„¸í•œ ì½”ë“œ ì˜ˆì œëŠ” [04-AIRFLOW_DAG_EXAMPLES.md](04-AIRFLOW_DAG_EXAMPLES.md) ì°¸ì¡°

---

## 4ë‹¨ê³„: MLflow í†µí•©

### 4.1 Autologging ì„¤ì •

ê° í”„ë ˆìž„ì›Œí¬ë³„ autologging í™œì„±í™”:

```python
# XGBoost
import mlflow.xgboost
mlflow.xgboost.autolog()

# LightGBM
import mlflow.lightgbm
mlflow.lightgbm.autolog()

# TensorFlow/Keras
import mlflow.tensorflow
mlflow.tensorflow.autolog()

# PyTorch
import mlflow.pytorch
mlflow.pytorch.autolog()
```

### 4.2 Manual Logging

```python
with mlflow.start_run(run_name="experiment_name"):
    # íŒŒë¼ë¯¸í„° ë¡œê¹…
    mlflow.log_param("learning_rate", 0.01)
    mlflow.log_param("batch_size", 128)
    mlflow.log_param("epochs", 10)

    # ë©”íŠ¸ë¦­ ë¡œê¹…
    mlflow.log_metric("train_accuracy", 0.95)
    mlflow.log_metric("test_accuracy", 0.93)
    mlflow.log_metric("loss", 0.05)

    # ì•„í‹°íŒ©íŠ¸ ë¡œê¹…
    mlflow.log_artifact("model_summary.txt")

    # ëª¨ë¸ ë¡œê¹…
    mlflow.sklearn.log_model(model, "model")
```

### 4.3 ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬

```python
# ìµœì‹  run ê°€ì ¸ì˜¤ê¸°
experiment = mlflow.get_experiment_by_name("Default")
runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])
latest_run_id = runs.iloc[0]['run_id']

# ëª¨ë¸ ë“±ë¡
model_uri = f"runs:/{latest_run_id}/model"
mlflow.register_model(model_uri, "model_name")

# ëª¨ë¸ stage ë³€ê²½
client = mlflow.MlflowClient()
client.transition_model_version_stage(
    name="model_name",
    version=1,
    stage="Production"
)
```

---

## 5ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### 5.1 DAG ë¬¸ë²• ì²´í¬

```bash
# DAG íŒŒì¼ í…ŒìŠ¤íŠ¸
docker compose -f docker-compose-mlops.yml exec airflow-worker \
    airflow dags test xgboost_classification_pipeline 2025-12-27
```

### 5.2 Airflow UIì—ì„œ í™•ì¸

1. Airflow UI ì ‘ì†: http://localhost:8082
2. DAG ëª©ë¡ì—ì„œ ìƒˆ DAG í™•ì¸
3. DAG í™œì„±í™” (í† ê¸€ ìŠ¤ìœ„ì¹˜)
4. "Trigger DAG" í´ë¦­í•˜ì—¬ ìˆ˜ë™ ì‹¤í–‰

### 5.3 MLflow UIì—ì„œ í™•ì¸

1. MLflow UI ì ‘ì†: http://localhost:5000
2. Experiments íƒ­ì—ì„œ ì‹¤í—˜ í™•ì¸
3. Runs íƒ­ì—ì„œ ë©”íŠ¸ë¦­/íŒŒë¼ë¯¸í„° í™•ì¸
4. Models íƒ­ì—ì„œ ë“±ë¡ëœ ëª¨ë¸ í™•ì¸

### 5.4 ë¡œê·¸ í™•ì¸

```bash
# Airflow task ë¡œê·¸
docker compose -f docker-compose-mlops.yml logs -f airflow-worker | grep -i "task_id\|error\|âœ…"

# MLflow ë¡œê·¸
docker compose -f docker-compose-mlops.yml logs -f mlflow
```

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: ë©”ëª¨ë¦¬ ë¶€ì¡±

**ì¦ìƒ**:
```
MemoryError: Unable to allocate array
RuntimeError: CUDA out of memory
```

**í•´ê²°**:
1. Docker ë©”ëª¨ë¦¬ ì œí•œ ì¦ê°€
2. ë°°ì¹˜ í¬ê¸° ê°ì†Œ
3. ì—í¬í¬ ìˆ˜ ê°ì†Œ

```yaml
# docker-compose-mlops.yml
airflow-worker:
  deploy:
    resources:
      limits:
        memory: 8G  # ë” ì¦ê°€
```

### ë¬¸ì œ 2: TensorFlow CPU ê²½ê³ 

**ì¦ìƒ**:
```
This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)
I tensorflow/cc/client/client_session.cc:305] Your CPU supports instructions that this TensorFlow binary was not compiled to use
```

**í•´ê²°**: ë¬´ì‹œí•´ë„ ë¨ (ì„±ëŠ¥ ìµœì í™” ê´€ë ¨ ì •ë³´ì„± ë©”ì‹œì§€)

### ë¬¸ì œ 3: PyTorch CUDA ê´€ë ¨ ê²½ê³ 

**ì¦ìƒ**:
```
UserWarning: CUDA not available, using CPU
```

**í•´ê²°**: ì •ìƒ (GPU ì—†ëŠ” í™˜ê²½ì—ì„œ ì˜ˆìƒë˜ëŠ” ë™ìž‘)

### ë¬¸ì œ 4: DAGê°€ UIì— ë‚˜íƒ€ë‚˜ì§€ ì•ŠìŒ

**ì›ì¸**: DAG íŒŒì¼ ë¬¸ë²• ì˜¤ë¥˜

**í•´ê²°**:
```bash
# ë¬¸ë²• ì²´í¬
python /home/i/work/ai/lakehouse-tick/dags/your_dag.py
```

### ë¬¸ì œ 5: MLflow ì—°ê²° ì‹¤íŒ¨

**ì¦ìƒ**:
```
mlflow.exceptions.MlflowException: Failed to connect to MLflow
```

**í•´ê²°**:
```python
# MLFLOW_TRACKING_URI í™•ì¸
MLFLOW_TRACKING_URI = 'http://mlflow:5000'  # ì»¨í…Œì´ë„ˆ ë‚´ë¶€
# ë˜ëŠ”
MLFLOW_TRACKING_URI = 'http://localhost:5000'  # í˜¸ìŠ¤íŠ¸
```

### ë¬¸ì œ 6: ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì—ëŸ¬

**ì¦ìƒ**:
```
ModuleNotFoundError: No module named 'xgboost'
```

**í•´ê²°**:
```bash
# ì´ë¯¸ì§€ ìž¬ë¹Œë“œ
docker compose -f docker-compose-mlops.yml build --no-cache airflow-worker
docker compose -f docker-compose-mlops.yml up -d

# ì„¤ì¹˜ í™•ì¸
docker compose -f docker-compose-mlops.yml exec airflow-worker \
    python -c "import xgboost; print(xgboost.__version__)"
```

### ë¬¸ì œ 7: Task timeout

**ì¦ìƒ**:
```
airflow.exceptions.AirflowTaskTimeout: Task exited after max_tries attempts
```

**í•´ê²°**:
```python
# DAGì˜ default_argsì—ì„œ timeout ì¦ê°€
default_args = {
    'execution_timeout': timedelta(hours=3),  # 3ì‹œê°„ìœ¼ë¡œ ì¦ê°€
}
```

---

**ë‹¤ìŒ**: [ëª¨ë¸ ì‚¬ì–‘ ì½ê¸° â†’](03-MODEL_SPECIFICATIONS.md)
