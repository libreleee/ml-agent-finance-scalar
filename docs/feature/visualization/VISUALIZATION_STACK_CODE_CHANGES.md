# ì‹œê°í™” ìŠ¤íƒ ì½”ë“œ ë³€ê²½ì‚¬í•­ (Code Changes Summary)

> **ìš”ì•½**: ë¬¸ì„œ ìƒì„±ì€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ íŒŒì¼ì€ í•´ë‹¹ ë¬¸ì„œë“¤ì„ **ì‹¤ì œë¡œ êµ¬í˜„**í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ëª¨ë“  ì½”ë“œ/ì„¤ì • ë³€ê²½ì‚¬í•­ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

---

## ğŸ“Š ë³€ê²½ì‚¬í•­ ê°œìš” (Change Overview)

### ì¶”ê°€ë  ì„œë¹„ìŠ¤ (9ê°œ ì‹ ê·œ ì»¨í…Œì´ë„ˆ)
1. **Superset** (BI Dashboard)
2. **Superset-DB** (PostgreSQL for Superset metadata)
3. **Superset-Redis** (Cache layer)
4. **Grafana** (Monitoring dashboards)
5. **OpenSearch** (Log storage)
6. **OpenSearch-Dashboards** (Log UI)
7. **Prometheus** (Metrics collection)
8. **Node-Exporter** (System metrics)
9. **Streamlit** (Unstructured data app)

### ì¶”ê°€ë  ë””ë ‰í† ë¦¬ êµ¬ì¡°
```
lakehouse-tick/
â”œâ”€â”€ docker-compose.yml                 # â† ìˆ˜ì •: 9ê°œ ì„œë¹„ìŠ¤ ì¶”ê°€
â”œâ”€â”€ .env.example                       # â† ì‹ ê·œ: í™˜ê²½ë³€ìˆ˜ í…œí”Œë¦¿
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ prometheus/
â”‚   â”‚   â””â”€â”€ prometheus.yml             # â† ì‹ ê·œ
â”‚   â”œâ”€â”€ superset/
â”‚   â”‚   â””â”€â”€ superset_config.py         # â† ì‹ ê·œ
â”‚   â”œâ”€â”€ grafana/
â”‚   â”‚   â””â”€â”€ provisioning/
â”‚   â”‚       â”œâ”€â”€ datasources/
â”‚   â”‚       â”‚   â”œâ”€â”€ opensearch.yml     # â† ì‹ ê·œ
â”‚   â”‚       â”‚   â””â”€â”€ prometheus.yml     # â† ì‹ ê·œ
â”‚   â”‚       â””â”€â”€ dashboards/
â”‚   â”‚           â””â”€â”€ lakehouse-overview.json  # â† ì‹ ê·œ (ì˜ˆì‹œ)
â”‚   â”œâ”€â”€ opensearch/
â”‚   â”‚   â”œâ”€â”€ opensearch.yml             # â† ì‹ ê·œ
â”‚   â”‚   â””â”€â”€ opensearch_dashboards.yml  # â† ì‹ ê·œ
â”‚   â””â”€â”€ fluentd/
â”‚       â””â”€â”€ fluent.conf                # â† ì‹ ê·œ (ì„ íƒì‚¬í•­)
â”œâ”€â”€ streamlit-app/                     # â† ì‹ ê·œ (Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜)
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ iceberg_connector.py
â”‚   â”‚   â””â”€â”€ s3_utils.py
â”‚   â””â”€â”€ pages/
â”‚       â”œâ”€â”€ 01_Gallery.py
â”‚       â”œâ”€â”€ 02_Search.py
â”‚       â””â”€â”€ 03_Statistics.py
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ superset/                      # â† ì‹ ê·œ (ë¡œê·¸ ë³¼ë¥¨)
â”‚   â”œâ”€â”€ grafana/                       # â† ì‹ ê·œ
â”‚   â”œâ”€â”€ streamlit/                     # â† ì‹ ê·œ
â”‚   â””â”€â”€ opensearch/                    # â† ì‹ ê·œ
â””â”€â”€ scripts/
    â””â”€â”€ setup-visualization.sh         # â† ì‹ ê·œ (ìë™í™” ìŠ¤í¬ë¦½íŠ¸)
```

---

## ğŸ”§ ìƒì„¸ ì½”ë“œ ë³€ê²½ì‚¬í•­

### 1ï¸âƒ£ docker-compose.yml í™•ì¥ (ì•½ 500ì¤„ ì¶”ê°€)

#### A. Superset + PostgreSQL + Redis ì¶”ê°€

```yaml
# ============================================================================
# Visualization Layer: Apache Superset
# ============================================================================

superset-db:
  image: postgres:15
  container_name: superset-db
  environment:
    POSTGRES_DB: superset
    POSTGRES_USER: superset
    POSTGRES_PASSWORD: superset
  volumes:
    - superset-db-data:/var/lib/postgresql/data
  networks:
    - lakehouse-net
  healthcheck:
    test: ["CMD", "pg_isready", "-U", "superset", "-d", "superset"]
    interval: 10s
    timeout: 5s
    retries: 5

superset-redis:
  image: redis:7-alpine
  container_name: superset-redis
  ports:
    - "6380:6379"
  volumes:
    - superset-redis-data:/data
  networks:
    - lakehouse-net
  command: redis-server --maxmemory 256mb --maxmemory-policy allkeys-lru

superset:
  image: apache/superset:latest-dev
  container_name: superset
  depends_on:
    superset-db:
      condition: service_healthy
    superset-redis:
      condition: service_started
    trino:
      condition: service_started
  ports:
    - "8088:8088"
  environment:
    SUPERSET_SECRET_KEY: "${SUPERSET_SECRET_KEY:-CHANGE_THIS_TO_A_RANDOM_SECRET_KEY}"
    SQLALCHEMY_DATABASE_URI: postgresql://superset:superset@superset-db:5432/superset
    REDIS_HOST: superset-redis
    REDIS_PORT: 6379
    SUPERSET_LOAD_EXAMPLES: "no"
    SUPERSET_WEBSERVER_TIMEOUT: 60
    SUPERSET_ROW_LIMIT: 10000
  volumes:
    - superset-data:/app/superset_home
    - ./config/superset/superset_config.py:/app/pythonpath/superset_config.py:ro
    - ./logs/superset:/app/logs
  networks:
    - lakehouse-net
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8088/health"]
    interval: 30s
    timeout: 10s
    retries: 3
    start_period: 60s
  deploy:
    resources:
      limits:
        cpus: '2'
        memory: 4G
      reservations:
        cpus: '1'
        memory: 2G
  command: >
    bash -c "
    superset db upgrade &&
    superset fab create-admin --username admin --firstname Admin --lastname User --email admin@example.com --password admin || true &&
    superset init &&
    gunicorn -w 4 -b 0.0.0.0:8088 --timeout 60 superset.app:create_app()
    "
```

#### B. Grafana + OpenSearch + Prometheus ì¶”ê°€

```yaml
# ============================================================================
# Monitoring Layer: Grafana, OpenSearch, Prometheus
# ============================================================================

opensearch:
  image: opensearchproject/opensearch:2.11.1
  container_name: opensearch
  environment:
    - cluster.name=lakehouse-logs
    - node.name=opensearch-node1
    - discovery.type=single-node
    - bootstrap.memory_lock=true
    - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m"
    - OPENSEARCH_INITIAL_ADMIN_PASSWORD="${OPENSEARCH_PASSWORD:-Admin@123}"
  ulimits:
    memlock:
      soft: -1
      hard: -1
    nofile:
      soft: 65536
      hard: 65536
  ports:
    - "9200:9200"
    - "9600:9600"
  volumes:
    - opensearch-data:/usr/share/opensearch/data
    - ./config/opensearch/opensearch.yml:/usr/share/opensearch/config/opensearch.yml:ro
  networks:
    - lakehouse-net
  healthcheck:
    test: ["CMD", "curl", "-ku", "admin:Admin@123", "https://localhost:9200/_cluster/health"]
    interval: 30s
    timeout: 10s
    retries: 5
    start_period: 60s

opensearch-dashboards:
  image: opensearchproject/opensearch-dashboards:2.11.1
  container_name: opensearch-dashboards
  depends_on:
    opensearch:
      condition: service_healthy
  ports:
    - "5601:5601"
  environment:
    OPENSEARCH_HOSTS: '["https://opensearch:9200"]'
    OPENSEARCH_USERNAME: admin
    OPENSEARCH_PASSWORD: "${OPENSEARCH_PASSWORD:-Admin@123}"
  volumes:
    - ./config/opensearch/opensearch_dashboards.yml:/usr/share/opensearch-dashboards/config/opensearch_dashboards.yml:ro
  networks:
    - lakehouse-net

prometheus:
  image: prom/prometheus:v2.49.0
  container_name: prometheus
  ports:
    - "9090:9090"
  command:
    - '--config.file=/etc/prometheus/prometheus.yml'
    - '--storage.tsdb.path=/prometheus'
    - '--storage.tsdb.retention.time=30d'
  volumes:
    - ./config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    - prometheus-data:/prometheus
  networks:
    - lakehouse-net
  healthcheck:
    test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
    interval: 30s
    timeout: 10s
    retries: 3

node-exporter:
  image: prom/node-exporter:v1.7.0
  container_name: node-exporter
  ports:
    - "9100:9100"
  command:
    - '--path.procfs=/host/proc'
    - '--path.sysfs=/host/sys'
  volumes:
    - /proc:/host/proc:ro
    - /sys:/host/sys:ro
    - /:/rootfs:ro
  networks:
    - lakehouse-net

grafana:
  image: grafana/grafana:10.3.0
  container_name: grafana
  depends_on:
    opensearch:
      condition: service_healthy
    prometheus:
      condition: service_started
  ports:
    - "3000:3000"
  environment:
    GF_SECURITY_ADMIN_USER: admin
    GF_SECURITY_ADMIN_PASSWORD: "${GRAFANA_PASSWORD:-admin}"
    GF_INSTALL_PLUGINS: grafana-opensearch-datasource,grafana-clock-panel
    GF_AUTH_ANONYMOUS_ENABLED: "false"
    GF_SERVER_ROOT_URL: http://localhost:3000
  volumes:
    - grafana-data:/var/lib/grafana
    - ./config/grafana/provisioning:/etc/grafana/provisioning:ro
    - ./logs/grafana:/var/log/grafana
  networks:
    - lakehouse-net
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
    interval: 30s
    timeout: 10s
    retries: 3
  deploy:
    resources:
      limits:
        cpus: '1'
        memory: 2G
```

#### C. Streamlit ì¶”ê°€

```yaml
# ============================================================================
# Application Layer: Streamlit
# ============================================================================

streamlit:
  image: python:3.11-slim
  container_name: streamlit-app
  working_dir: /app
  depends_on:
    hive-metastore:
      condition: service_healthy
    seaweedfs-s3:
      condition: service_healthy
  ports:
    - "8501:8501"
  environment:
    AWS_ACCESS_KEY_ID: seaweedfs_access_key
    AWS_SECRET_ACCESS_KEY: seaweedfs_secret_key
    AWS_ENDPOINT_URL_S3: http://seaweedfs-s3:8333
    AWS_REGION: us-east-1
    HIVE_METASTORE_URI: thrift://hive-metastore:9083
    STREAMLIT_SERVER_PORT: 8501
    STREAMLIT_SERVER_HEADLESS: "true"
    STREAMLIT_BROWSER_GATHER_USAGE_STATS: "false"
  volumes:
    - ./streamlit-app:/app
    - ./logs/streamlit:/app/logs
  networks:
    - lakehouse-net
  command: >
    bash -c "
    pip install --no-cache-dir -r requirements.txt &&
    streamlit run app.py --server.port=8501 --server.headless=true --server.address=0.0.0.0
    "
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8501/_stcore/health"]
    interval: 30s
    timeout: 10s
    retries: 3
    start_period: 30s
  deploy:
    resources:
      limits:
        cpus: '1'
        memory: 2G
```

#### D. Volumes ì¶”ê°€

```yaml
volumes:
  # ê¸°ì¡´
  warehouse:
  postgres-data:
  seaweedfs-data:

  # ì‹ ê·œ
  superset-data:
  superset-db-data:
  superset-redis-data:
  grafana-data:
  opensearch-data:
  prometheus-data:
```

---

### 2ï¸âƒ£ í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ (.env.example ì‹ ê·œ)

```bash
# ============================================================================
# Visualization Stack Configuration
# ============================================================================

# Superset Settings
SUPERSET_SECRET_KEY=your-super-secret-key-change-this-in-production
SUPERSET_ADMIN_USER=admin
SUPERSET_ADMIN_PASSWORD=admin

# Grafana Settings
GRAFANA_PASSWORD=admin

# OpenSearch Settings
OPENSEARCH_PASSWORD=Admin@123

# S3 Settings (SeaweedFS)
AWS_ACCESS_KEY_ID=seaweedfs_access_key
AWS_SECRET_ACCESS_KEY=seaweedfs_secret_key
AWS_ENDPOINT_URL_S3=http://seaweedfs-s3:8333

# Hive Metastore
HIVE_METASTORE_URI=thrift://hive-metastore:9083

# Network
LAKEHOUSE_NETWORK=lakehouse-net

# Storage
LAKEHOUSE_DATA_PATH=/home/iceberg/warehouse
LAKEHOUSE_S3_BUCKET=lakehouse
```

---

### 3ï¸âƒ£ Prometheus ì„¤ì • (config/prometheus/prometheus.yml ì‹ ê·œ)

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    monitor: 'lakehouse-monitor'

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

  - job_name: 'trino'
    static_configs:
      - targets: ['trino:8080']

  - job_name: 'seaweedfs-master'
    static_configs:
      - targets: ['seaweedfs-master:9333']
```

---

### 4ï¸âƒ£ Superset ì„¤ì • (config/superset/superset_config.py ì‹ ê·œ)

```python
# ============================================================================
# Superset Configuration
# ============================================================================

import os
from datetime import timedelta

# Database
SQLALCHEMY_DATABASE_URI = os.getenv(
    'SQLALCHEMY_DATABASE_URI',
    'postgresql://superset:superset@superset-db:5432/superset'
)

# Cache
CACHE_CONFIG = {
    'CACHE_TYPE': 'RedisCache',
    'CACHE_DEFAULT_TIMEOUT': 300,  # 5 minutes
    'CACHE_REDIS_HOST': 'superset-redis',
    'CACHE_REDIS_PORT': 6379,
    'CACHE_REDIS_DB': 0,
}

# Security
SECRET_KEY = os.getenv('SUPERSET_SECRET_KEY', 'change-me-in-production')
SUPERSET_WEBSERVER_TIMEOUT = 60
ROW_LIMIT = 10000

# Features
SUPERSET_FEATURE_FLAGS = {
    'ALLOW_USER_PROFILE_EDIT': True,
    'ENABLE_FORMULA_EDITING': True,
    'ENABLE_EXPLORE_JSON_CSRF_PROTECTION': False,
}

# Logging
LOGGING_CONFIGURATION = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'default': {
            'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        },
    },
    'handlers': {
        'file': {
            'class': 'logging.handlers.RotatingFileHandler',
            'filename': '/app/logs/superset.log',
            'maxBytes': 10485760,  # 10MB
            'backupCount': 5,
            'formatter': 'default',
        },
    },
    'root': {
        'level': 'INFO',
        'handlers': ['file'],
    },
}

# Session
PERMANENT_SESSION_LIFETIME = timedelta(days=7)

# SQL Alchemy
SQLALCHEMY_ECHO = False
SQLALCHEMY_TRACK_MODIFICATIONS = False

# Babel Config for translations
BABEL_DEFAULT_LOCALE = 'en'
LANGUAGES = {
    'en': {'flag': 'us', 'name': 'English'},
    'ko': {'flag': 'kr', 'name': 'í•œêµ­ì–´'},
}
```

---

### 5ï¸âƒ£ OpenSearch ì„¤ì • (config/opensearch/opensearch.yml ì‹ ê·œ)

```yaml
cluster.name: lakehouse-logs
node.name: opensearch-node1

discovery.type: single-node

# Network settings
network.host: 0.0.0.0
http.port: 9200

# Cluster settings
cluster.initial_master_nodes: ["opensearch-node1"]

# Performance
thread_pool.search.queue_size: 1000
thread_pool.bulk.queue_size: 1000

# Security (ensure HTTPS in production)
plugins.security.ssl.http.enabled: true
plugins.security.ssl.http.pemcert_filepath: certs/node1.pem
plugins.security.ssl.http.pemkey_filepath: certs/node1-key.pem
plugins.security.ssl.http.pemtrustedcas_filepath: certs/root-ca.pem
plugins.security.ssl.http.enforce_hostname_verification: false

plugins.security.ssl.transport.pemcert_filepath: certs/node1.pem
plugins.security.ssl.transport.pemkey_filepath: certs/node1-key.pem
plugins.security.ssl.transport.pemtrustedcas_filepath: certs/root-ca.pem
plugins.security.ssl.transport.enforce_hostname_verification: false

# Admin credentials (INTERNAL USE ONLY - NEVER COMMIT)
plugins.security.authcz.admin_dn:
  - "CN=admin,O=Example Com,ST=London,C=UK"

plugins.security.nodes_dn:
  - "CN=opensearch-node1,O=Example Com,ST=London,C=UK"

# Allow anonymous access for development (DISABLE IN PRODUCTION)
plugins.security.allow_default_init: true
```

---

### 6ï¸âƒ£ OpenSearch Dashboards ì„¤ì • (config/opensearch/opensearch_dashboards.yml ì‹ ê·œ)

```yaml
server.port: 5601
server.host: "0.0.0.0"

opensearch.hosts: ["https://opensearch:9200"]
opensearch.username: "admin"
opensearch.password: "Admin@123"

opensearch.ssl.verificationMode: "none"

opensearchDashboards.defaultAppId: "home"

logging.dest: /var/log/opensearch-dashboards/opensearch-dashboards.log
logging.verbose: false
```

---

### 7ï¸âƒ£ Grafana Provisioning ì„¤ì •

#### config/grafana/provisioning/datasources/opensearch.yml

```yaml
apiVersion: 1

datasources:
  - name: OpenSearch
    type: grafana-opensearch-datasource
    access: proxy
    url: https://opensearch:9200
    basicAuth: true
    basicAuthUser: admin
    basicAuthPassword: Admin@123
    isDefault: false
    jsonData:
      tlsSkipVerify: true
      logMessageField: message
      logLevelField: level
      esVersion: "7.10.0"
```

#### config/grafana/provisioning/datasources/prometheus.yml

```yaml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: true
```

---

### 8ï¸âƒ£ Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜ (streamlit-app/ ì‹ ê·œ)

#### streamlit-app/requirements.txt

```
streamlit==1.30.0
pyiceberg==0.5.1
pandas==2.1.4
boto3==1.34.0
Pillow==10.1.0
pyarrow==14.0.0
trino==0.22.0
```

#### streamlit-app/app.py

```python
import streamlit as st
import pandas as pd
from pathlib import Path
import sys

# Add modules to path
sys.path.insert(0, str(Path(__file__).parent))

from modules.iceberg_connector import get_iceberg_table
from modules.s3_utils import get_s3_client

st.set_page_config(
    page_title="Unstructured Data Explorer",
    page_icon="ğŸ–¼ï¸",
    layout="wide"
)

st.title("ğŸ–¼ï¸ Lakehouse Unstructured Data Explorer")
st.markdown("---")

# Navigation
st.sidebar.header("Navigation")
st.sidebar.markdown("""
- **Gallery**: Browse images with filtering
- **Search**: Find images by metadata
- **Statistics**: View data insights
""")

st.sidebar.markdown("---")

# Status checks
st.sidebar.header("System Status")
try:
    table = get_iceberg_table("hive_prod.media_db.image_metadata")
    st.sidebar.success("âœ… Iceberg connected")
except Exception as e:
    st.sidebar.error(f"âŒ Iceberg error: {e}")

try:
    s3_client = get_s3_client()
    s3_client.list_buckets()
    st.sidebar.success("âœ… S3 connected")
except Exception as e:
    st.sidebar.error(f"âŒ S3 error: {e}")

st.markdown("""
Welcome to the Lakehouse Unstructured Data Explorer!

This application provides access to images and unstructured data stored in the lakehouse.
Use the sidebar to navigate to different features.
""")
```

#### streamlit-app/modules/iceberg_connector.py

```python
"""
Iceberg Catalog Connector Module
"""
import os
from pyiceberg.catalog import load_catalog

def get_iceberg_table(table_name: str):
    """
    Load an Iceberg table from the catalog

    Args:
        table_name: Table name in format 'catalog.database.table'

    Returns:
        pyiceberg.table.Table object

    Raises:
        Exception: If catalog connection fails
    """
    try:
        catalog = load_catalog("default", **{
            "type": "hive",
            "uri": os.getenv("HIVE_METASTORE_URI", "thrift://hive-metastore:9083"),
            "s3.endpoint": os.getenv("AWS_ENDPOINT_URL_S3", "http://seaweedfs-s3:8333"),
            "s3.access-key-id": os.getenv("AWS_ACCESS_KEY_ID", "seaweedfs_access_key"),
            "s3.secret-access-key": os.getenv("AWS_SECRET_ACCESS_KEY", "seaweedfs_secret_key"),
            "s3.path-style-access": "true"
        })

        return catalog.load_table(table_name)
    except Exception as e:
        raise Exception(f"Failed to load table '{table_name}': {str(e)}")
```

#### streamlit-app/modules/s3_utils.py

```python
"""
S3/SeaweedFS Utility Module
"""
import boto3
import os

def get_s3_client():
    """
    Create a boto3 S3 client configured for SeaweedFS

    Returns:
        boto3.client: S3 client instance
    """
    return boto3.client(
        's3',
        endpoint_url=os.getenv('AWS_ENDPOINT_URL_S3', 'http://seaweedfs-s3:8333'),
        aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID', 'seaweedfs_access_key'),
        aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY', 'seaweedfs_secret_key'),
        region_name=os.getenv('AWS_REGION', 'us-east-1'),
        verify=False  # SeaweedFS uses self-signed certs
    )

def list_images(bucket: str, prefix: str = "raw/images/"):
    """
    List all images in S3 bucket

    Args:
        bucket: S3 bucket name
        prefix: S3 key prefix

    Returns:
        List of image keys
    """
    client = get_s3_client()
    response = client.list_objects_v2(Bucket=bucket, Prefix=prefix)

    if 'Contents' not in response:
        return []

    return [obj['Key'] for obj in response['Contents']]

def download_image(bucket: str, key: str) -> bytes:
    """
    Download image from S3

    Args:
        bucket: S3 bucket name
        key: S3 object key

    Returns:
        Image bytes
    """
    client = get_s3_client()
    response = client.get_object(Bucket=bucket, Key=key)
    return response['Body'].read()
```

#### streamlit-app/pages/01_Gallery.py

```python
import streamlit as st
import pandas as pd
from modules.iceberg_connector import get_iceberg_table
from modules.s3_utils import get_s3_client, download_image

st.set_page_config(page_title="Gallery", page_icon="ğŸ–¼ï¸", layout="wide")
st.title("ğŸ–¼ï¸ Image Gallery")

# Sidebar filters
st.sidebar.header("Filters")

tag_options = ['all', 'product', 'user', 'analytics']
selected_tag = st.sidebar.selectbox("Tag", tag_options)

date_range = st.sidebar.date_input("Upload Date Range", [])

min_size = st.sidebar.number_input("Min Size (KB)", 0, value=0)
max_size = st.sidebar.number_input("Max Size (KB)", 0, value=100000)

# Load metadata
@st.cache_data(ttl=300)
def load_metadata(tag, date_range, min_size, max_size):
    try:
        table = get_iceberg_table("hive_prod.media_db.image_metadata")
        df = table.scan().to_pandas()

        if tag != 'all':
            df = df[df['tag'] == tag]

        if len(date_range) == 2:
            df = df[(df['upload_time'] >= pd.Timestamp(date_range[0])) &
                    (df['upload_time'] <= pd.Timestamp(date_range[1]))]

        df = df[(df['file_size'] >= min_size * 1024) &
                (df['file_size'] <= max_size * 1024)]

        return df
    except Exception as e:
        st.error(f"Failed to load metadata: {e}")
        return pd.DataFrame()

df = load_metadata(selected_tag, date_range, min_size, max_size)

# Statistics
col1, col2, col3, col4 = st.columns(4)
with col1:
    st.metric("Total Images", len(df))
with col2:
    total_size_mb = df['file_size'].sum() / 1024 / 1024 if len(df) > 0 else 0
    st.metric("Total Size (MB)", f"{total_size_mb:.2f}")
with col3:
    avg_size_kb = df['file_size'].mean() / 1024 if len(df) > 0 else 0
    st.metric("Avg Size (KB)", f"{avg_size_kb:.2f}")
with col4:
    st.metric("Unique Tags", df['tag'].nunique() if len(df) > 0 else 0)

st.markdown("---")

# Gallery
if len(df) > 0:
    st.subheader("Images")
    cols = st.columns(4)
    s3_client = get_s3_client()

    for idx, (_, row) in enumerate(df.iterrows()):
        col = cols[idx % 4]

        with col:
            try:
                s3_path = row['s3_path'].replace('s3a://', '')
                bucket, key = s3_path.split('/', 1)

                image_bytes = download_image(bucket, key)
                st.image(image_bytes, caption=row['image_id'], use_container_width=True)

                with st.expander("Metadata"):
                    st.json({
                        "ID": row['image_id'],
                        "Size": f"{row['file_size'] / 1024:.2f} KB",
                        "Type": row['mime_type'],
                        "Dimensions": f"{row['width']}x{row['height']}" if 'width' in row else "N/A",
                        "Upload Time": str(row['upload_time']),
                        "Tag": row['tag']
                    })
            except Exception as e:
                st.error(f"Failed to load {row['image_id']}: {e}")
else:
    st.info("No images found matching your filters")

# Data table
with st.expander("View Metadata Table"):
    st.dataframe(df, use_container_width=True)
```

#### streamlit-app/pages/02_Search.py

```python
import streamlit as st
import pandas as pd
from modules.iceberg_connector import get_iceberg_table

st.set_page_config(page_title="Search", page_icon="ğŸ”", layout="wide")
st.title("ğŸ” Metadata Search")

# Search input
search_field = st.selectbox("Search By", ["image_id", "tag", "source_system"])
search_query = st.text_input("Search Query")

if search_query:
    @st.cache_data(ttl=300)
    def search_metadata(field, query):
        try:
            table = get_iceberg_table("hive_prod.media_db.image_metadata")
            df = table.scan().to_pandas()
            return df[df[field].astype(str).str.contains(query, case=False)]
        except Exception as e:
            st.error(f"Search failed: {e}")
            return pd.DataFrame()

    results = search_metadata(search_field, search_query)

    st.metric("Results Found", len(results))
    st.dataframe(results, use_container_width=True)
else:
    st.info("Enter a search query to begin")
```

#### streamlit-app/pages/03_Statistics.py

```python
import streamlit as st
import pandas as pd
from modules.iceberg_connector import get_iceberg_table
import plotly.express as px

st.set_page_config(page_title="Statistics", page_icon="ğŸ“Š", layout="wide")
st.title("ğŸ“Š Statistics Dashboard")

@st.cache_data(ttl=300)
def load_stats():
    try:
        table = get_iceberg_table("hive_prod.media_db.image_metadata")
        return table.scan().to_pandas()
    except Exception as e:
        st.error(f"Failed to load statistics: {e}")
        return pd.DataFrame()

df = load_stats()

if len(df) > 0:
    col1, col2 = st.columns(2)

    with col1:
        st.subheader("Images by Tag")
        tag_counts = df['tag'].value_counts()
        fig = px.bar(tag_counts, title="Count by Tag")
        st.plotly_chart(fig, use_container_width=True)

    with col2:
        st.subheader("File Size Distribution")
        fig = px.histogram(df, x='file_size', nbins=20, title="File Size Distribution (bytes)")
        st.plotly_chart(fig, use_container_width=True)

    st.subheader("Upload Timeline")
    df['upload_date'] = pd.to_datetime(df['upload_time']).dt.date
    timeline = df.groupby('upload_date').size()
    fig = px.line(timeline, title="Images Uploaded Over Time")
    st.plotly_chart(fig, use_container_width=True)
else:
    st.info("No data available")
```

---

### 9ï¸âƒ£ ì„¤ì • ë””ë ‰í† ë¦¬ êµ¬ì¡° ì •ë¦¬

```bash
# ìƒì„±í•  ë””ë ‰í† ë¦¬ë“¤
mkdir -p config/prometheus
mkdir -p config/superset
mkdir -p config/grafana/provisioning/{datasources,dashboards}
mkdir -p config/opensearch
mkdir -p streamlit-app/{modules,pages}
mkdir -p logs/{superset,grafana,streamlit,opensearch}
mkdir -p scripts
```

---

### ğŸ”Ÿ ìë™í™” ì„¤ì • ìŠ¤í¬ë¦½íŠ¸ (scripts/setup-visualization.sh ì‹ ê·œ)

```bash
#!/bin/bash

# ============================================================================
# Visualization Stack Setup Script
# ============================================================================

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"

echo "ğŸ“Š Setting up Lakehouse Visualization Stack..."

# 1. Create directories
echo "ğŸ“ Creating directories..."
mkdir -p "$PROJECT_ROOT/config/prometheus"
mkdir -p "$PROJECT_ROOT/config/superset"
mkdir -p "$PROJECT_ROOT/config/grafana/provisioning/{datasources,dashboards}"
mkdir -p "$PROJECT_ROOT/config/opensearch"
mkdir -p "$PROJECT_ROOT/streamlit-app/{modules,pages}"
mkdir -p "$PROJECT_ROOT/logs/{superset,grafana,streamlit,opensearch}"

# 2. Create .env from example if not exists
echo "ğŸ”§ Checking environment configuration..."
if [ ! -f "$PROJECT_ROOT/.env" ]; then
    cp "$PROJECT_ROOT/.env.example" "$PROJECT_ROOT/.env"
    echo "âœ… Created .env from template (edit with your values)"
else
    echo "âœ… .env already exists"
fi

# 3. Start services
echo "ğŸš€ Starting visualization services..."
cd "$PROJECT_ROOT"
docker-compose up -d superset-db superset-redis superset
docker-compose up -d opensearch opensearch-dashboards grafana prometheus node-exporter
docker-compose up -d streamlit

# 4. Wait for services
echo "â³ Waiting for services to be healthy..."
max_attempts=30
attempt=0

while [ $attempt -lt $max_attempts ]; do
    if curl -s http://localhost:8088/health > /dev/null 2>&1; then
        echo "âœ… Superset ready"
        break
    fi
    echo "â³ Waiting for Superset... ($((attempt+1))/$max_attempts)"
    sleep 5
    attempt=$((attempt+1))
done

# 5. Print status
echo ""
echo "=========================================="
echo "âœ… Visualization Stack Setup Complete!"
echo "=========================================="
echo ""
echo "ğŸ“Š Service URLs:"
echo "  - Superset:            http://localhost:8088 (admin/admin)"
echo "  - Grafana:             http://localhost:3000 (admin/admin)"
echo "  - OpenSearch Dashboards: http://localhost:5601 (admin/Admin@123)"
echo "  - Streamlit:           http://localhost:8501"
echo "  - Prometheus:          http://localhost:9090"
echo "  - Trino UI:            http://localhost:8080/ui"
echo ""
echo "ğŸ“ Next steps:"
echo "  1. Configure Trino data source in Superset"
echo "  2. Create dashboards in Superset"
echo "  3. Add OpenSearch data source to Grafana"
echo "  4. Upload sample images for Streamlit"
echo ""
```

---

## ğŸ“ ë³€ê²½ì‚¬í•­ ìš”ì•½í‘œ

| í•­ëª© | íŒŒì¼/ë””ë ‰í† ë¦¬ | ë³€ê²½ ìœ í˜• | ë¼ì¸ ìˆ˜ | ì„¤ëª… |
|------|-------------|----------|--------|------|
| **docker-compose.yml** | ë£¨íŠ¸ | ìˆ˜ì • | +500 | 9ê°œ ì‹ ê·œ ì„œë¹„ìŠ¤ ì¶”ê°€ |
| **.env.example** | ë£¨íŠ¸ | ì‹ ê·œ | 30 | í™˜ê²½ ë³€ìˆ˜ í…œí”Œë¦¿ |
| **prometheus.yml** | config/prometheus/ | ì‹ ê·œ | 40 | ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì„¤ì • |
| **superset_config.py** | config/superset/ | ì‹ ê·œ | 70 | ìºì‹œ, ë³´ì•ˆ, ë¡œê¹… ì„¤ì • |
| **opensearch.yml** | config/opensearch/ | ì‹ ê·œ | 40 | í´ëŸ¬ìŠ¤í„° ë° ë³´ì•ˆ ì„¤ì • |
| **opensearch_dashboards.yml** | config/opensearch/ | ì‹ ê·œ | 15 | OpenSearch Dashboards ì„¤ì • |
| **datasources/*.yml** | config/grafana/provisioning/ | ì‹ ê·œ | 30 | Grafana ë°ì´í„° ì†ŒìŠ¤ ìë™ ì„¤ì • |
| **app.py** | streamlit-app/ | ì‹ ê·œ | 60 | Streamlit ë©”ì¸ ì•± |
| **pages/01_Gallery.py** | streamlit-app/pages/ | ì‹ ê·œ | 100 | ì´ë¯¸ì§€ ê°¤ëŸ¬ë¦¬ í˜ì´ì§€ |
| **pages/02_Search.py** | streamlit-app/pages/ | ì‹ ê·œ | 40 | ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ í˜ì´ì§€ |
| **pages/03_Statistics.py** | streamlit-app/pages/ | ì‹ ê·œ | 50 | í†µê³„ ëŒ€ì‹œë³´ë“œ í˜ì´ì§€ |
| **iceberg_connector.py** | streamlit-app/modules/ | ì‹ ê·œ | 40 | Iceberg ì—°ê²° ëª¨ë“ˆ |
| **s3_utils.py** | streamlit-app/modules/ | ì‹ ê·œ | 60 | S3 ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ |
| **requirements.txt** | streamlit-app/ | ì‹ ê·œ | 10 | Python ì˜ì¡´ì„± |
| **setup-visualization.sh** | scripts/ | ì‹ ê·œ | 80 | ìë™í™” ì„¤ì • ìŠ¤í¬ë¦½íŠ¸ |
| **ë¡œê·¸ ë””ë ‰í† ë¦¬** | logs/ | ì‹ ê·œ | - | 4ê°œ ì„œë¹„ìŠ¤ ë¡œê·¸ ì €ì¥ì†Œ |
| **TOTAL** | - | - | **~1,120ì¤„** | - |

---

## ğŸ¯ êµ¬í˜„ ìˆœì„œ

### Phase 1: ì„¤ì • íŒŒì¼ ìƒì„± (30ë¶„)
1. docker-compose.yml í™•ì¥
2. .env.example ìƒì„±
3. config/ ë””ë ‰í† ë¦¬ í•˜ìœ„ ì„¤ì • íŒŒì¼ ìƒì„±

### Phase 2: ì„œë¹„ìŠ¤ ì‹œì‘ (15ë¶„)
```bash
docker-compose up -d superset-db superset-redis superset
docker-compose up -d opensearch opensearch-dashboards grafana prometheus node-exporter
docker-compose up -d streamlit
```

### Phase 3: Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬ (1ì‹œê°„)
1. streamlit-app/ ë””ë ‰í† ë¦¬ ìƒì„±
2. Python íŒŒì¼ ìƒì„± (app.py, modules/*, pages/*)
3. requirements.txt ì„¤ì •
4. ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘

### Phase 4: ë°ì´í„° ì¤€ë¹„ (30ë¶„)
1. Iceberg í…Œì´ë¸” ìƒì„± (image_metadata)
2. ìƒ˜í”Œ ì´ë¯¸ì§€ ì—…ë¡œë“œ
3. Trinoì—ì„œ ë©”íƒ€ë°ì´í„° ê²€ì¦

### Phase 5: ëŒ€ì‹œë³´ë“œ ì„¤ì • (2ì‹œê°„)
1. Supersetì—ì„œ Trino ë°ì´í„° ì†ŒìŠ¤ ì—°ê²°
2. Superset ëŒ€ì‹œë³´ë“œ ìƒì„±
3. Grafana ë°ì´í„° ì†ŒìŠ¤ ì—°ê²°
4. Grafana ëŒ€ì‹œë³´ë“œ ìƒì„±

---

## âœ… ë°°í¬ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] docker-compose.yml í™•ì¥ ì™„ë£Œ
- [ ] .env íŒŒì¼ ì„¤ì • ì™„ë£Œ
- [ ] config/ í•˜ìœ„ ì„¤ì • íŒŒì¼ ìƒì„± ì™„ë£Œ
- [ ] streamlit-app/ Python ì½”ë“œ ì‘ì„± ì™„ë£Œ
- [ ] ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„± ì™„ë£Œ
- [ ] Docker ì´ë¯¸ì§€ ì‚¬ìš© ê°€ëŠ¥ í™•ì¸ (offline í™˜ê²½ì¸ ê²½ìš°)
- [ ] ë„¤íŠ¸ì›Œí¬ í¬íŠ¸ ì¶©ëŒ í™•ì¸ (8088, 3000, 8501, 9200, 9090 ë“±)
- [ ] ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ (ìµœì†Œ 20GB)
- [ ] ë©”ëª¨ë¦¬ ì—¬ìœ  í™•ì¸ (ìµœì†Œ 8GB ê¶Œì¥)

---

## ğŸ“ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### "No space left on device" ì˜¤ë¥˜
```bash
# ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ í™•ì¸
docker system df

# ë¶ˆí•„ìš”í•œ ì´ë¯¸ì§€/ì»¨í…Œì´ë„ˆ/ë³¼ë¥¨ ì •ë¦¬
docker system prune -a --volumes
```

### Superset ì»¨í…Œì´ë„ˆê°€ ì‹œì‘ë˜ì§€ ì•ŠìŒ
```bash
# ë¡œê·¸ í™•ì¸
docker logs superset

# Redis ì—°ê²° í™•ì¸
docker logs superset-redis

# PostgreSQL ì—°ê²° í™•ì¸
docker logs superset-db
```

### Streamlitì—ì„œ Iceberg ì—°ê²° ì‹¤íŒ¨
```bash
# Hive Metastore ìƒíƒœ í™•ì¸
docker logs hive-metastore

# Streamlit í™˜ê²½ ë³€ìˆ˜ í™•ì¸
docker exec streamlit-app env | grep HIVE
docker exec streamlit-app env | grep AWS
```

---

ì´ ë¬¸ì„œëŠ” **ë¬¸ì„œí™” ì™„ë£Œ ìƒíƒœ**ì—ì„œ **ì‹¤ì œ êµ¬í˜„**ìœ¼ë¡œ ì „í™˜í•˜ëŠ” ë° í•„ìš”í•œ ëª¨ë“  ì½”ë“œì™€ ì„¤ì •ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

**ë‹¤ìŒ ë‹¨ê³„**:
1. ì´ ë¬¸ì„œì˜ ì½”ë“œë¥¼ ì‹¤ì œ íŒŒì¼ë¡œ ìƒì„±
2. docker-compose.yml ìˆ˜ì •
3. Docker ì»¨í…Œì´ë„ˆ ì‹œì‘
4. ê° ì„œë¹„ìŠ¤ì— ì ‘ì†í•˜ì—¬ ì„¤ì • ì™„ë£Œ

