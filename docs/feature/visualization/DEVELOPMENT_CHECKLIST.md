# âœ… ì‹œê°í™” ìŠ¤íƒ ê°œë°œ ì²´í¬ë¦¬ìŠ¤íŠ¸ (Development Checklist)

> **ìš©ë„**: ê°œë°œ ë° ë°°í¬ì‹œ ì´ íŒŒì¼ í•˜ë‚˜ë§Œ ì°¸ê³ í•˜ë©´ ë©ë‹ˆë‹¤.
> **ìƒíƒœ**: ğŸ¯ ì‹¤í–‰ ê°€ëŠ¥í•œ ëª¨ë“  í•­ëª© í¬í•¨

---

## ğŸ“‹ ì‚¬ìš© ê°€ì´ë“œ

### ì²´í¬ë¦¬ìŠ¤íŠ¸ ì§„í–‰ ë°©ì‹
```
[ ]  ë¯¸ì™„ë£Œ
[x]  ì™„ë£Œ
[~]  ì§„í–‰ ì¤‘
```

### ì°¸ê³  ìë£Œ
- **ê°œìš”**: README.md
- **ë¹ ë¥¸ ì°¸ì¡°**: QUICK_REFERENCE.md
- **ìƒì„¸ êµ¬í˜„**: 01-tier1.md, 02-tier2.md, 03-tier3.md
- **ì½”ë“œ ì˜ˆì‹œ**: VISUALIZATION_STACK_CODE_CHANGES.md

---

## ğŸ¯ Phase 0: ì‚¬ì „ ì¤€ë¹„ (30ë¶„)

### 0.1 í™˜ê²½ í™•ì¸
- [ ] Docker ì„¤ì¹˜ í™•ì¸ (`docker --version`)
- [ ] Docker Compose ì„¤ì¹˜ í™•ì¸ (`docker-compose --version`)
- [ ] ìµœì†Œ 8GB ë©”ëª¨ë¦¬ í™•ì¸
- [ ] 50GB ë””ìŠ¤í¬ ì—¬ìœ  í™•ì¸
- [ ] í¬íŠ¸ ì¶©ëŒ í™•ì¸ (8088, 3000, 8501, 9200, 9090 ë“±)

### 0.2 íŒŒì¼ ì¤€ë¹„
- [ ] ë£¨íŠ¸ ë””ë ‰í† ë¦¬ í™•ì¸: `/home/i/work/ai/lakehouse-tick/`
- [ ] `docker-compose.yml` ë°±ì—… ìƒì„±
- [ ] `config/` ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ì¸

---

## ğŸ—ï¸ Phase 1: Docker Compose ìˆ˜ì • (1ì‹œê°„)

### 1.0 ê³µí†µ ë³´ì™„
- [ ] ì´ë¯¸ì§€ íƒœê·¸ ê³ ì • (latest-dev ì§€ì–‘)
- [ ] ì„œë¹„ìŠ¤ë³„ ë¦¬ì†ŒìŠ¤ ì œí•œ/ì˜ˆì•½ì¹˜ ì„¤ì • (opensearch, trino, superset ë“±)
- [x] `.env` ë³€ìˆ˜ ì‚¬ìš©ì„ ìœ„í•œ env_file ë˜ëŠ” ë³€ìˆ˜ ì¹˜í™˜ ì ìš©

### 1.1 Superset ìŠ¤íƒ ì¶”ê°€

#### A. PostgreSQL (Superset ë©”íƒ€ìŠ¤í† ì–´)
```yaml
superset-db:
  image: postgres:15
  container_name: superset-db
  environment:
    POSTGRES_DB: superset
    POSTGRES_USER: superset
    POSTGRES_PASSWORD: superset
  volumes:
    - superset-db-data:/var/lib/postgresql/data
  networks:
    - lakehouse-net
  healthcheck:
    test: ["CMD", "pg_isready", "-U", "superset", "-d", "superset"]
    interval: 10s
    timeout: 5s
    retries: 5
```

- [x] `superset-db` ì„œë¹„ìŠ¤ ì¶”ê°€
- [x] í¬íŠ¸ ì„¤ì • (5432 â†’ 5432)
- [x] ë³¼ë¥¨ ìƒì„±: `superset-db-data`
- [x] healthcheck ì„¤ì •

#### B. Redis (ìºì‹œ)
```yaml
superset-redis:
  image: redis:7-alpine
  container_name: superset-redis
  ports:
    - "6380:6379"
  volumes:
    - superset-redis-data:/data
  networks:
    - lakehouse-net
  command: redis-server --maxmemory 256mb --maxmemory-policy allkeys-lru
```

- [x] `superset-redis` ì„œë¹„ìŠ¤ ì¶”ê°€
- [x] í¬íŠ¸ ì„¤ì • (6380:6379)
- [x] ë³¼ë¥¨ ìƒì„±: `superset-redis-data`
- [x] ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •

#### C. Superset (BI ëŒ€ì‹œë³´ë“œ)
```yaml
superset:
  image: apache/superset:latest-dev
  container_name: superset
  depends_on:
    superset-db:
      condition: service_healthy
    superset-redis:
      condition: service_started
    trino:
      condition: service_started
  ports:
    - "8088:8088"
  env_file:
    - .env
  environment:
    SUPERSET_SECRET_KEY: ${SUPERSET_SECRET_KEY}
    SQLALCHEMY_DATABASE_URI: postgresql://superset:superset@superset-db:5432/superset
    REDIS_HOST: superset-redis
    REDIS_PORT: 6379
    SUPERSET_ADMIN_USER: ${SUPERSET_ADMIN_USER}
    SUPERSET_ADMIN_PASSWORD: ${SUPERSET_ADMIN_PASSWORD}
  volumes:
    - superset-data:/app/superset_home
    - ./config/superset/superset_config.py:/app/pythonpath/superset_config.py:ro
    - ./logs/superset:/app/logs
  networks:
    - lakehouse-net
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8088/health"]
    interval: 30s
    timeout: 10s
    retries: 3
    start_period: 60s
  command: >
    bash -c "
    pip install --no-cache-dir 'trino[sqlalchemy]' &&
    superset db upgrade &&
    superset fab create-admin --username ${SUPERSET_ADMIN_USER} --firstname Admin --lastname User --email admin@example.com --password ${SUPERSET_ADMIN_PASSWORD} || true &&
    superset init &&
    gunicorn -w 4 -b 0.0.0.0:8088 --timeout 60 superset.app:create_app()
    "
```

- [x] `superset` ì„œë¹„ìŠ¤ ì¶”ê°€
- [x] í¬íŠ¸ ì„¤ì • (8088:8088)
- [x] í™˜ê²½ë³€ìˆ˜ ì„¤ì • (SECRET_KEY, DATABASE_URI, REDIS)
- [x] ë³¼ë¥¨ ìƒì„±: `superset-data`
- [x] `superset_config.py` ë§ˆìš´íŠ¸ ê²½ë¡œ í™•ì¸
- [~] Trino SQLAlchemy ë“œë¼ì´ë²„ ì„¤ì¹˜ í™•ì¸ (ê°œë°œ: command, ìš´ì˜: ì»¤ìŠ¤í…€ ì´ë¯¸ì§€ ê¶Œì¥)
- [x] healthcheck ì„¤ì •
- [x] ì´ˆê¸° admin ê³„ì • ìë™ ìƒì„± command ì„¤ì •

### 1.2 Grafana + OpenSearch ìŠ¤íƒ ì¶”ê°€

#### A. OpenSearch (ë¡œê·¸ ì €ì¥ì†Œ)
```yaml
opensearch:
  image: opensearchproject/opensearch:2.11.1
  container_name: opensearch
  environment:
    - cluster.name=lakehouse-logs
    - node.name=opensearch-node1
    - discovery.type=single-node
    - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m"
    - OPENSEARCH_INITIAL_ADMIN_PASSWORD=${OPENSEARCH_PASSWORD}
  ports:
    - "9200:9200"
    - "9600:9600"
  volumes:
    - opensearch-data:/usr/share/opensearch/data
    - ./config/opensearch/opensearch.yml:/usr/share/opensearch/config/opensearch.yml:ro
  networks:
    - lakehouse-net
  healthcheck:
    test: ["CMD", "curl", "-ku", "admin:${OPENSEARCH_PASSWORD}", "https://localhost:9200/_cluster/health"]
    interval: 30s
    timeout: 10s
    retries: 5
    start_period: 60s
```

- [x] `opensearch` ì„œë¹„ìŠ¤ ì¶”ê°€
- [x] í¬íŠ¸ ì„¤ì • (9200, 9600)
- [x] í™˜ê²½ë³€ìˆ˜ ì„¤ì • (admin ë¹„ë°€ë²ˆí˜¸)
- [x] ë³¼ë¥¨ ìƒì„±: `opensearch-data`
- [x] `opensearch.yml` ë§ˆìš´íŠ¸ ê²½ë¡œ í™•ì¸
- [x] healthcheck ì„¤ì •
- [x] JVM ë©”ëª¨ë¦¬ ì„¤ì •

#### B. OpenSearch Dashboards (ë¡œê·¸ UI)
```yaml
opensearch-dashboards:
  image: opensearchproject/opensearch-dashboards:2.11.1
  container_name: opensearch-dashboards
  depends_on:
    opensearch:
      condition: service_healthy
  ports:
    - "5601:5601"
  environment:
    OPENSEARCH_HOSTS: '["https://opensearch:9200"]'
    OPENSEARCH_USERNAME: admin
    OPENSEARCH_PASSWORD: ${OPENSEARCH_PASSWORD}
    OPENSEARCH_SSL_VERIFICATIONMODE: none
  networks:
    - lakehouse-net
```

- [x] `opensearch-dashboards` ì„œë¹„ìŠ¤ ì¶”ê°€
- [x] í¬íŠ¸ ì„¤ì • (5601:5601)
- [x] í™˜ê²½ë³€ìˆ˜ ì„¤ì •
- [x] SSL ê²€ì¦ ì„¤ì • í™•ì¸ (self-signed í™˜ê²½)

#### C. Prometheus (ë©”íŠ¸ë¦­ ìˆ˜ì§‘)
```yaml
prometheus:
  image: prom/prometheus:v2.49.0
  container_name: prometheus
  ports:
    - "9090:9090"
  command:
    - '--config.file=/etc/prometheus/prometheus.yml'
    - '--storage.tsdb.path=/prometheus'
    - '--storage.tsdb.retention.time=30d'
  volumes:
    - ./config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    - prometheus-data:/prometheus
  networks:
    - lakehouse-net
  healthcheck:
    test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
    interval: 30s
    timeout: 10s
    retries: 3
```

- [x] `prometheus` ì„œë¹„ìŠ¤ ì¶”ê°€
- [x] í¬íŠ¸ ì„¤ì • (9090:9090)
- [x] ì„¤ì • íŒŒì¼ ë§ˆìš´íŠ¸ í™•ì¸
- [ ] Trino ë©”íŠ¸ë¦­ ë…¸ì¶œ(JMX/Exporter) ì„¤ì • í™•ì¸
- [x] ë³¼ë¥¨ ìƒì„±: `prometheus-data`
- [x] healthcheck ì„¤ì •

#### D. Node Exporter (ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­)
```yaml
node-exporter:
  image: prom/node-exporter:v1.7.0
  container_name: node-exporter
  ports:
    - "9100:9100"
  command:
    - '--path.procfs=/host/proc'
    - '--path.sysfs=/host/sys'
  volumes:
    - /proc:/host/proc:ro
    - /sys:/host/sys:ro
    - /:/rootfs:ro
  networks:
    - lakehouse-net
```

- [x] `node-exporter` ì„œë¹„ìŠ¤ ì¶”ê°€
- [x] í¬íŠ¸ ì„¤ì • (9100:9100)
- [x] ë³¼ë¥¨ ë§ˆìš´íŠ¸ ì„¤ì •

#### E. Grafana (ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ)
```yaml
grafana:
  image: grafana/grafana:10.3.0
  container_name: grafana
  depends_on:
    opensearch:
      condition: service_healthy
    prometheus:
      condition: service_started
  ports:
    - "3000:3000"
  env_file:
    - .env
  environment:
    GF_SECURITY_ADMIN_USER: admin
    GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD}
    GF_INSTALL_PLUGINS: grafana-opensearch-datasource,grafana-clock-panel
    GF_AUTH_ANONYMOUS_ENABLED: "false"
  volumes:
    - grafana-data:/var/lib/grafana
    - ./config/grafana/provisioning:/etc/grafana/provisioning:ro
    - ./logs/grafana:/var/log/grafana
  networks:
    - lakehouse-net
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
    interval: 30s
    timeout: 10s
    retries: 3
```

- [x] `grafana` ì„œë¹„ìŠ¤ ì¶”ê°€
- [x] í¬íŠ¸ ì„¤ì • (3000:3000)
- [x] í™˜ê²½ë³€ìˆ˜ ì„¤ì • (admin ê³„ì •)
- [~] OPENSEARCH_PASSWORD í™˜ê²½ë³€ìˆ˜ ì „ë‹¬ í™•ì¸ (provisioningì—ì„œ ì‚¬ìš©)
- [x] í”ŒëŸ¬ê·¸ì¸ ì„¤ì¹˜ ì„¤ì •
- [x] í”„ë¡œë¹„ì €ë‹ ë³¼ë¥¨ ë§ˆìš´íŠ¸
- [x] ë³¼ë¥¨ ìƒì„±: `grafana-data`
- [x] healthcheck ì„¤ì •

#### F. ë¡œê·¸ ìˆ˜ì§‘ ì—ì´ì „íŠ¸ (ì„ íƒ)
- [ ] Fluent Bit ë˜ëŠ” Vector ì¶”ê°€ (Docker ë¡œê·¸/íŒŒì¼ ë¡œê·¸ë¥¼ OpenSearchë¡œ ì „ì†¡)
- [ ] ì¸ë±ìŠ¤ íŒ¨í„´/ë³´ì¡´ ì •ì±… ì •ì˜ (ì˜ˆ: `logs-*`)

### 1.3 Streamlit ì¶”ê°€

```yaml
streamlit:
  image: python:3.11-slim
  container_name: streamlit-app
  working_dir: /app
  depends_on:
    hive-metastore:
      condition: service_healthy
    seaweedfs-s3:
      condition: service_healthy
  ports:
    - "8501:8501"
  environment:
    AWS_ACCESS_KEY_ID: seaweedfs_access_key
    AWS_SECRET_ACCESS_KEY: seaweedfs_secret_key
    AWS_ENDPOINT_URL_S3: http://seaweedfs-s3:8333
    AWS_REGION: us-east-1
    HIVE_METASTORE_URI: thrift://hive-metastore:9083
    STREAMLIT_SERVER_PORT: 8501
    STREAMLIT_SERVER_HEADLESS: "true"
  volumes:
    - ./streamlit-app:/app
    - ./logs/streamlit:/app/logs
  networks:
    - lakehouse-net
  command: >
    bash -c "
    pip install --no-cache-dir -r requirements.txt &&
    streamlit run app.py --server.port=8501 --server.headless=true --server.address=0.0.0.0
    "
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8501/_stcore/health"]
    interval: 30s
    timeout: 10s
    retries: 3
    start_period: 30s
```

- [x] `streamlit` ì„œë¹„ìŠ¤ ì¶”ê°€
- [x] í¬íŠ¸ ì„¤ì • (8501:8501)
- [x] í™˜ê²½ë³€ìˆ˜ ì„¤ì • (S3, Hive, Streamlit)
- [x] ë³¼ë¥¨ ì„¤ì • (app ì½”ë“œ, ë¡œê·¸)
- [x] healthcheck ì„¤ì •
- [ ] (ê¶Œì¥) Dockerfileë¡œ ì˜ì¡´ì„± ê³ ì •/ë¹Œë“œí•˜ì—¬ ì¬ì‹œì‘ ì‹œ ì¬ì„¤ì¹˜ ë°©ì§€

### 1.4 Volumes ì¶”ê°€

```yaml
volumes:
  # ê¸°ì¡´
  warehouse:
  postgres-data:
  seaweedfs-data:

  # ì‹ ê·œ
  superset-data:
  superset-db-data:
  superset-redis-data:
  grafana-data:
  opensearch-data:
  prometheus-data:
```

- [x] `superset-data` ì¶”ê°€
- [x] `superset-db-data` ì¶”ê°€
- [x] `superset-redis-data` ì¶”ê°€
- [x] `grafana-data` ì¶”ê°€
- [x] `opensearch-data` ì¶”ê°€
- [x] `prometheus-data` ì¶”ê°€

---

## âš™ï¸ Phase 2: ì„¤ì • íŒŒì¼ ìƒì„± (1ì‹œê°„)

### 2.1 ë””ë ‰í† ë¦¬ ìƒì„±
- [x] `mkdir -p config/prometheus`
- [x] `mkdir -p config/superset`
- [x] `mkdir -p config/grafana/provisioning/datasources`
- [x] `mkdir -p config/grafana/provisioning/dashboards`
- [x] `mkdir -p config/opensearch`
- [x] `mkdir -p streamlit-app/modules`
- [x] `mkdir -p streamlit-app/pages`
- [x] `mkdir -p logs/{superset,grafana,streamlit,opensearch}`

### 2.2 Prometheus ì„¤ì • (config/prometheus/prometheus.yml)

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    monitor: 'lakehouse-monitor'

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

  - job_name: 'trino'
    static_configs:
      - targets: ['trino:8080']
```

- [x] `config/prometheus/prometheus.yml` ìƒì„±
- [x] Prometheus ì „ì—­ ì„¤ì • í™•ì¸
- [x] scrape_configs í™•ì¸

### 2.3 Superset ì„¤ì • (config/superset/superset_config.py)

```python
import os
from datetime import timedelta

# Database
SQLALCHEMY_DATABASE_URI = 'postgresql://superset:superset@superset-db:5432/superset'

# Cache
CACHE_CONFIG = {
    'CACHE_TYPE': 'RedisCache',
    'CACHE_DEFAULT_TIMEOUT': 300,
    'CACHE_REDIS_HOST': 'superset-redis',
    'CACHE_REDIS_PORT': 6379,
}

# Security
SECRET_KEY = os.getenv('SUPERSET_SECRET_KEY', 'change-me-in-production')
SUPERSET_WEBSERVER_TIMEOUT = 60
ROW_LIMIT = 10000

# Features
SUPERSET_FEATURE_FLAGS = {
    'ALLOW_USER_PROFILE_EDIT': True,
    'ENABLE_FORMULA_EDITING': True,
}
```

- [x] `config/superset/superset_config.py` ìƒì„±
- [x] ë°ì´í„°ë² ì´ìŠ¤ URI í™•ì¸
- [x] Redis ìºì‹œ ì„¤ì • í™•ì¸
- [x] SECRET_KEY ì„¤ì • í™•ì¸

### 2.4 OpenSearch ì„¤ì • (config/opensearch/opensearch.yml)

```yaml
cluster.name: lakehouse-logs
node.name: opensearch-node1
discovery.type: single-node
network.host: 0.0.0.0
http.port: 9200
```

- [x] `config/opensearch/opensearch.yml` ìƒì„±
- [x] í´ëŸ¬ìŠ¤í„° ì´ë¦„ ì„¤ì •
- [x] ë„¤íŠ¸ì›Œí¬ ì„¤ì • í™•ì¸

### 2.5 Grafana ë°ì´í„° ì†ŒìŠ¤ (config/grafana/provisioning/datasources/opensearch.yml)

```yaml
apiVersion: 1

datasources:
  - name: OpenSearch
    type: grafana-opensearch-datasource
    access: proxy
    url: https://opensearch:9200
    basicAuth: true
    basicAuthUser: admin
    basicAuthPassword: ${OPENSEARCH_PASSWORD}
    isDefault: false
    jsonData:
      tlsSkipVerify: true
```

- [x] `config/grafana/provisioning/datasources/opensearch.yml` ìƒì„±
- [x] OpenSearch URL ì„¤ì •
- [x] ì¸ì¦ ì„¤ì •

### 2.6 Grafana ë°ì´í„° ì†ŒìŠ¤ (prometheus.yml)

```yaml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
```

- [x] `config/grafana/provisioning/datasources/prometheus.yml` ìƒì„±
- [x] Prometheus URL ì„¤ì •

### 2.7 í™˜ê²½ ì„¤ì • (.env íŒŒì¼)

```bash
SUPERSET_SECRET_KEY=your-super-secret-key
SUPERSET_ADMIN_USER=admin
SUPERSET_ADMIN_PASSWORD=admin
GRAFANA_PASSWORD=admin
OPENSEARCH_PASSWORD=Admin@123
```

- [x] `.env` íŒŒì¼ ìƒì„±
- [x] í™˜ê²½ë³€ìˆ˜ ì„¤ì •
- [ ] ê°œë°œ í™˜ê²½: ê¸°ë³¸ê°’ ìœ ì§€ ê°€ëŠ¥, ìš´ì˜ ì „ì—ëŠ” ë°˜ë“œì‹œ ë³€ê²½

---

## ğŸ Phase 3: Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„± (2ì‹œê°„)

### 3.1 requirements.txt

```
streamlit==1.30.0
pyiceberg[hive]==0.5.1
pandas==2.1.4
boto3==1.34.0
Pillow==10.1.0
pyarrow==14.0.0
plotly==5.17.0
```

- [x] `streamlit-app/requirements.txt` ìƒì„±

### 3.2 app.py (ë©”ì¸)

```python
import streamlit as st
import pandas as pd
from pathlib import Path
import sys

sys.path.insert(0, str(Path(__file__).parent))

from modules.iceberg_connector import get_iceberg_table
from modules.s3_utils import get_s3_client

st.set_page_config(
    page_title="Unstructured Data Explorer",
    page_icon="ğŸ–¼ï¸",
    layout="wide"
)

st.title("ğŸ–¼ï¸ Lakehouse Unstructured Data Explorer")

# Status checks
st.sidebar.header("System Status")
try:
    table = get_iceberg_table("media_db.image_metadata")
    st.sidebar.success("âœ… Iceberg connected")
except Exception as e:
    st.sidebar.error(f"âŒ Iceberg error: {e}")
```

- [x] `streamlit-app/app.py` ìƒì„±
- [x] ê¸°ë³¸ ë ˆì´ì•„ì›ƒ ì„¤ì •
- [x] ìƒíƒœ í™•ì¸ êµ¬í˜„

### 3.3 modules/iceberg_connector.py

```python
import os
from pyiceberg.catalog import load_catalog

def get_iceberg_table(table_name: str):
    catalog = load_catalog("default", **{
        "type": "hive",
        "uri": os.getenv("HIVE_METASTORE_URI", "thrift://hive-metastore:9083"),
        "s3.endpoint": os.getenv("AWS_ENDPOINT_URL_S3", "http://seaweedfs-s3:8333"),
        "s3.access-key-id": os.getenv("AWS_ACCESS_KEY_ID", "seaweedfs_access_key"),
        "s3.secret-access-key": os.getenv("AWS_SECRET_ACCESS_KEY", "seaweedfs_secret_key"),
        "s3.path-style-access": "true"
    })
    return catalog.load_table(table_name)
```

- [x] `streamlit-app/modules/iceberg_connector.py` ìƒì„±
- [x] Iceberg ì¹´íƒˆë¡œê·¸ ì—°ê²° í•¨ìˆ˜ êµ¬í˜„

### 3.4 modules/s3_utils.py

```python
import boto3
import os

def get_s3_client():
    return boto3.client(
        's3',
        endpoint_url=os.getenv('AWS_ENDPOINT_URL_S3', 'http://seaweedfs-s3:8333'),
        aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID', 'seaweedfs_access_key'),
        aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY', 'seaweedfs_secret_key'),
        region_name=os.getenv('AWS_REGION', 'us-east-1'),
        verify=False
    )

def parse_s3_path(s3_path: str):
    if s3_path.startswith("s3a://"):
        s3_path = s3_path[len("s3a://"):]
    elif s3_path.startswith("s3://"):
        s3_path = s3_path[len("s3://"):]
    else:
        raise ValueError(f"Unsupported S3 path: {s3_path}")
    bucket, _, key = s3_path.partition("/")
    if not bucket or not key:
        raise ValueError(f"Incomplete S3 path: {s3_path}")
    return bucket, key

def fetch_object_bytes(s3_path: str):
    bucket, key = parse_s3_path(s3_path)
    client = get_s3_client()
    response = client.get_object(Bucket=bucket, Key=key)
    return response["Body"].read()
```

- [x] `streamlit-app/modules/s3_utils.py` ìƒì„±
- [x] S3 í´ë¼ì´ì–¸íŠ¸ í•¨ìˆ˜ êµ¬í˜„

### 3.5 pages/01_Gallery.py

```python
import streamlit as st
import pandas as pd
from modules.iceberg_connector import get_iceberg_table
from modules.s3_utils import fetch_object_bytes

st.set_page_config(page_title="Gallery", page_icon="ğŸ–¼ï¸", layout="wide")
st.title("ğŸ–¼ï¸ Image Gallery")

# Sidebar filters
st.sidebar.header("Filters")
tag_options = ['all', 'product', 'user', 'analytics']
selected_tag = st.sidebar.selectbox("Tag", tag_options)

@st.cache_data(ttl=300)
def load_metadata(tag):
    table = get_iceberg_table("media_db.image_metadata")
    df = table.scan().to_pandas()
    if tag != 'all':
        df = df[df['tag'] == tag]
    return df

df = load_metadata(selected_tag)

def is_image_row(row):
    mime_type = str(getattr(row, "mime_type", "") or "")
    if mime_type.startswith("image/"):
        return True
    s3_path = str(getattr(row, "s3_path", "") or "").lower()
    return s3_path.endswith((".png", ".jpg", ".jpeg", ".gif", ".webp"))

image_rows = [row for row in df.itertuples(index=False) if is_image_row(row)]
st.metric("Total Images", len(image_rows))
for row in image_rows:
    image_bytes = fetch_object_bytes(row.s3_path)
    st.image(image_bytes, caption=row.image_id, use_column_width=True)
```

- [x] `streamlit-app/pages/01_Gallery.py` ìƒì„±
- [x] ê°¤ëŸ¬ë¦¬ í•„í„°ë§ êµ¬í˜„
- [x] ë©”íƒ€ë°ì´í„° ë¡œë“œ êµ¬í˜„
- [x] S3 ì´ë¯¸ì§€ ë Œë”ë§ êµ¬í˜„

### 3.6 pages/02_Search.py

```python
import streamlit as st
import pandas as pd
from modules.iceberg_connector import get_iceberg_table

st.set_page_config(page_title="Search", page_icon="ğŸ”", layout="wide")
st.title("ğŸ” Metadata Search")

search_field = st.selectbox("Search By", ["image_id", "tag", "source_system"])
search_query = st.text_input("Search Query")

if search_query:
    @st.cache_data(ttl=300)
    def search_metadata(field, query):
        table = get_iceberg_table("media_db.image_metadata")
        df = table.scan().to_pandas()
        return df[df[field].astype(str).str.contains(query, case=False)]

    results = search_metadata(search_field, search_query)
    st.metric("Results Found", len(results))
```

- [x] `streamlit-app/pages/02_Search.py` ìƒì„±
- [x] ê²€ìƒ‰ ê¸°ëŠ¥ êµ¬í˜„

### 3.7 pages/03_Statistics.py

```python
import streamlit as st
import pandas as pd
from modules.iceberg_connector import get_iceberg_table
import plotly.express as px

st.set_page_config(page_title="Statistics", page_icon="ğŸ“Š", layout="wide")
st.title("ğŸ“Š Statistics Dashboard")

@st.cache_data(ttl=300)
def load_stats():
    table = get_iceberg_table("media_db.image_metadata")
    return table.scan().to_pandas()

df = load_stats()
if len(df) > 0:
    tag_counts = df['tag'].value_counts()
    fig = px.bar(tag_counts, title="Count by Tag")
    st.plotly_chart(fig, use_container_width=True)
```

- [x] `streamlit-app/pages/03_Statistics.py` ìƒì„±
- [x] í†µê³„ ì‹œê°í™” êµ¬í˜„
- [ ] ëŒ€ìš©ëŸ‰ ëŒ€ë¹„: í•„ìš”í•œ ì»¬ëŸ¼/í•„í„°ë§Œ ì½ê³  limit/pagination ì ìš©

### 3.8 modules/__init__.py

- [x] `streamlit-app/modules/__init__.py` ìƒì„± (ë¹ˆ íŒŒì¼)

---

## ğŸš€ Phase 4: ì„œë¹„ìŠ¤ ì‹œì‘ (30ë¶„)

### 4.1 Docker Compose ê²€ì¦
- [ ] `docker-compose config` ì‹¤í–‰ (ì˜¤ë¥˜ í™•ì¸)
- [ ] ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸: `docker ps -a`
- [ ] ê¸°ì¡´ ì´ë¯¸ì§€ í™•ì¸: `docker images`

### 4.2 ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
- [ ] `docker-compose pull` ì‹¤í–‰ (ëª¨ë“  ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ)

### 4.3 ì„œë¹„ìŠ¤ ì‹œì‘ (ìˆœì°¨ì )

#### Step 1: ê¸°ë³¸ ì¸í”„ë¼ (ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸)
- [ ] Seaweedfs í´ëŸ¬ìŠ¤í„° í™•ì¸: `docker logs seaweedfs-s3 | tail -5`
- [ ] Hive Metastore í™•ì¸: `docker logs hive-metastore | tail -5`
- [ ] Trino í™•ì¸: `docker logs trino | tail -5`

#### Step 2: Superset ìŠ¤íƒ ì‹œì‘
```bash
docker-compose up -d superset-db superset-redis superset
```
- [ ] Superset-db ì‹œì‘ ëŒ€ê¸° (healthcheck í™•ì¸)
- [ ] Superset-redis ì‹œì‘ ëŒ€ê¸°
- [ ] Superset ì‹œì‘ (ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰)
- [ ] ë¡œê·¸ í™•ì¸: `docker logs superset | grep -E "initialize|listening"`
- [ ] ì ‘ì† í™•ì¸: http://localhost:8088 (admin/admin)

#### Step 3: OpenSearch + Grafana ìŠ¤íƒ ì‹œì‘
```bash
docker-compose up -d opensearch opensearch-dashboards prometheus node-exporter grafana
```
- [ ] OpenSearch ì‹œì‘ ëŒ€ê¸° (healthcheck í™•ì¸)
- [ ] OpenSearch Dashboards ì‹œì‘ ëŒ€ê¸°
- [ ] Prometheus ì‹œì‘ ëŒ€ê¸°
- [ ] Node Exporter ì‹œì‘ ëŒ€ê¸°
- [ ] Grafana ì‹œì‘ ëŒ€ê¸°
- [ ] ì ‘ì† í™•ì¸: http://localhost:3000 (admin/admin)
- [ ] OpenSearch ì ‘ì†: http://localhost:5601 (admin/${OPENSEARCH_PASSWORD}, ê¸°ë³¸ê°’: Admin@123)

#### Step 4: Streamlit ì‹œì‘
```bash
docker-compose up -d streamlit
```
- [ ] Streamlit ì‹œì‘ (pip install ëŒ€ê¸°)
- [ ] ë¡œê·¸ í™•ì¸: `docker logs streamlit-app | grep -E "Streamlit|Listening"`
- [ ] ì ‘ì† í™•ì¸: http://localhost:8501

### 4.4 ëª¨ë“  ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
```bash
docker-compose ps
```
- [ ] ëª¨ë“  ì»¨í…Œì´ë„ˆê°€ `Up (healthy)` ìƒíƒœ
- [ ] í¬íŠ¸ ë§¤í•‘ í™•ì¸
- [ ] healthcheck í†µê³¼ í™•ì¸

---

## ğŸ“Š Phase 5: ë°ì´í„° ì¤€ë¹„ (2ì‹œê°„)

### 5.0 ì‹œê°í™” ëŒ€ìƒ ë°ì´í„°ì…‹ ì •í•©ì„±
- [ ] Superset ì˜ˆì‹œ(`option_ticks_db`)ì™€ Streamlit ì˜ˆì‹œ(`media_db`) ì¤‘ ì‹¤ì œ ëª©í‘œ ë°ì´í„°ì…‹ìœ¼ë¡œ í†µì¼
- [ ] ìŠ¤í‚¤ë§ˆ/ìƒ˜í”Œ ë°ì´í„°ê°€ ëŒ€ì‹œë³´ë“œì™€ ì•±ì—ì„œ ë™ì¼í•˜ê²Œ ì¡°íšŒë˜ëŠ”ì§€ í™•ì¸

### 5.1 Iceberg ë©”íƒ€ë°ì´í„° í…Œì´ë¸” ìƒì„±

```sql
CREATE SCHEMA IF NOT EXISTS media_db;

CREATE TABLE IF NOT EXISTS media_db.image_metadata (
    image_id STRING,
    s3_path STRING,
    file_size BIGINT,
    mime_type STRING,
    upload_time TIMESTAMP,
    source_system STRING,
    tag STRING,
    width INT,
    height INT,
    checksum STRING,
    created_at TIMESTAMP
)
WITH (
    format = 'PARQUET',
    partitioning = ARRAY['day(upload_time)', 'tag']
);
```

- [ ] Trino CLI ì ‘ì†: `docker exec -it trino trino --server localhost:8080`
- [ ] ìŠ¤í‚¤ë§ˆ ìƒì„±
- [ ] í…Œì´ë¸” ìƒì„±
- [ ] í…Œì´ë¸” í™•ì¸: `SHOW TABLES FROM media_db;`

### 5.2 ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„

```bash
# Python ìŠ¤í¬ë¦½íŠ¸ë¡œ ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„± ë° S3 ì—…ë¡œë“œ
# (fspark_raw_examples.py:92-121 ì°¸ê³ )
python python/fspark_raw_examples.py
```

- [ ] ìƒ˜í”Œ ì´ë¯¸ì§€ 5ê°œ S3ì— ì—…ë¡œë“œ
- [ ] S3 ê²½ë¡œ í™•ì¸: `s3a://lakehouse/raw/images/{date}/`

### 5.3 ë©”íƒ€ë°ì´í„° INSERT

```sql
INSERT INTO media_db.image_metadata VALUES
('img-001', 's3a://lakehouse/raw/images/2025-12-25/image1.png', 102400, 'image/png', TIMESTAMP '2025-12-25 10:00:00', 'manual', 'product', 800, 600, 'abc123', TIMESTAMP '2025-12-25 10:00:00');
```

- [ ] ë©”íƒ€ë°ì´í„° INSERT ì‹¤í–‰
- [ ] ë°ì´í„° í™•ì¸: `SELECT COUNT(*) FROM media_db.image_metadata;`

---

## âš™ï¸ Phase 6: Superset ì„¤ì • (1ì‹œê°„)

### 6.1 Trino ë°ì´í„° ì†ŒìŠ¤ ì¶”ê°€
1. [ ] http://localhost:8088 ì ‘ì† (admin/admin)
2. [ ] **Settings** â†’ **Database Connections** â†’ **+ Database**
3. [ ] **Trino** ì„ íƒ
4. [ ] Connection URI: `trino://user@trino:8080/hive_prod`
5. [ ] **Test Connection** í´ë¦­
6. [ ] **Connect** í´ë¦­

### 6.2 ë°ì´í„°ì…‹ ìƒì„±
1. [ ] **Data** â†’ **Datasets** â†’ **+ Dataset**
2. [ ] Database: **Trino** ì„ íƒ
3. [ ] Schema: **option_ticks_db** ì„ íƒ
4. [ ] Table: **bronze_option_ticks** ì„ íƒ
5. [ ] **Create Dataset and Create Chart** í´ë¦­

### 6.3 ì°¨íŠ¸ ìƒì„±: Line Chart (ì‹œê°„ë³„ ê°€ê²© ë³€í™”)
```sql
SELECT timestamp, symbol, last_price
FROM hive_prod.option_ticks_db.bronze_option_ticks
WHERE timestamp >= CURRENT_DATE - INTERVAL '7' DAY
ORDER BY timestamp
```
- [ ] SQL ì…ë ¥
- [ ] Chart Type: **Time-series Line Chart** ì„ íƒ
- [ ] X-Axis: **timestamp**
- [ ] Metrics: **AVG(last_price)**
- [ ] Group by: **symbol**
- [ ] **Save** í´ë¦­

### 6.4 ì°¨íŠ¸ ìƒì„±: Bar Chart (ê±°ë˜ëŸ‰)
```sql
SELECT symbol, SUM(volume) as total_volume
FROM hive_prod.option_ticks_db.bronze_option_ticks
WHERE timestamp >= CURRENT_DATE - INTERVAL '7' DAY
GROUP BY symbol
ORDER BY total_volume DESC
```
- [ ] SQL ì…ë ¥
- [ ] Chart Type: **Bar Chart** ì„ íƒ
- [ ] X-Axis: **symbol**
- [ ] Metrics: **SUM(volume)**
- [ ] **Save** í´ë¦­

### 6.5 ëŒ€ì‹œë³´ë“œ ìƒì„±
1. [ ] **Dashboards** â†’ **+ Dashboard**
2. [ ] Title: **"Lakehouse Analytics"**
3. [ ] ìƒì„±í•œ ì°¨íŠ¸ 2ê°œ ì¶”ê°€
4. [ ] í•„í„° ì¶”ê°€: Date Range, Symbol
5. [ ] **Save** í´ë¦­

---

## ğŸ“ˆ Phase 7: Grafana ì„¤ì • (1ì‹œê°„)

### 7.1 ë°ì´í„° ì†ŒìŠ¤ í™•ì¸
1. [ ] http://localhost:3000 ì ‘ì† (admin/admin)
2. [ ] **Configuration** (âš™ï¸) â†’ **Data Sources**
3. [ ] **OpenSearch** ë°ì´í„° ì†ŒìŠ¤ í™•ì¸ (ìë™ í”„ë¡œë¹„ì €ë‹)
4. [ ] **Prometheus** ë°ì´í„° ì†ŒìŠ¤ í™•ì¸ (ìë™ í”„ë¡œë¹„ì €ë‹)
5. [ ] OpenSearch ì¸ë±ìŠ¤/ë°ì´í„° ìœ ì… í™•ì¸ (ë¡œê·¸ ìˆ˜ì§‘ ì—ì´ì „íŠ¸ ë™ì‘)

### 7.2 ìƒ˜í”Œ ëŒ€ì‹œë³´ë“œ ìƒì„±
1. [ ] **Dashboards** â†’ **+ New Dashboard**
2. [ ] **+ Add a new panel** í´ë¦­
3. [ ] **Panel Title**: "System CPU Usage"
4. [ ] Data Source: **Prometheus** ì„ íƒ
5. [ ] Query: `100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)`
6. [ ] **Save** í´ë¦­

### 7.3 ì•Œë¦¼ ê·œì¹™ ì„¤ì • (ì„ íƒì‚¬í•­)
1. [ ] **Alerting** â†’ **Alert Rules** â†’ **Create Alert Rule**
2. [ ] Condition ì„¤ì •
3. [ ] Notification Channel ì„¤ì •
4. [ ] **Save** í´ë¦­

---

## ğŸ–¼ï¸ Phase 8: Streamlit í…ŒìŠ¤íŠ¸ (30ë¶„)

### 8.1 ì•± ì ‘ì†
- [ ] http://localhost:8501 ì ‘ì†
- [ ] í˜ì´ì§€ ë¡œë“œ í™•ì¸

### 8.2 ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ (ì‚¬ì´ë“œë°”)
- [ ] âœ… Iceberg connected
- [ ] âœ… S3 connected

### 8.3 ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
1. [ ] **Gallery** í˜ì´ì§€ ë°©ë¬¸
   - [ ] ì´ë¯¸ì§€ ë¡œë“œ í™•ì¸
   - [ ] íƒœê·¸ í•„í„° ì‘ë™ í™•ì¸
   - [ ] ë‚ ì§œ ë²”ìœ„ í•„í„° ì‘ë™ í™•ì¸

2. [ ] **Search** í˜ì´ì§€ ë°©ë¬¸
   - [ ] ê²€ìƒ‰ ê¸°ëŠ¥ ì‘ë™ í™•ì¸
   - [ ] ê²°ê³¼ ë°˜í™˜ í™•ì¸

3. [ ] **Statistics** í˜ì´ì§€ ë°©ë¬¸
   - [ ] ê·¸ë˜í”„ ë Œë”ë§ í™•ì¸
   - [ ] í†µê³„ ê³„ì‚° í™•ì¸

---

## âœ¨ Phase 9: ì„±ëŠ¥ ê²€ì¦ (1ì‹œê°„)

### 9.1 Superset ì„±ëŠ¥
- [ ] ëŒ€ì‹œë³´ë“œ ë¡œë”© ì‹œê°„ ì¸¡ì • (ëª©í‘œ: < 5ì´ˆ)
- [ ] ì°¨íŠ¸ ë Œë”ë§ ì‹œê°„ ì¸¡ì • (ëª©í‘œ: < 3ì´ˆ)
- [ ] SQL Lab ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ (ëª©í‘œ: < 30ì´ˆ)

### 9.2 Grafana ì„±ëŠ¥
- [ ] ëŒ€ì‹œë³´ë“œ ë¡œë”© ì‹œê°„ ì¸¡ì • (ëª©í‘œ: < 5ì´ˆ)
- [ ] ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ í™•ì¸

### 9.3 Streamlit ì„±ëŠ¥
- [ ] ì•± ë¡œë”© ì‹œê°„ ì¸¡ì • (ëª©í‘œ: < 3ì´ˆ)
- [ ] ê°¤ëŸ¬ë¦¬ ë Œë”ë§ ì‹œê°„ (ëª©í‘œ: < 3ì´ˆ)
- [ ] ê²€ìƒ‰ ì‘ë‹µ ì‹œê°„ (ëª©í‘œ: < 5ì´ˆ)

### 9.4 ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥ 
```bash
docker stats
```
- [ ] CPU ì‚¬ìš©ë¥  í™•ì¸
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  í™•ì¸
- [ ] ë„¤íŠ¸ì›Œí¬ I/O í™•ì¸

---

## ğŸ”’ Phase 10: ë³´ì•ˆ ë° ìš´ì˜ (1ì‹œê°„)

### 10.1 ë¹„ë°€ë²ˆí˜¸ ë³€ê²½
- [ ] ìš´ì˜ ì „ Superset admin ë¹„ë°€ë²ˆí˜¸ ë³€ê²½ (ê°œë°œ í™˜ê²½ì€ ê¸°ë³¸ê°’ ìœ ì§€ ê°€ëŠ¥)
- [ ] ìš´ì˜ ì „ OpenSearch admin ë¹„ë°€ë²ˆí˜¸ ë³€ê²½ (ê°œë°œ í™˜ê²½ì€ ê¸°ë³¸ê°’ ìœ ì§€ ê°€ëŠ¥)
- [ ] ìš´ì˜ ì „ Grafana admin ë¹„ë°€ë²ˆí˜¸ ë³€ê²½ (ê°œë°œ í™˜ê²½ì€ ê¸°ë³¸ê°’ ìœ ì§€ ê°€ëŠ¥)

### 10.2 ë¡œê¹… í™•ì¸
```bash
docker logs <container-name> | grep ERROR
```
- [ ] Superset ë¡œê·¸ í™•ì¸
- [ ] Grafana ë¡œê·¸ í™•ì¸
- [ ] OpenSearch ë¡œê·¸ í™•ì¸
- [ ] Streamlit ë¡œê·¸ í™•ì¸

### 10.3 ë°±ì—… ì„¤ì •
- [ ] Superset ë©”íƒ€ìŠ¤í† ì–´ ë°±ì—… ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [ ] Grafana ì„¤ì • ë°±ì—… ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

### 10.4 ëª¨ë‹ˆí„°ë§ ì„¤ì •
- [ ] Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í™•ì¸
- [ ] Grafana ëŒ€ì‹œë³´ë“œ ëª¨ë‹ˆí„°ë§ í™•ì¸

---

## ğŸ“Š ìµœì¢… ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ëª¨ë“  ì„œë¹„ìŠ¤ ì •ìƒ ì‘ë™
- [ ] Superset: http://localhost:8088 âœ…
- [ ] Grafana: http://localhost:3000 âœ…
- [ ] OpenSearch Dashboards: http://localhost:5601 âœ…
- [ ] Streamlit: http://localhost:8501 âœ…
- [ ] Prometheus: http://localhost:9090 âœ…
- [ ] Trino UI: http://localhost:8080/ui âœ…

### ë°ì´í„° ì¡°íšŒ ê°€ëŠ¥
- [ ] Supersetì—ì„œ Trino ë°ì´í„° ì¡°íšŒ ê°€ëŠ¥
- [ ] Grafanaì—ì„œ OpenSearch ë¡œê·¸ ì¡°íšŒ ê°€ëŠ¥
- [ ] Streamlitì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ ê°€ëŠ¥

### ì„±ëŠ¥ ê¸°ì¤€ ì¶©ì¡±
- [ ] Superset ëŒ€ì‹œë³´ë“œ: < 5ì´ˆ
- [ ] Streamlit ì•±: < 3ì´ˆ
- [ ] Grafana ëŒ€ì‹œë³´ë“œ: < 5ì´ˆ

---

## ğŸ¯ ìš´ì˜ ê°€ì´ë“œ

### ì¼ì¼ ì ê²€
```bash
# ëª¨ë“  ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
docker-compose ps

# ì—ëŸ¬ ë¡œê·¸ í™•ì¸
docker-compose logs --tail=50 | grep ERROR
```

### ì£¼ê°„ ì ê²€
- [ ] ë°±ì—… ì‹¤í–‰
- [ ] ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê²€í† 
- [ ] ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ í™•ì¸

### ì›”ê°„ ì ê²€
- [ ] Iceberg í…Œì´ë¸” ìµœì í™”
- [ ] OpenSearch ì¸ë±ìŠ¤ ì •ë¦¬
- [ ] Prometheus ë°ì´í„° ì •ë¦¬

---

## ğŸ“ ë¬¸ì œ í•´ê²°

### Supersetì´ ì‹œì‘ë˜ì§€ ì•ŠìŒ
```bash
# ë¡œê·¸ í™•ì¸
docker logs superset

# PostgreSQL í™•ì¸
docker logs superset-db

# Redis í™•ì¸
docker logs superset-redis
```

### Streamlit ì—ëŸ¬
```bash
# ë¡œê·¸ í™•ì¸
docker logs streamlit-app

# Iceberg ì—°ê²° í…ŒìŠ¤íŠ¸
docker exec streamlit-app python -c "
from modules.iceberg_connector import get_iceberg_table
table = get_iceberg_table('hive_prod.media_db.image_metadata')
print(f'Tables: {len(table.scan().to_pandas())}')
"
```

### Grafana ë°ì´í„° ì†ŒìŠ¤ ì—°ê²° ì‹¤íŒ¨
```bash
# OpenSearch í™•ì¸
# .env ì‚¬ìš© ì‹œ ë¨¼ì € export í•„ìš”: export OPENSEARCH_PASSWORD=Admin@123
curl -ku admin:${OPENSEARCH_PASSWORD} https://localhost:9200/_cluster/health

# Prometheus í™•ì¸
curl http://localhost:9090/-/healthy
```

---

## ğŸ“Š ìµœì¢… ì ‘ê·¼ ê°€ëŠ¥ ë„êµ¬ ìš”ì•½

ë°°í¬ ì™„ë£Œ í›„ ë‹¤ìŒ ë„êµ¬ë“¤ì— ì ‘ê·¼ ê°€ëŠ¥í•©ë‹ˆë‹¤:

| # | ë„êµ¬ | URL | ë¡œê·¸ì¸ | ëª©ì  | í¬íŠ¸ | ìƒíƒœ |
|---|------|-----|--------|------|------|------|
| **1** | ğŸ“Š **Superset** | http://localhost:8088 | admin/admin | BI ëŒ€ì‹œë³´ë“œ (ì •í˜• ë°ì´í„°) | 8088 | ğŸš€ Phase 6 í›„ |
| **2** | ğŸ“ˆ **Grafana** | http://localhost:3000 | admin/admin | ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ | 3000 | ğŸš€ Phase 7 í›„ |
| **3** | ğŸ–¼ï¸ **Streamlit** | http://localhost:8501 | (ì—†ìŒ) | ì´ë¯¸ì§€ ê°¤ëŸ¬ë¦¬ (ë¹„ì •í˜• ë°ì´í„°) | 8501 | ğŸš€ Phase 8 í›„ |
| **4** | ğŸ“ **OpenSearch Dashboards** | http://localhost:5601 | admin/Admin@123 | ë¡œê·¸ íƒìƒ‰ ë° ë¶„ì„ | 5601 | ğŸš€ Phase 4 í›„ |
| **5** | ğŸ”¥ **Prometheus** | http://localhost:9090 | (ì—†ìŒ) | ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ì¿¼ë¦¬ | 9090 | ğŸš€ Phase 4 í›„ |
| **6** | ğŸ”§ **Trino UI** | http://localhost:8080 | (ì—†ìŒ) | ì¿¼ë¦¬ ëª¨ë‹ˆí„°ë§ (ê¸°ì¡´) | 8080 | ğŸš€ ê¸°ì¡´ ì„œë¹„ìŠ¤ |

---

## ğŸ“‹ ë¹ ë¥¸ ì°¸ê³ í‘œ

### ê° Tierë³„ ëª©ì ê³¼ êµ¬ì„±

| Tier | ì´ë¦„ | ë°ì´í„° ìœ í˜• | ì£¼ ë„êµ¬ | ë³´ì¡° ë„êµ¬ | ì‚¬ìš© ì‚¬ë¡€ |
|------|------|-----------|--------|---------|---------|
| **Tier 1** | BI ëŒ€ì‹œë³´ë“œ | ì •í˜• (Structured) | Superset | Trino, PostgreSQL, Redis | ë§¤ì¶œ ë¶„ì„, KPI ì¶”ì , ê²½ì˜ ëŒ€ì‹œë³´ë“œ |
| **Tier 2** | ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ | ë°˜ì •í˜• (Semi-structured) | Grafana | OpenSearch, Prometheus | ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§, ë¡œê·¸ ë¶„ì„, ì•Œë¦¼ |
| **Tier 3** | ì´ë¯¸ì§€ ê°¤ëŸ¬ë¦¬ | ë¹„ì •í˜• (Unstructured) | Streamlit | PyIceberg, boto3, S3 | ì´ë¯¸ì§€ íƒìƒ‰, ë©”íƒ€ë°ì´í„° ê²€ìƒ‰, í†µê³„ |

---

## ğŸš€ ë°°í¬ ë‹¨ê³„ë³„ ë„êµ¬ í™œì„±í™” ì¼ì •

```
Phase 0-3: ì¤€ë¹„ (ì•„ë¬´ê²ƒë„ ì‹¤í–‰ ì•ˆ ë¨)
    â†“
Phase 4: ì„œë¹„ìŠ¤ ì‹œì‘ âœ…
    â””â”€ ëª¨ë“  ì„œë¹„ìŠ¤ ì‹¤í–‰ ì‹œì‘
    â””â”€ Prometheus, OpenSearch í™œì„±í™”

Phase 5: ë°ì´í„° ì¤€ë¹„ âœ…
    â””â”€ Iceberg í…Œì´ë¸” ìƒì„±
    â””â”€ ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„

Phase 6: Superset ì„¤ì • âœ…
    â””â”€ Superset (http://localhost:8088) ì‚¬ìš© ê°€ëŠ¥
    â””â”€ Trino ë°ì´í„° ì†ŒìŠ¤ ì—°ê²°
    â””â”€ BI ëŒ€ì‹œë³´ë“œ ìƒì„±

Phase 7: Grafana ì„¤ì • âœ…
    â””â”€ Grafana (http://localhost:3000) ì‚¬ìš© ê°€ëŠ¥
    â””â”€ OpenSearch Dashboards (http://localhost:5601) ì‚¬ìš© ê°€ëŠ¥
    â””â”€ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ìƒì„±

Phase 8: Streamlit í…ŒìŠ¤íŠ¸ âœ…
    â””â”€ Streamlit (http://localhost:8501) ì‚¬ìš© ê°€ëŠ¥
    â””â”€ ì´ë¯¸ì§€ ê°¤ëŸ¬ë¦¬ í…ŒìŠ¤íŠ¸
    â””â”€ ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸

Phase 9: ì„±ëŠ¥ ê²€ì¦ âœ…
    â””â”€ ëª¨ë“  ë„êµ¬ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
    â””â”€ ì‘ë‹µ ì‹œê°„ ì¸¡ì •

Phase 10: ë³´ì•ˆ ë° ìš´ì˜ âœ…
    â””â”€ ë¹„ë°€ë²ˆí˜¸ ê°•í™”
    â””â”€ ë°±ì—… ìë™í™”
    â””â”€ ë¡œê¹… êµ¬ì„±
```

---

## ğŸ¯ ë„êµ¬ë³„ ì£¼ìš” ì‘ì—…

### 1ï¸âƒ£ Superset (BI ëŒ€ì‹œë³´ë“œ)

**ìš©ë„**: ì •í˜• ë°ì´í„° ì‹œê°í™” ë° BI ë¶„ì„

**Phase 6ì—ì„œ ì„¤ì •**:
- [ ] Trino ë°ì´í„° ì†ŒìŠ¤ ì¶”ê°€
- [ ] `option_ticks_db.bronze_option_ticks` ë°ì´í„°ì…‹ ìƒì„±
- [ ] ì‹œê³„ì—´ ì°¨íŠ¸ (ê°€ê²© ì¶”ì´)
- [ ] ë§‰ëŒ€ ì°¨íŠ¸ (ê±°ë˜ëŸ‰)
- [ ] ëŒ€ì‹œë³´ë“œ í†µí•©

**ì ‘ê·¼ ì£¼ì†Œ**: http://localhost:8088
**ê¸°ë³¸ ê³„ì •**: admin/admin

---

### 2ï¸âƒ£ Grafana (ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§)

**ìš©ë„**: ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

**Phase 7ì—ì„œ ì„¤ì •**:
- [ ] Prometheus ë°ì´í„° ì†ŒìŠ¤ ì—°ê²°
- [ ] OpenSearch ë°ì´í„° ì†ŒìŠ¤ ì—°ê²°
- [ ] CPU ì‚¬ìš©ë¥  ê·¸ë˜í”„
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ê·¸ë˜í”„
- [ ] ë””ìŠ¤í¬ ê³µê°„ ê²Œì´ì§€
- [ ] ì•Œë¦¼ ê·œì¹™ ì„¤ì •

**ì ‘ê·¼ ì£¼ì†Œ**: http://localhost:3000
**ê¸°ë³¸ ê³„ì •**: admin/admin

---

### 3ï¸âƒ£ Streamlit (ì´ë¯¸ì§€ ê°¤ëŸ¬ë¦¬)

**ìš©ë„**: ë¹„ì •í˜• ë°ì´í„° (ì´ë¯¸ì§€) íƒìƒ‰ ë° ë¶„ì„

**Phase 8ì—ì„œ í…ŒìŠ¤íŠ¸**:
- [ ] ê°¤ëŸ¬ë¦¬ í˜ì´ì§€ - ì´ë¯¸ì§€ 4ì—´ ê·¸ë¦¬ë“œ
- [ ] ê²€ìƒ‰ í˜ì´ì§€ - ë©”íƒ€ë°ì´í„° ê²€ìƒ‰
- [ ] í†µê³„ í˜ì´ì§€ - íƒœê·¸ë³„ ì¹´ìš´íŠ¸, í¬ê¸° ë¶„í¬
- [ ] í•„í„° ê¸°ëŠ¥ - íƒœê·¸, ë‚ ì§œ, í¬ê¸°ë¡œ í•„í„°ë§

**ì ‘ê·¼ ì£¼ì†Œ**: http://localhost:8501
**ì¸ì¦**: ì—†ìŒ (ê³µê°œ)

---

### 4ï¸âƒ£ OpenSearch Dashboards (ë¡œê·¸ ë¶„ì„)

**ìš©ë„**: ì‹œìŠ¤í…œ ë¡œê·¸ íƒìƒ‰ ë° ë¶„ì„

**Phase 4ì—ì„œ í™œì„±í™”**:
- [ ] ì¸ë±ìŠ¤ ìƒì„± (`logs-*` íŒ¨í„´)
- [ ] ë¡œê·¸ ìŠ¤íŠ¸ë¦¼ í™•ì¸
- [ ] Discoverì—ì„œ ë¡œê·¸ ê²€ìƒ‰
- [ ] ëŒ€ì‹œë³´ë“œ ìƒì„± (ì„ íƒ)

**ì ‘ê·¼ ì£¼ì†Œ**: http://localhost:5601
**ê¸°ë³¸ ê³„ì •**: admin/Admin@123

---

### 5ï¸âƒ£ Prometheus (ë©”íŠ¸ë¦­ ìˆ˜ì§‘)

**ìš©ë„**: ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ì¿¼ë¦¬

**Phase 4ì—ì„œ í™œì„±í™”**:
- [ ] Node Exporter ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- [ ] Prometheus UIì—ì„œ ì¿¼ë¦¬ ì‹¤í–‰
- [ ] Grafanaì—ì„œ ì‹œê°í™”

**ì ‘ê·¼ ì£¼ì†Œ**: http://localhost:9090
**ì¸ì¦**: ì—†ìŒ (ê³µê°œ)

---

## âœ… ì™„ë£Œ!

ëª¨ë“  ë‹¨ê³„ë¥¼ ì™„ë£Œí•˜ë©´ Lakehouse ì‹œê°í™” ìŠ¤íƒì´ ì™„ì„±ë©ë‹ˆë‹¤! ğŸ‰

**ì¶•í•˜í•©ë‹ˆë‹¤!** ğŸŠ ì´ì œ ë‹¤ìŒì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- âœ… Supersetìœ¼ë¡œ ë°ì´í„° ë¶„ì„
- âœ… Grafanaë¡œ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§
- âœ… Streamlitìœ¼ë¡œ ì´ë¯¸ì§€ íƒìƒ‰
- âœ… OpenSearchë¡œ ë¡œê·¸ ë¶„ì„
- âœ… Prometheusë¡œ ë©”íŠ¸ë¦­ ì¶”ì 

ë” ìì„¸í•œ ì •ë³´ëŠ” [docs/feature/visualization/README.md](../README.md) ì°¸ê³ 
