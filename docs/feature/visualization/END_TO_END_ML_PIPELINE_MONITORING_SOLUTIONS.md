# ğŸ¯ End-to-End ML íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§ ì†”ë£¨ì…˜ ê°€ì´ë“œ

**ì‘ì„±ì¼**: 2025-12-25
**ëª©ì **: RAW â†’ Bronze â†’ Silver â†’ Gold â†’ Feature Engineering â†’ ëª¨ë¸ í•™ìŠµ â†’ ìš´ì˜ ì „ì²´ ML íŒŒì´í”„ë¼ì¸ì„ ëª¨ë‹ˆí„°ë§í•  ìˆ˜ ìˆëŠ” ì—”í„°í”„ë¼ì´ì¦ˆ ì†”ë£¨ì…˜ ë¹„êµ

---

## ğŸ“‹ ëª©ì°¨

1. [ì†”ë£¨ì…˜ ê°œìš”](#ì†”ë£¨ì…˜-ê°œìš”)
2. [MLflow](#1-mlflow-ì¶”ì²œ-)
3. [Apache Airflow + MLflow](#2-apache-airflow--mlflow)
4. [Kubeflow](#3-kubeflow-kubernetes-ê¸°ë°˜)
5. [Prefect](#4-prefect-í˜„ëŒ€ì -ëŒ€ì•ˆ)
6. [DVC + CML](#5-dvc--cml)
7. [Feast](#6-feast-feature-store)
8. [ì†”ë£¨ì…˜ ë¹„êµí‘œ](#-ì†”ë£¨ì…˜-ë¹„êµí‘œ)
9. [í˜„ì¬ í”„ë¡œì íŠ¸ ì¶”ì²œ](#-í˜„ì¬-í”„ë¡œì íŠ¸-ì¶”ì²œ)
10. [í†µí•© ê°€ì´ë“œ](#-í†µí•©-ê°€ì´ë“œ)

---

## ì†”ë£¨ì…˜ ê°œìš”

ML íŒŒì´í”„ë¼ì¸ì˜ ê° ë‹¨ê³„ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì¶”ì í•˜ê³  ëª¨ë‹ˆí„°ë§í•˜ë ¤ë©´ ì „ë¬¸ ë„êµ¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.

### ML íŒŒì´í”„ë¼ì¸ ë‹¨ê³„

```
RAW Data (S3)
    â†“
Bronze Layer (Iceberg) - ì›ì‹œ ë°ì´í„° ìˆ˜ì§‘
    â†“
Silver Layer (Iceberg) - ë°ì´í„° ì •ì œ/ê²€ì¦
    â†“
Gold Layer (Iceberg) - ì§‘ê³„/ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
    â†“
Feature Engineering - í”¼ì²˜ ìƒì„±/ì„ íƒ
    â†“
Model Training - ëª¨ë¸ í•™ìŠµ/í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
    â†“
Model Registry - ëª¨ë¸ ë²„ì „ ê´€ë¦¬
    â†“
Production Deployment - ìš´ì˜ ë°°í¬
    â†“
Monitoring - ì„±ëŠ¥/ë“œë¦¬í”„íŠ¸ ëª¨ë‹ˆí„°ë§
```

---

## 1. MLflow (ì¶”ì²œ â­)

**ê°€ì¥ ì¸ê¸° ìˆëŠ” ì˜¤í”ˆì†ŒìŠ¤ ML í”Œë«í¼**

### í•µì‹¬ ê¸°ëŠ¥

| ê¸°ëŠ¥ | ì„¤ëª… | ì§€ì› ì—¬ë¶€ |
|------|------|----------|
| **Experiment Tracking** | ëª¨ë¸ íŒŒë¼ë¯¸í„°, ë©”íŠ¸ë¦­, ì•„í‹°íŒ©íŠ¸ ì¶”ì  | âœ… |
| **Model Registry** | ëª¨ë¸ ë²„ì „ ê´€ë¦¬ (Staging â†’ Production) | âœ… |
| **Pipeline Tracking** | ë°ì´í„° ì²˜ë¦¬ ê° ë‹¨ê³„ ë¡œê¹… | âœ… |
| **GUI Dashboard** | ì „ì²´ ì‹¤í—˜ ë¹„êµ, ëª¨ë¸ ì„±ëŠ¥ ì‹œê°í™” | âœ… |
| **Auto Logging** | scikit-learn, PyTorch, TensorFlow ìë™ ë¡œê¹… | âœ… |

### ì¥ì 

- âœ… **ê°€ë³ê³  ë¹ ë¥¸ ì„¤ì¹˜**: Docker 1ê°œ ì»¨í…Œì´ë„ˆë¡œ ì‹œì‘
- âœ… **ê´‘ë²”ìœ„í•œ ì§€ì›**: Apache Spark, scikit-learn, PyTorch, TensorFlow ëª¨ë‘ ì§€ì›
- âœ… **ì‰¬ìš´ í†µí•©**: Python ì½”ë“œ ëª‡ ì¤„ ì¶”ê°€ë¡œ ì¦‰ì‹œ ì‚¬ìš©
- âœ… **S3 ë°±ì—”ë“œ**: í˜„ì¬ í”„ë¡œì íŠ¸ì˜ SeaweedFS S3ì™€ ë°”ë¡œ í†µí•© ê°€ëŠ¥
- âœ… **ë¬´ë£Œ ì˜¤í”ˆì†ŒìŠ¤**: Apache 2.0 ë¼ì´ì„ ìŠ¤

### ë‹¨ì 

- âš ï¸ **ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì•½í•¨**: íŒŒì´í”„ë¼ì¸ ìë™ ì‹¤í–‰ ê¸°ëŠ¥ ì œí•œì 
- âš ï¸ **ìŠ¤ì¼€ì¤„ë§ ì—†ìŒ**: Cronì´ë‚˜ ì´ë²¤íŠ¸ ê¸°ë°˜ ì‹¤í–‰ ë¶ˆê°€
- âš ï¸ **ë³µì¡í•œ ì˜ì¡´ì„± ê´€ë¦¬**: DAG í˜•íƒœ íŒŒì´í”„ë¼ì¸ ì •ì˜ ì–´ë ¤ì›€

### ì‚¬ìš© ì˜ˆì‹œ

```python
import mlflow
import mlflow.spark
from pyspark.sql import SparkSession

# MLflow ì„œë²„ ì„¤ì •
mlflow.set_tracking_uri("http://mlflow:5000")
mlflow.set_experiment("lakehouse-tick-pipeline")

# Bronze ë‹¨ê³„
with mlflow.start_run(run_name="bronze_ingestion") as run:
    mlflow.log_param("source", "s3a://lakehouse/raw")
    mlflow.log_param("catalog", "hive_prod")

    # Spark ì‘ì—… ì‹¤í–‰
    df = spark.read.parquet("s3a://lakehouse/raw/ticks")
    row_count = df.count()

    mlflow.log_metric("rows_ingested", row_count)
    mlflow.log_metric("duration_seconds", 120)
    mlflow.set_tag("layer", "bronze")

# Silver ë‹¨ê³„
with mlflow.start_run(run_name="silver_cleaning") as run:
    mlflow.log_param("bronze_table", "hive_prod.option_ticks_db.bronze_option_ticks")

    # ë°ì´í„° ì •ì œ
    cleaned_df = df.dropna()
    cleaned_count = cleaned_df.count()

    mlflow.log_metric("rows_before", row_count)
    mlflow.log_metric("rows_after", cleaned_count)
    mlflow.log_metric("null_percentage", (1 - cleaned_count/row_count) * 100)
    mlflow.set_tag("layer", "silver")

# Gold ë‹¨ê³„
with mlflow.start_run(run_name="gold_aggregation") as run:
    # ì§‘ê³„ ì‘ì—…
    agg_df = cleaned_df.groupBy("symbol").agg(...)

    mlflow.log_metric("final_rows", agg_df.count())
    mlflow.log_metric("unique_symbols", agg_df.select("symbol").distinct().count())
    mlflow.set_tag("layer", "gold")

# ëª¨ë¸ í•™ìŠµ
with mlflow.start_run(run_name="model_training") as run:
    mlflow.log_param("algorithm", "RandomForest")
    mlflow.log_param("max_depth", 10)

    # ëª¨ë¸ í•™ìŠµ
    model = train_model(...)

    mlflow.log_metric("accuracy", 0.95)
    mlflow.log_metric("f1_score", 0.93)

    # ëª¨ë¸ ì €ì¥
    mlflow.sklearn.log_model(model, "model")
    mlflow.register_model(f"runs:/{run.info.run_id}/model", "TickPricePredictor")
```

### Docker Compose í†µí•©

```yaml
# docker-compose.ymlì— ì¶”ê°€
mlflow:
  image: ghcr.io/mlflow/mlflow:v2.9.2
  container_name: mlflow
  ports:
    - "5000:5000"
  environment:
    AWS_ACCESS_KEY_ID: seaweedfs_access_key
    AWS_SECRET_ACCESS_KEY: seaweedfs_secret_key
    AWS_ENDPOINT_URL_S3: http://seaweedfs-s3:8333
  command: >
    mlflow server
    --host 0.0.0.0
    --port 5000
    --backend-store-uri sqlite:///mlflow/mlflow.db
    --default-artifact-root s3://lakehouse/mlflow
  volumes:
    - mlflow-data:/mlflow
  networks:
    - lakehouse-net
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
    interval: 30s
    timeout: 10s
    retries: 3

volumes:
  mlflow-data:
```

### ì ‘ì† ì •ë³´

- **URL**: http://localhost:5000
- **ì¸ì¦**: ì—†ìŒ (ê¸°ë³¸ ì„¤ì •)
- **ë°±ì—”ë“œ ìŠ¤í† ì–´**: SQLite (ê°œë°œìš©) / PostgreSQL (ìš´ì˜ ê¶Œì¥)
- **ì•„í‹°íŒ©íŠ¸ ìŠ¤í† ì–´**: SeaweedFS S3

---

## 2. Apache Airflow + MLflow

**ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ + ML ì¶”ì ì˜ ì™„ë²½í•œ ì¡°í•©**

### í•µì‹¬ ê¸°ëŠ¥

| ê¸°ëŠ¥ | ì„¤ëª… | ì§€ì› ì—¬ë¶€ |
|------|------|----------|
| **DAG UI** | ê° ë‹¨ê³„ ì‹¤í–‰ ìƒíƒœ ì‹œê°í™” | âœ… |
| **Task Monitoring** | ì‹¤íŒ¨/ì„±ê³µ/ì¬ì‹œë„ ì¶”ì  | âœ… |
| **Scheduling** | Cron ê¸°ë°˜ ìë™ ì‹¤í–‰ | âœ… |
| **Dependency Management** | ë³µì¡í•œ ì‘ì—… ì˜ì¡´ì„± ê´€ë¦¬ | âœ… |
| **Alerts** | Slack, Email, PagerDuty ì•Œë¦¼ | âœ… |
| **MLflow í†µí•©** | MLflow Trackingê³¼ ì™„ë²½ ì—°ë™ | âœ… |

### ì¥ì 

- âœ… **ì—…ê³„ í‘œì¤€**: Netflix, Airbnb, Spotify, Uber ì‚¬ìš©
- âœ… **ë³µì¡í•œ ì˜ì¡´ì„± ê´€ë¦¬**: Bronze ì™„ë£Œ í›„ Silver ì‹œì‘ ë“± ì¡°ê±´ë¶€ ì‹¤í–‰
- âœ… **ì¬ì‹œë„ ë¡œì§**: ì‹¤íŒ¨ ì‹œ ìë™ ì¬ì‹œë„ ë° ë°±ì˜¤í”„
- âœ… **í’ë¶€í•œ í”ŒëŸ¬ê·¸ì¸**: Spark, Trino, S3, Slack ë“± 300+ Operators
- âœ… **ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ**: ì „ì²´ íŒŒì´í”„ë¼ì¸ ìƒíƒœ í•œëˆˆì— í™•ì¸

### ë‹¨ì 

- âš ï¸ **ë³µì¡í•œ ì„¤ì¹˜**: PostgreSQL, Redis ë“± ì—¬ëŸ¬ ì»´í¬ë„ŒíŠ¸ í•„ìš”
- âš ï¸ **ëŸ¬ë‹ ì»¤ë¸Œ**: DAG ì‘ì„±ì— Python ë° Airflow ì§€ì‹ í•„ìš”
- âš ï¸ **ë¦¬ì†ŒìŠ¤ ì‚¬ìš©**: ë©”ëª¨ë¦¬ 2GB+ í•„ìš”

### DAG êµ¬ì¡° ì˜ˆì‹œ

```python
# dags/ml_pipeline_dag.py
from airflow import DAG
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
import mlflow

default_args = {
    'owner': 'data-team',
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    'ml_pipeline',
    default_args=default_args,
    description='End-to-end ML Pipeline',
    schedule_interval='0 2 * * *',  # ë§¤ì¼ 02:00 ì‹¤í–‰
    start_date=days_ago(1),
    catchup=False,
    tags=['ml', 'lakehouse'],
) as dag:

    # Task 1: RAW â†’ Bronze (Spark Job)
    raw_to_bronze = SparkSubmitOperator(
        task_id='raw_to_bronze',
        application='/opt/airflow/dags/scripts/bronze_ingestion.py',
        conn_id='spark_default',
        conf={
            'spark.sql.catalog.hive_prod': 'org.apache.iceberg.spark.SparkCatalog',
        },
    )

    # Task 2: Bronze â†’ Silver (Data Cleaning)
    bronze_to_silver = SparkSubmitOperator(
        task_id='bronze_to_silver',
        application='/opt/airflow/dags/scripts/silver_cleaning.py',
        conn_id='spark_default',
    )

    # Task 3: Silver â†’ Gold (Aggregation)
    silver_to_gold = SparkSubmitOperator(
        task_id='silver_to_gold',
        application='/opt/airflow/dags/scripts/gold_aggregation.py',
        conn_id='spark_default',
    )

    # Task 4: Feature Engineering
    def feature_engineering(**context):
        mlflow.set_tracking_uri("http://mlflow:5000")
        with mlflow.start_run(run_name="feature_engineering"):
            # Feature ìƒì„± ë¡œì§
            mlflow.log_metric("features_created", 25)
            return "success"

    feature_task = PythonOperator(
        task_id='feature_engineering',
        python_callable=feature_engineering,
    )

    # Task 5: Model Training (MLflow ì—°ë™)
    def train_model(**context):
        mlflow.set_tracking_uri("http://mlflow:5000")
        with mlflow.start_run(run_name="model_training"):
            mlflow.log_param("algorithm", "XGBoost")
            # ëª¨ë¸ í•™ìŠµ
            mlflow.log_metric("accuracy", 0.95)
            return "model_trained"

    train_task = PythonOperator(
        task_id='model_training',
        python_callable=train_model,
    )

    # Task 6: Model Deployment
    def deploy_model(**context):
        # ëª¨ë¸ ë°°í¬ ë¡œì§
        return "deployed"

    deploy_task = PythonOperator(
        task_id='model_deployment',
        python_callable=deploy_model,
    )

    # ì˜ì¡´ì„± ì •ì˜
    raw_to_bronze >> bronze_to_silver >> silver_to_gold >> feature_task >> train_task >> deploy_task
```

### Docker Compose í†µí•©

```yaml
# Airflow + MLflow ì „ì²´ ìŠ¤íƒ
airflow-postgres:
  image: postgres:15
  container_name: airflow-postgres
  environment:
    POSTGRES_USER: airflow
    POSTGRES_PASSWORD: airflow
    POSTGRES_DB: airflow
  volumes:
    - airflow-postgres-data:/var/lib/postgresql/data
  networks:
    - lakehouse-net

airflow-redis:
  image: redis:7-alpine
  container_name: airflow-redis
  networks:
    - lakehouse-net

airflow-webserver:
  image: apache/airflow:2.8.0-python3.11
  container_name: airflow-webserver
  depends_on:
    - airflow-postgres
    - airflow-redis
  environment:
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@airflow-postgres/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://airflow-redis:6379/0
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
  ports:
    - "8080:8080"
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
  networks:
    - lakehouse-net
  command: webserver

airflow-scheduler:
  image: apache/airflow:2.8.0-python3.11
  container_name: airflow-scheduler
  depends_on:
    - airflow-postgres
    - airflow-redis
  environment:
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@airflow-postgres/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://airflow-redis:6379/0
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
  networks:
    - lakehouse-net
  command: scheduler

volumes:
  airflow-postgres-data:
```

### ì ‘ì† ì •ë³´

- **URL**: http://localhost:8080
- **ê¸°ë³¸ ê³„ì •**: admin / admin
- **MLflow ì—°ë™**: http://mlflow:5000 (ë‚´ë¶€ ë„¤íŠ¸ì›Œí¬)

---

## 3. Kubeflow (Kubernetes ê¸°ë°˜)

**ì „ì²´ ML ì›Œí¬í”Œë¡œìš° ê´€ë¦¬ í”Œë«í¼**

### í•µì‹¬ ê¸°ëŠ¥

| ê¸°ëŠ¥ | ì„¤ëª… | ì§€ì› ì—¬ë¶€ |
|------|------|----------|
| **Pipelines** | DAG í˜•íƒœ íŒŒì´í”„ë¼ì¸ ì‹œê°í™” | âœ… |
| **Experiments** | í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì¶”ì  | âœ… |
| **Notebooks** | Jupyter í†µí•© | âœ… |
| **Model Serving** | TensorFlow Serving, KFServing | âœ… |
| **Auto Scaling** | Kubernetes ë„¤ì´í‹°ë¸Œ ìŠ¤ì¼€ì¼ë§ | âœ… |
| **Multi-Tenancy** | íŒ€ë³„ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê²©ë¦¬ | âœ… |

### ì¥ì 

- âœ… **Google ì§€ì›**: TFX(TensorFlow Extended) í†µí•©
- âœ… **Kubernetes ë„¤ì´í‹°ë¸Œ**: ìë™ ìŠ¤ì¼€ì¼ë§ ë° ë¦¬ì†ŒìŠ¤ ê´€ë¦¬
- âœ… **ì „ì²´ MLOps ìŠ¤íƒ**: íŒŒì´í”„ë¼ì¸ + ì‹¤í—˜ + ì„œë¹™ ëª¨ë‘ í¬í•¨
- âœ… **ì—”í„°í”„ë¼ì´ì¦ˆê¸‰**: ëŒ€ê·œëª¨ ML íŒ€ì— ì í•©

### ë‹¨ì 

- âŒ **ë³µì¡í•œ ì„¤ì¹˜**: Kubernetes í´ëŸ¬ìŠ¤í„° í•„ìˆ˜
- âŒ **ì˜¤ë²„í‚¬**: ì‘ì€ í”„ë¡œì íŠ¸ì—ëŠ” ë„ˆë¬´ ë³µì¡
- âŒ **ë†’ì€ ëŸ¬ë‹ ì»¤ë¸Œ**: Kubernetes, Kubeflow SDK ëª¨ë‘ í•™ìŠµ í•„ìš”
- âŒ **ë¦¬ì†ŒìŠ¤ ì§‘ì•½ì **: ìµœì†Œ 8GB ë©”ëª¨ë¦¬ í•„ìš”

### ê¶Œì¥ ì‚¬ìš© ì‚¬ë¡€

- ëŒ€ê·œëª¨ ML íŒ€ (10ëª… ì´ìƒ)
- Kubernetes ì¸í”„ë¼ ì´ë¯¸ ë³´ìœ 
- ë³µì¡í•œ ë¶„ì‚° í•™ìŠµ í•„ìš”
- ì—”í„°í”„ë¼ì´ì¦ˆ MLOps êµ¬ì¶•

### í˜„ì¬ í”„ë¡œì íŠ¸ ì í•©ë„

âš ï¸ **ê¶Œì¥í•˜ì§€ ì•ŠìŒ** - í˜„ì¬ Docker Compose ê¸°ë°˜ í™˜ê²½ì— ì˜¤ë²„í‚¬

---

## 4. Prefect (í˜„ëŒ€ì  ëŒ€ì•ˆ)

**Python ë„¤ì´í‹°ë¸Œ ì›Œí¬í”Œë¡œìš° ì—”ì§„**

### í•µì‹¬ ê¸°ëŠ¥

| ê¸°ëŠ¥ | ì„¤ëª… | ì§€ì› ì—¬ë¶€ |
|------|------|----------|
| **Flow UI** | ì‹¤ì‹œê°„ íŒŒì´í”„ë¼ì¸ ì§„í–‰ë¥  | âœ… |
| **Task States** | ê° ë‹¨ê³„ ì„±ê³µ/ì‹¤íŒ¨/ìŠ¤í‚µ ìƒíƒœ | âœ… |
| **Retry Logic** | ìë™ ì¬ì‹œë„ ì„¤ì • | âœ… |
| **Cloud/Self-hosted** | í´ë¼ìš°ë“œ ë˜ëŠ” ì…€í”„ í˜¸ìŠ¤íŒ… | âœ… |
| **Python Decorators** | ê¸°ì¡´ í•¨ìˆ˜ì— @task ì¶”ê°€ë§Œìœ¼ë¡œ ì‚¬ìš© | âœ… |

### ì¥ì 

- âœ… **Airflowë³´ë‹¤ ê°„ë‹¨**: Python ë°ì½”ë ˆì´í„°ë§Œ ì¶”ê°€
- âœ… **ì•„ë¦„ë‹¤ìš´ UI**: í˜„ëŒ€ì ì´ê³  ì§ê´€ì ì¸ ëŒ€ì‹œë³´ë“œ
- âœ… **ë¹ ë¥¸ ê°œë°œ**: ê¸°ì¡´ ì½”ë“œ ìˆ˜ì • ìµœì†Œí™”
- âœ… **ìœ ì—°í•œ ë°°í¬**: Cloud ë˜ëŠ” Self-hosted ì„ íƒ ê°€ëŠ¥

### ë‹¨ì 

- âš ï¸ **ìƒíƒœê³„ ì‘ìŒ**: Airflowë³´ë‹¤ í”ŒëŸ¬ê·¸ì¸ ì ìŒ
- âš ï¸ **ë¹„êµì  ì‹ ìƒ**: Airflow ëŒ€ë¹„ ê²€ì¦ ë¶€ì¡±
- âš ï¸ **í´ë¼ìš°ë“œ ì˜ì¡´**: ë¬´ë£Œ ë²„ì „ì€ ê¸°ëŠ¥ ì œí•œì 

### ì‚¬ìš© ì˜ˆì‹œ

```python
from prefect import flow, task
from prefect.deployments import Deployment
from prefect.server.schemas.schedules import CronSchedule
import mlflow

@task(retries=3, retry_delay_seconds=60)
def raw_to_bronze():
    """RAW ë°ì´í„°ë¥¼ Bronze Layerë¡œ ì´ë™"""
    mlflow.set_tracking_uri("http://mlflow:5000")
    with mlflow.start_run(run_name="bronze_ingestion"):
        # Spark ì‘ì—…
        row_count = 10000
        mlflow.log_metric("rows_ingested", row_count)
        return row_count

@task(retries=3)
def bronze_to_silver(bronze_count: int):
    """Bronze ë°ì´í„°ë¥¼ ì •ì œí•˜ì—¬ Silver Layerë¡œ"""
    with mlflow.start_run(run_name="silver_cleaning"):
        cleaned_count = int(bronze_count * 0.95)
        mlflow.log_metric("rows_cleaned", cleaned_count)
        return cleaned_count

@task(retries=3)
def silver_to_gold(silver_count: int):
    """Silver ë°ì´í„°ë¥¼ ì§‘ê³„í•˜ì—¬ Gold Layerë¡œ"""
    with mlflow.start_run(run_name="gold_aggregation"):
        agg_count = int(silver_count * 0.9)
        mlflow.log_metric("final_rows", agg_count)
        return agg_count

@task
def train_model(gold_count: int):
    """ëª¨ë¸ í•™ìŠµ"""
    with mlflow.start_run(run_name="model_training"):
        mlflow.log_param("data_size", gold_count)
        mlflow.log_metric("accuracy", 0.95)
        return "model_v1"

@flow(name="ml-pipeline", log_prints=True)
def ml_pipeline():
    """ì „ì²´ ML íŒŒì´í”„ë¼ì¸"""
    bronze_count = raw_to_bronze()
    silver_count = bronze_to_silver(bronze_count)
    gold_count = silver_to_gold(silver_count)
    model = train_model(gold_count)
    print(f"Pipeline completed! Model: {model}")

# ìŠ¤ì¼€ì¤„ ì„¤ì • (ë§¤ì¼ 02:00 ì‹¤í–‰)
deployment = Deployment.build_from_flow(
    flow=ml_pipeline,
    name="daily-ml-pipeline",
    schedule=CronSchedule(cron="0 2 * * *"),
)

if __name__ == "__main__":
    deployment.apply()
```

### Docker Compose í†µí•©

```yaml
prefect-server:
  image: prefecthq/prefect:2.14-python3.11
  container_name: prefect-server
  ports:
    - "4200:4200"
  environment:
    PREFECT_SERVER_API_HOST: 0.0.0.0
    PREFECT_API_DATABASE_CONNECTION_URL: postgresql+asyncpg://prefect:prefect@prefect-postgres:5432/prefect
  command: prefect server start
  depends_on:
    - prefect-postgres
  networks:
    - lakehouse-net

prefect-postgres:
  image: postgres:15
  container_name: prefect-postgres
  environment:
    POSTGRES_USER: prefect
    POSTGRES_PASSWORD: prefect
    POSTGRES_DB: prefect
  volumes:
    - prefect-data:/var/lib/postgresql/data
  networks:
    - lakehouse-net

volumes:
  prefect-data:
```

### ì ‘ì† ì •ë³´

- **URL**: http://localhost:4200
- **ì¸ì¦**: ì—†ìŒ (Self-hosted ë²„ì „)
- **MLflow ì—°ë™**: Python ì½”ë“œì—ì„œ ì§ì ‘ í˜¸ì¶œ

---

## 5. DVC + CML

**Git ìŠ¤íƒ€ì¼ ë°ì´í„°/ëª¨ë¸ ë²„ì „ ê´€ë¦¬**

### í•µì‹¬ ê¸°ëŠ¥

| ê¸°ëŠ¥ | ì„¤ëª… | ì§€ì› ì—¬ë¶€ |
|------|------|----------|
| **Data Versioning** | Bronze/Silver/Gold ë°ì´í„°ì…‹ ë²„ì „ ì¶”ì  | âœ… |
| **Model Versioning** | í•™ìŠµëœ ëª¨ë¸ ë²„ì „ ê´€ë¦¬ | âœ… |
| **Pipeline as Code** | YAMLë¡œ íŒŒì´í”„ë¼ì¸ ì •ì˜ | âœ… |
| **CML (CI/CD)** | GitHub Actionsì—ì„œ ëª¨ë¸ ë©”íŠ¸ë¦­ ìë™ ë¦¬í¬íŠ¸ | âœ… |
| **Experiments** | Git ë¸Œëœì¹˜ì²˜ëŸ¼ ì‹¤í—˜ ê´€ë¦¬ | âœ… |

### ì¥ì 

- âœ… **Git ì›Œí¬í”Œë¡œìš°**: Git ì‚¬ìš©ìì—ê²Œ ì¹œìˆ™
- âœ… **S3 ë°±ì—”ë“œ**: SeaweedFS S3ì™€ ì™„ë²½ í†µí•©
- âœ… **ê°€ë²¼ì›€**: Python íŒ¨í‚¤ì§€ ì„¤ì¹˜ë§Œìœ¼ë¡œ ì‚¬ìš©
- âœ… **CI/CD í†µí•©**: GitHub Actions, GitLab CIì™€ ì—°ë™

### ë‹¨ì 

- âš ï¸ **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì•½í•¨**: GUI ëŒ€ì‹œë³´ë“œ ì—†ìŒ
- âš ï¸ **ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë¶€ì¡±**: ìë™ ì‹¤í–‰ ê¸°ëŠ¥ ì œí•œì 
- âš ï¸ **ëŸ¬ë‹ ì»¤ë¸Œ**: DVC CLI ë° ê°œë… í•™ìŠµ í•„ìš”

### dvc.yaml ì˜ˆì‹œ

```yaml
# dvc.yaml - íŒŒì´í”„ë¼ì¸ ì •ì˜
stages:
  raw_to_bronze:
    cmd: python scripts/bronze_ingestion.py
    deps:
      - scripts/bronze_ingestion.py
      - s3://lakehouse/raw/ticks
    params:
      - config.yaml:
          - bronze.batch_size
          - bronze.partition_cols
    outs:
      - s3://lakehouse/bronze/option_ticks:
          cache: false
    metrics:
      - metrics/bronze.json:
          cache: false

  bronze_to_silver:
    cmd: python scripts/silver_cleaning.py
    deps:
      - scripts/silver_cleaning.py
      - s3://lakehouse/bronze/option_ticks
    params:
      - config.yaml:
          - silver.null_threshold
          - silver.validation_rules
    outs:
      - s3://lakehouse/silver/option_ticks:
          cache: false
    metrics:
      - metrics/silver.json:
          cache: false

  silver_to_gold:
    cmd: python scripts/gold_aggregation.py
    deps:
      - scripts/gold_aggregation.py
      - s3://lakehouse/silver/option_ticks
    outs:
      - s3://lakehouse/gold/option_ticks:
          cache: false
    metrics:
      - metrics/gold.json:
          cache: false

  feature_engineering:
    cmd: python scripts/features.py
    deps:
      - scripts/features.py
      - s3://lakehouse/gold/option_ticks
    outs:
      - features/train.parquet
      - features/test.parquet
    metrics:
      - metrics/features.json

  train_model:
    cmd: python scripts/train.py
    deps:
      - scripts/train.py
      - features/train.parquet
      - features/test.parquet
    params:
      - config.yaml:
          - model.algorithm
          - model.max_depth
          - model.learning_rate
    outs:
      - models/model.pkl
    metrics:
      - metrics/train.json:
          cache: false
    plots:
      - plots/confusion_matrix.png
      - plots/roc_curve.png
```

### ì‹¤í–‰ ë°©ë²•

```bash
# ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
dvc repro

# íŠ¹ì • ë‹¨ê³„ë§Œ ì‹¤í–‰
dvc repro silver_to_gold

# ì‹¤í—˜ ë¹„êµ
dvc exp show

# ë©”íŠ¸ë¦­ ë¹„êµ
dvc metrics diff
```

---

## 6. Feast (Feature Store)

**Feature Engineering ì „ìš© ì†”ë£¨ì…˜**

### í•µì‹¬ ê¸°ëŠ¥

| ê¸°ëŠ¥ | ì„¤ëª… | ì§€ì› ì—¬ë¶€ |
|------|------|----------|
| **Feature Registry** | Feature ë©”íƒ€ë°ì´í„° ì¤‘ì•™ ê´€ë¦¬ | âœ… |
| **Point-in-time Correctness** | í•™ìŠµ/ì„œë¹™ ë°ì´í„° ì¼ê´€ì„± ë³´ì¥ | âœ… |
| **Online/Offline Store** | Redis (ì˜¨ë¼ì¸) + S3 (ì˜¤í”„ë¼ì¸) | âœ… |
| **Feature Reuse** | íŒ€ ê°„ Feature ê³µìœ  | âœ… |

### ì¥ì 

- âœ… **Feature ì¬ì‚¬ìš©**: í•œ ë²ˆ ì •ì˜í•œ Featureë¥¼ ì—¬ëŸ¬ ëª¨ë¸ì—ì„œ ì‚¬ìš©
- âœ… **í•™ìŠµ/ìš´ì˜ ì¼ì¹˜**: Point-in-time Joinìœ¼ë¡œ ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€
- âœ… **ë¹ ë¥¸ ì„œë¹™**: Redisì—ì„œ ë°€ë¦¬ì´ˆ ë‹¨ìœ„ Feature ì¡°íšŒ

### ë‹¨ì 

- âŒ **íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§ ì—†ìŒ**: Feature Store ê¸°ëŠ¥ë§Œ ì œê³µ
- âŒ **ì œí•œì  ë²”ìœ„**: Feature Engineering ë‹¨ê³„ì—ë§Œ ì‚¬ìš©

### í˜„ì¬ í”„ë¡œì íŠ¸ ì í•©ë„

âš ï¸ **ì¶”ê°€ ë„êµ¬ë¡œ ê³ ë ¤** - MLflow/Airflowì™€ í•¨ê»˜ ì‚¬ìš©

---

## ğŸ“Š ì†”ë£¨ì…˜ ë¹„êµí‘œ

| ì†”ë£¨ì…˜ | ì„¤ì¹˜ ë‚œì´ë„ | íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§ | ëª¨ë¸ ì¶”ì  | ìš´ì˜ ë°°í¬ | ë¹„ìš© | ì¶”ì²œë„ |
|--------|------------|-------------------|----------|----------|------|--------|
| **MLflow** | â­ ì‰¬ì›€ | âš ï¸ ì œí•œì  | âœ… ìµœê³  | âœ… ê°€ëŠ¥ | ë¬´ë£Œ | â­â­â­â­â­ |
| **Airflow + MLflow** | â­â­â­ ì¤‘ê°„ | âœ… ìµœê³  | âœ… ìµœê³  | âœ… ê°€ëŠ¥ | ë¬´ë£Œ | â­â­â­â­â­ |
| **Kubeflow** | â­â­â­â­â­ ì–´ë ¤ì›€ | âœ… ìš°ìˆ˜ | âœ… ìš°ìˆ˜ | âœ… ìµœê³  | ë¬´ë£Œ | â­â­â­ |
| **Prefect** | â­â­ ì‰¬ì›€ | âœ… ìµœê³  | âš ï¸ ì œí•œì  | âœ… ê°€ëŠ¥ | ë¬´ë£Œ/ìœ ë£Œ | â­â­â­â­ |
| **DVC + CML** | â­â­ ì‰¬ì›€ | âœ… ìš°ìˆ˜ | âœ… ìš°ìˆ˜ | âš ï¸ ì œí•œì  | ë¬´ë£Œ | â­â­â­â­ |
| **Feast** | â­â­â­ ì¤‘ê°„ | âŒ ì—†ìŒ | âŒ ì—†ìŒ | âœ… Featureë§Œ | ë¬´ë£Œ | â­â­â­ |

### ì„¸ë¶€ ë¹„êµ

#### ì„¤ì¹˜ ì‹œê°„

| ì†”ë£¨ì…˜ | ì´ˆê¸° ì„¤ì¹˜ | í†µí•© ì‹œê°„ | ì´ ì†Œìš” ì‹œê°„ |
|--------|----------|----------|-------------|
| MLflow | 10ë¶„ | 2ì‹œê°„ | **1ì¼** |
| Airflow + MLflow | 1ì‹œê°„ | 1ì£¼ | **1ì£¼** |
| Kubeflow | 1ì¼ | 2ì£¼ | **3ì£¼** |
| Prefect | 15ë¶„ | 3ì‹œê°„ | **3ì¼** |
| DVC + CML | 5ë¶„ | 1ì¼ | **2ì¼** |

#### ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­

| ì†”ë£¨ì…˜ | ë©”ëª¨ë¦¬ | CPU | ë””ìŠ¤í¬ | ì»¨í…Œì´ë„ˆ ìˆ˜ |
|--------|--------|-----|--------|------------|
| MLflow | 512MB | 0.5 | 10GB | 1ê°œ |
| Airflow | 2GB | 1.0 | 20GB | 4ê°œ |
| Kubeflow | 8GB | 4.0 | 100GB | 10ê°œ+ |
| Prefect | 1GB | 0.5 | 15GB | 2ê°œ |
| DVC | 100MB | 0.1 | 5GB | 0ê°œ (CLI) |

---

## ğŸ¯ í˜„ì¬ í”„ë¡œì íŠ¸ ì¶”ì²œ

### ì˜µì…˜ 1: MLflow ë‹¨ë… (â­ ê°€ì¥ ë¹ ë¦„ - 1ì¼)

**ì¶”ì²œ ëŒ€ìƒ**: ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…, ëª¨ë¸ í•™ìŠµ ì¶”ì ë§Œ í•„ìš”í•œ ê²½ìš°

**ì¥ì **:
- âœ… Docker ì»¨í…Œì´ë„ˆ 1ê°œ ì¶”ê°€ë§Œìœ¼ë¡œ ì¦‰ì‹œ ì‚¬ìš©
- âœ… Spark ì½”ë“œì— `mlflow.log_*()` ëª‡ ì¤„ë§Œ ì¶”ê°€
- âœ… í˜„ì¬ SeaweedFS S3ì™€ ì™„ë²½ í†µí•©

**ë‹¨ì **:
- âš ï¸ íŒŒì´í”„ë¼ì¸ ìë™ ì‹¤í–‰ ë¶ˆê°€ (ìˆ˜ë™ìœ¼ë¡œ fspark.py ì‹¤í–‰)
- âš ï¸ ìŠ¤ì¼€ì¤„ë§ ê¸°ëŠ¥ ì—†ìŒ

**êµ¬í˜„ ê³„íš**:
1. docker-compose.ymlì— mlflow ì„œë¹„ìŠ¤ ì¶”ê°€
2. fspark.pyì— MLflow ë¡œê¹… ì½”ë“œ ì¶”ê°€
3. http://localhost:5000 ì ‘ì†í•˜ì—¬ ì‹¤í—˜ í™•ì¸

---

### ì˜µì…˜ 2: Airflow + MLflow (â­â­ ì™„ë²½í•œ ì†”ë£¨ì…˜ - 1ì£¼)

**ì¶”ì²œ ëŒ€ìƒ**: ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ MLOps, ìë™í™”ëœ íŒŒì´í”„ë¼ì¸ í•„ìš”

**ì¥ì **:
- âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ìë™í™” + ëª¨ë¸ ì¶”ì 
- âœ… ì—…ê³„ í‘œì¤€ ì¡°í•©
- âœ… ë³µì¡í•œ ì˜ì¡´ì„± ê´€ë¦¬ ê°€ëŠ¥
- âœ… Cron ê¸°ë°˜ ìë™ ì‹¤í–‰

**ë‹¨ì **:
- âš ï¸ ì´ˆê¸° ì„¤ì • ì‹œê°„ í•„ìš” (1ì£¼)
- âš ï¸ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì¦ê°€ (ë©”ëª¨ë¦¬ 2GB+)

**êµ¬í˜„ ê³„íš**:
1. docker-compose.ymlì— Airflow ìŠ¤íƒ ì¶”ê°€ (PostgreSQL, Redis, Webserver, Scheduler)
2. fspark.pyë¥¼ Airflow DAGë¡œ ë³€í™˜
3. MLflow ì—°ë™ ì„¤ì •
4. ìŠ¤ì¼€ì¤„ ì„¤ì • ë° í…ŒìŠ¤íŠ¸

---

### ì˜µì…˜ 3: Prefect + MLflow (â­â­ í˜„ëŒ€ì  - 3ì¼)

**ì¶”ì²œ ëŒ€ìƒ**: Airflowë³´ë‹¤ ê°„ë‹¨í•˜ê³  ë¹ ë¥¸ ê°œë°œ ì›í•˜ëŠ” ê²½ìš°

**ì¥ì **:
- âœ… Airflowë³´ë‹¤ ì„¤ì • ê°„ë‹¨
- âœ… Python ë°ì½”ë ˆì´í„° ë°©ì‹ìœ¼ë¡œ ì‰¬ìš´ í†µí•©
- âœ… ì•„ë¦„ë‹¤ìš´ UI

**ë‹¨ì **:
- âš ï¸ Airflow ëŒ€ë¹„ ìƒíƒœê³„ ì‘ìŒ
- âš ï¸ í´ë¼ìš°ë“œ ë²„ì „ì€ ìœ ë£Œ

**êµ¬í˜„ ê³„íš**:
1. docker-compose.ymlì— Prefect ì„œë²„ ì¶”ê°€
2. fspark.pyì— @flow, @task ë°ì½”ë ˆì´í„° ì¶”ê°€
3. Prefect UIì—ì„œ ìŠ¤ì¼€ì¤„ ì„¤ì •

---

## ğŸš€ í†µí•© ê°€ì´ë“œ

### Step 1: MLflow ë¹ ë¥¸ ì‹œì‘ (30ë¶„)

#### 1.1 Docker Compose ìˆ˜ì •

```bash
cd /home/i/work/ai/lakehouse-tick
```

docker-compose.ymlì— ì¶”ê°€:

```yaml
mlflow:
  image: ghcr.io/mlflow/mlflow:v2.9.2
  container_name: mlflow
  ports:
    - "5000:5000"
  environment:
    AWS_ACCESS_KEY_ID: seaweedfs_access_key
    AWS_SECRET_ACCESS_KEY: seaweedfs_secret_key
    MLFLOW_S3_ENDPOINT_URL: http://seaweedfs-s3:8333
  command: >
    mlflow server
    --host 0.0.0.0
    --port 5000
    --backend-store-uri sqlite:///mlflow/mlflow.db
    --default-artifact-root s3://lakehouse/mlflow
  volumes:
    - mlflow-data:/mlflow
  networks:
    - lakehouse-net
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
    interval: 30s
    timeout: 10s
    retries: 3

volumes:
  mlflow-data:
```

#### 1.2 ì„œë¹„ìŠ¤ ì‹œì‘

```bash
docker compose up -d mlflow
docker compose ps mlflow
```

#### 1.3 fspark.pyì— MLflow ì¶”ê°€

```python
# python/fspark.py ìƒë‹¨ì— ì¶”ê°€
import mlflow

# MLflow ì„¤ì •
mlflow.set_tracking_uri("http://localhost:5000")  # ë¡œì»¬ ê°œë°œ
# mlflow.set_tracking_uri("http://mlflow:5000")  # Docker ë‚´ë¶€
mlflow.set_experiment("lakehouse-tick-pipeline")

# ê¸°ì¡´ ì½”ë“œ ìˆ˜ì • ì˜ˆì‹œ
with mlflow.start_run(run_name="bronze_ingestion"):
    mlflow.log_param("catalog", "hive_prod")
    mlflow.log_param("warehouse", "s3a://lakehouse/warehouse")

    # ê¸°ì¡´ Spark ì‘ì—… ì‹¤í–‰
    # ...

    mlflow.log_metric("rows_processed", row_count)
    mlflow.log_metric("duration_seconds", duration)
```

#### 1.4 ì ‘ì† í™•ì¸

```bash
# ë¸Œë¼ìš°ì €ì—ì„œ ì ‘ì†
http://localhost:5000
```

---

### Step 2: Airflow í†µí•© (1ì£¼)

#### 2.1 ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±

```bash
mkdir -p dags/scripts
mkdir -p logs
mkdir -p plugins
```

#### 2.2 DAG íŒŒì¼ ì‘ì„±

`dags/ml_pipeline_dag.py` ìƒì„± (ìœ„ Airflow ì„¹ì…˜ ì°¸ê³ )

#### 2.3 Docker Composeì— Airflow ì¶”ê°€

ìœ„ Airflow ì„¹ì…˜ì˜ docker-compose.yml ì°¸ê³ 

#### 2.4 ì´ˆê¸°í™” ë° ì‹œì‘

```bash
# DB ì´ˆê¸°í™”
docker compose run airflow-webserver airflow db init

# Admin ì‚¬ìš©ì ìƒì„±
docker compose run airflow-webserver airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com \
    --password admin

# ì„œë¹„ìŠ¤ ì‹œì‘
docker compose up -d airflow-webserver airflow-scheduler

# ì ‘ì† í™•ì¸
http://localhost:8080
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

### MLflow ë‹¨ë… êµ¬í˜„

- [ ] docker-compose.ymlì— mlflow ì„œë¹„ìŠ¤ ì¶”ê°€
- [ ] mlflow-data ë³¼ë¥¨ ìƒì„±
- [ ] `docker compose up -d mlflow` ì‹¤í–‰
- [ ] http://localhost:5000 ì ‘ì† í™•ì¸
- [ ] fspark.pyì— MLflow import ì¶”ê°€
- [ ] ì‹¤í—˜ ì¶”ì  ì½”ë“œ ì‘ì„±
- [ ] ì²« ì‹¤í—˜ ì‹¤í–‰ ë° UI í™•ì¸

### Airflow + MLflow êµ¬í˜„

- [ ] Airflow ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
- [ ] docker-compose.ymlì— Airflow ìŠ¤íƒ ì¶”ê°€
- [ ] DB ì´ˆê¸°í™” ë° admin ì‚¬ìš©ì ìƒì„±
- [ ] DAG íŒŒì¼ ì‘ì„±
- [ ] Airflow UI ì ‘ì† í™•ì¸
- [ ] fspark.pyë¥¼ SparkSubmitOperatorë¡œ ë³€í™˜
- [ ] MLflow ì—°ë™ í…ŒìŠ¤íŠ¸
- [ ] ìŠ¤ì¼€ì¤„ ì„¤ì • ë° ìë™ ì‹¤í–‰ í™•ì¸

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ

- **MLflow**: https://mlflow.org/docs/latest/
- **Apache Airflow**: https://airflow.apache.org/docs/
- **Kubeflow**: https://www.kubeflow.org/docs/
- **Prefect**: https://docs.prefect.io/
- **DVC**: https://dvc.org/doc
- **Feast**: https://docs.feast.dev/

### íŠœí† ë¦¬ì–¼

- [MLflow with Spark](https://mlflow.org/docs/latest/tracking.html#apache-spark)
- [Airflow DAG Tutorial](https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html)
- [Prefect Quickstart](https://docs.prefect.io/latest/getting-started/quickstart/)

---

## ğŸ‰ ê²°ë¡ 

**í˜„ì¬ í”„ë¡œì íŠ¸ì— ê°€ì¥ ì í•©í•œ ì†”ë£¨ì…˜**:

### ğŸ¥‡ 1ìˆœìœ„: MLflow ë‹¨ë… (ë¹ ë¥¸ ì‹œì‘)
- **ì‹œê°„**: 1ì¼
- **ë‚œì´ë„**: â­ ì‰¬ì›€
- **ì¶”ì²œ**: í”„ë¡œí† íƒ€ì…, ëª¨ë¸ ì¶”ì ë§Œ í•„ìš”

### ğŸ¥ˆ 2ìˆœìœ„: Airflow + MLflow (ì™„ë²½)
- **ì‹œê°„**: 1ì£¼
- **ë‚œì´ë„**: â­â­â­ ì¤‘ê°„
- **ì¶”ì²œ**: ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ MLOps êµ¬ì¶•

### ğŸ¥‰ 3ìˆœìœ„: Prefect + MLflow (í˜„ëŒ€ì )
- **ì‹œê°„**: 3ì¼
- **ë‚œì´ë„**: â­â­ ì‰¬ì›€
- **ì¶”ì²œ**: ë¹ ë¥¸ ê°œë°œ + ì•„ë¦„ë‹¤ìš´ UI

---

**ì‘ì„±**: 2025-12-25
**ë²„ì „**: 1.0
**ë‹¤ìŒ ë¬¸ì„œ**: [DEVELOPMENT_CHECKLIST.md](DEVELOPMENT_CHECKLIST.md)

---

## ğŸ”§ ì˜µì…˜ 2: ë³„ë„ docker-compose-mlops.yml êµ¬í˜„ ê°€ì´ë“œ

**ì„ íƒ ì´ìœ **: MLOps ì„œë¹„ìŠ¤ë¥¼ ê¸°ì¡´ Lakehouse ì¸í”„ë¼ì™€ ë¶„ë¦¬í•˜ì—¬ ë…ë¦½ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³  ì‹¶ì„ ë•Œ

### ğŸ“‹ ê°œìš”

ì´ ê°€ì´ë“œëŠ” Airflow + MLflow ìŠ¤íƒì„ **ë³„ë„ì˜ docker-compose-mlops.yml íŒŒì¼**ë¡œ ë¶„ë¦¬í•˜ì—¬ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.

#### ì¥ì 
- âœ… **ë…ë¦½ì  ìƒëª…ì£¼ê¸°**: MLOps ìŠ¤íƒë§Œ ë³„ë„ë¡œ ì‹œì‘/ì¤‘ì§€ ê°€ëŠ¥
- âœ… **íŒŒì¼ ê´€ë¦¬ ëª…í™•**: ê° íŒŒì¼ì˜ ì±…ì„ì´ ëª…í™•íˆ ë¶„ë¦¬ë¨
- âœ… **ê°œë°œ/ìš´ì˜ ë¶„ë¦¬**: ë°ì´í„° ì¸í”„ë¼ì™€ ML ì›Œí¬í”Œë¡œìš° ë…ë¦½ ê´€ë¦¬
- âœ… **ë¡¤ë°± ìš©ì´**: MLOps ìŠ¤íƒì— ë¬¸ì œ ë°œìƒ ì‹œ ì‰½ê²Œ ì œê±° ê°€ëŠ¥

#### ë‹¨ì 
- âš ï¸ **ë‘ ë²ˆì˜ ëª…ë ¹ì–´**: ë‘ ê°œì˜ docker-compose íŒŒì¼ ë³„ë„ ì‹¤í–‰ í•„ìš”
- âš ï¸ **ë„¤íŠ¸ì›Œí¬ ì„¤ì •**: External network ì„¤ì • í•„ìˆ˜
- âš ï¸ **í™˜ê²½ ë³€ìˆ˜ ì¤‘ë³µ**: ì¼ë¶€ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë‘ íŒŒì¼ì—ì„œ ê´€ë¦¬

---

### 1ï¸âƒ£ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```bash
/home/i/work/ai/lakehouse-tick/
â”œâ”€â”€ docker-compose.yml              # ê¸°ì¡´ Lakehouse ì¸í”„ë¼ (19ê°œ ì„œë¹„ìŠ¤)
â”œâ”€â”€ docker-compose-mlops.yml        # ì‹ ê·œ MLOps ìŠ¤íƒ (5ê°œ ì„œë¹„ìŠ¤)
â”œâ”€â”€ .env                            # ê³µí†µ í™˜ê²½ ë³€ìˆ˜
â”œâ”€â”€ .env.mlops                      # MLOps ì „ìš© í™˜ê²½ ë³€ìˆ˜ (ì„ íƒ)
â”œâ”€â”€ dags/                           # Airflow DAG íŒŒì¼
â”‚   â”œâ”€â”€ ml_pipeline_dag.py
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ bronze_ingestion.py
â”‚       â”œâ”€â”€ silver_cleaning.py
â”‚       â””â”€â”€ gold_aggregation.py
â”œâ”€â”€ logs/                           # ë¡œê·¸ ë””ë ‰í† ë¦¬
â”‚   â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ mlflow/
â””â”€â”€ plugins/                        # Airflow í”ŒëŸ¬ê·¸ì¸ (ì„ íƒ)
```

---

### 2ï¸âƒ£ docker-compose-mlops.yml ì „ì²´ ì½”ë“œ

#### 2.1 íŒŒì¼ ìƒì„±

```bash
cd /home/i/work/ai/lakehouse-tick
touch docker-compose-mlops.yml
```

#### 2.2 ì „ì²´ YAML ë‚´ìš©

```yaml
# ============================================================================
# MLOps Stack - Separate Compose File
# ============================================================================
#
# ì´ íŒŒì¼ì€ ê¸°ì¡´ docker-compose.ymlê³¼ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.
#
# ì‹¤í–‰ ë°©ë²•:
#   docker compose -f docker-compose-mlops.yml up -d
#
# ì¤‘ì§€ ë°©ë²•:
#   docker compose -f docker-compose-mlops.yml down
#
# ê¸°ì¡´ Lakehouse ì¸í”„ë¼ì™€ í†µì‹ í•˜ê¸° ìœ„í•´ external network ì‚¬ìš©
# ============================================================================

version: '3.8'

services:
  # ============================================================================
  # MLflow - Experiment Tracking & Model Registry
  # ============================================================================
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.9.2
    container_name: mlflow
    ports:
      - "5000:5000"
    environment:
      # S3 Backend (SeaweedFS)
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:-seaweedfs_access_key}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY:-seaweedfs_secret_key}
      MLFLOW_S3_ENDPOINT_URL: http://seaweedfs-s3:8333

      # MLflow ì„¤ì •
      MLFLOW_BACKEND_STORE_URI: sqlite:///mlflow/mlflow.db
      MLFLOW_DEFAULT_ARTIFACT_ROOT: s3://lakehouse/mlflow
    command: >
      mlflow server
      --host 0.0.0.0
      --port 5000
      --backend-store-uri sqlite:///mlflow/mlflow.db
      --default-artifact-root s3://lakehouse/mlflow
    volumes:
      - mlflow-data:/mlflow
      - ./logs/mlflow:/mlflow/logs
    networks:
      - lakehouse-net  # External network (ê¸°ì¡´ docker-compose.ymlì—ì„œ ìƒì„±)
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M

  # ============================================================================
  # Airflow PostgreSQL - Airflow ë©”íƒ€ìŠ¤í† ì–´
  # ============================================================================
  airflow-postgres:
    image: postgres:15-alpine
    container_name: airflow-postgres
    environment:
      POSTGRES_USER: ${AIRFLOW_POSTGRES_USER:-airflow}
      POSTGRES_PASSWORD: ${AIRFLOW_POSTGRES_PASSWORD:-airflow}
      POSTGRES_DB: ${AIRFLOW_POSTGRES_DB:-airflow}
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - airflow-postgres-data:/var/lib/postgresql/data
    networks:
      - lakehouse-net
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow", "-d", "airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  # ============================================================================
  # Airflow Redis - Celery ë¸Œë¡œì»¤
  # ============================================================================
  airflow-redis:
    image: redis:7-alpine
    container_name: airflow-redis
    ports:
      - "6379:6379"
    command: redis-server --maxmemory 256mb --maxmemory-policy allkeys-lru
    volumes:
      - airflow-redis-data:/data
    networks:
      - lakehouse-net
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  # ============================================================================
  # Airflow Webserver - UI & API
  # ============================================================================
  airflow-webserver:
    image: apache/airflow:2.8.0-python3.11
    container_name: airflow-webserver
    depends_on:
      airflow-postgres:
        condition: service_healthy
      airflow-redis:
        condition: service_healthy
    ports:
      - "8080:8080"
    environment:
      # Core
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY:-46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__DEFAULT_TIMEZONE: Asia/Seoul

      # Database
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow

      # Celery
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://airflow-redis:6379/0

      # Webserver
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_WEBSERVER_SECRET_KEY:-airflow-secret-key}
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'

      # Logging
      AIRFLOW__LOGGING__BASE_LOG_FOLDER: /opt/airflow/logs
      AIRFLOW__LOGGING__LOGGING_LEVEL: INFO

      # Scheduler
      AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'

      # S3 ì—°ë™ (SeaweedFS)
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:-seaweedfs_access_key}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY:-seaweedfs_secret_key}
      AWS_ENDPOINT_URL_S3: http://seaweedfs-s3:8333

      # MLflow ì—°ë™
      MLFLOW_TRACKING_URI: http://mlflow:5000
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs/airflow:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    networks:
      - lakehouse-net
    command: webserver
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G

  # ============================================================================
  # Airflow Scheduler - DAG ìŠ¤ì¼€ì¤„ë§
  # ============================================================================
  airflow-scheduler:
    image: apache/airflow:2.8.0-python3.11
    container_name: airflow-scheduler
    depends_on:
      airflow-postgres:
        condition: service_healthy
      airflow-redis:
        condition: service_healthy
    environment:
      # Core
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY:-46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__DEFAULT_TIMEZONE: Asia/Seoul

      # Database
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow

      # Celery
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://airflow-redis:6379/0

      # Logging
      AIRFLOW__LOGGING__BASE_LOG_FOLDER: /opt/airflow/logs
      AIRFLOW__LOGGING__LOGGING_LEVEL: INFO

      # Scheduler
      AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
      AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT: 'false'

      # S3 ì—°ë™ (SeaweedFS)
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:-seaweedfs_access_key}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY:-seaweedfs_secret_key}
      AWS_ENDPOINT_URL_S3: http://seaweedfs-s3:8333

      # MLflow ì—°ë™
      MLFLOW_TRACKING_URI: http://mlflow:5000
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs/airflow:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    networks:
      - lakehouse-net
    command: scheduler
    healthcheck:
      test: ["CMD", "airflow", "jobs", "check", "--job-type", "SchedulerJob", "--hostname", "$HOSTNAME"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G

# ============================================================================
# Networks - External Network ì‚¬ìš©
# ============================================================================
networks:
  lakehouse-net:
    external: true  # ê¸°ì¡´ docker-compose.ymlì—ì„œ ìƒì„±ëœ ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©

# ============================================================================
# Volumes - MLOps ì „ìš© ë³¼ë¥¨
# ============================================================================
volumes:
  mlflow-data:
    driver: local
  airflow-postgres-data:
    driver: local
  airflow-redis-data:
    driver: local
```

---

### 3ï¸âƒ£ External Network ì„¤ì •

#### 3.1 ê¸°ì¡´ docker-compose.yml ìˆ˜ì •

ê¸°ì¡´ `docker-compose.yml`ì—ì„œ **ë„¤íŠ¸ì›Œí¬ë¥¼ externalë¡œ ë³€ê²½**í•´ì•¼ í•©ë‹ˆë‹¤.

**ë³€ê²½ ì „**:
```yaml
networks:
  default:
    name: lakehouse-net
```

**ë³€ê²½ í›„**:
```yaml
networks:
  lakehouse-net:
    driver: bridge
    name: lakehouse-net
```

#### 3.2 ë„¤íŠ¸ì›Œí¬ ìƒì„± í™•ì¸

```bash
# ê¸°ì¡´ ë„¤íŠ¸ì›Œí¬ê°€ ì—†ë‹¤ë©´ ìˆ˜ë™ ìƒì„±
docker network create lakehouse-net

# ë„¤íŠ¸ì›Œí¬ í™•ì¸
docker network ls | grep lakehouse-net
```

---

### 4ï¸âƒ£ í™˜ê²½ ë³€ìˆ˜ ê´€ë¦¬

#### 4.1 .env íŒŒì¼ í™•ì¥

ê¸°ì¡´ `.env` íŒŒì¼ì— MLOps ê´€ë ¨ í™˜ê²½ ë³€ìˆ˜ ì¶”ê°€:

```bash
# ============================================================================
# ê¸°ì¡´ í™˜ê²½ ë³€ìˆ˜ (ìœ ì§€)
# ============================================================================
AWS_ACCESS_KEY_ID=seaweedfs_access_key
AWS_SECRET_ACCESS_KEY=seaweedfs_secret_key
AWS_REGION=us-east-1

# ============================================================================
# MLOps í™˜ê²½ ë³€ìˆ˜ (ì¶”ê°€)
# ============================================================================

# Airflow PostgreSQL
AIRFLOW_POSTGRES_USER=airflow
AIRFLOW_POSTGRES_PASSWORD=airflow
AIRFLOW_POSTGRES_DB=airflow

# Airflow Security Keys
# Fernet Key ìƒì„±: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
AIRFLOW_FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=

# Webserver Secret Key
AIRFLOW_WEBSERVER_SECRET_KEY=airflow-secret-key-change-this

# MLflow
MLFLOW_TRACKING_URI=http://mlflow:5000
```

#### 4.2 .env.mlops íŒŒì¼ (ì„ íƒì‚¬í•­)

MLOps ì „ìš© í™˜ê²½ ë³€ìˆ˜ë¥¼ ë³„ë„ë¡œ ê´€ë¦¬í•˜ê³  ì‹¶ë‹¤ë©´:

```bash
# .env.mlops
AIRFLOW_POSTGRES_USER=airflow
AIRFLOW_POSTGRES_PASSWORD=airflow
AIRFLOW_POSTGRES_DB=airflow
AIRFLOW_FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
AIRFLOW_WEBSERVER_SECRET_KEY=airflow-secret-key
```

ì‹¤í–‰ ì‹œ:
```bash
docker compose -f docker-compose-mlops.yml --env-file .env.mlops up -d
```

---

### 5ï¸âƒ£ ë””ë ‰í† ë¦¬ ìƒì„±

```bash
cd /home/i/work/ai/lakehouse-tick

# DAG ë””ë ‰í† ë¦¬
mkdir -p dags/scripts

# ë¡œê·¸ ë””ë ‰í† ë¦¬
mkdir -p logs/airflow
mkdir -p logs/mlflow

# í”ŒëŸ¬ê·¸ì¸ ë””ë ‰í† ë¦¬ (ì„ íƒ)
mkdir -p plugins

# ê¶Œí•œ ì„¤ì • (AirflowëŠ” UID 50000 ì‚¬ìš©)
sudo chown -R 50000:50000 dags logs plugins
```

---

### 6ï¸âƒ£ ì‹¤í–‰ ê°€ì´ë“œ

#### 6.1 ì „ì²´ ìŠ¤íƒ ì‹œì‘ (ìˆœì„œ ì¤‘ìš”)

```bash
cd /home/i/work/ai/lakehouse-tick

# 1ë‹¨ê³„: ê¸°ì¡´ Lakehouse ì¸í”„ë¼ ì‹œì‘
docker compose up -d

# 2ë‹¨ê³„: ë„¤íŠ¸ì›Œí¬ ì¡´ì¬ í™•ì¸
docker network ls | grep lakehouse-net

# 3ë‹¨ê³„: MLOps ìŠ¤íƒ ì‹œì‘
docker compose -f docker-compose-mlops.yml up -d

# 4ë‹¨ê³„: ì „ì²´ ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
docker compose ps
docker compose -f docker-compose-mlops.yml ps
```

#### 6.2 Airflow ì´ˆê¸°í™” (ìµœì´ˆ 1íšŒë§Œ)

```bash
# DB ë§ˆì´ê·¸ë ˆì´ì…˜
docker compose -f docker-compose-mlops.yml run --rm airflow-webserver airflow db migrate

# Admin ì‚¬ìš©ì ìƒì„±
docker compose -f docker-compose-mlops.yml run --rm airflow-webserver \
  airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com \
    --password admin

# ì´ˆê¸°í™” ì™„ë£Œ í™•ì¸
docker compose -f docker-compose-mlops.yml logs airflow-webserver | grep "Webserver started"
```

#### 6.3 ì ‘ì† í™•ì¸

```bash
# MLflow UI
curl -f http://localhost:5000/health && echo "âœ… MLflow OK"

# Airflow UI
curl -f http://localhost:8080/health && echo "âœ… Airflow OK"

# ë¸Œë¼ìš°ì € ì ‘ì†
# MLflow: http://localhost:5000
# Airflow: http://localhost:8080 (admin/admin)
```

---

### 7ï¸âƒ£ DAG íŒŒì¼ ì‘ì„±

#### 7.1 ìƒ˜í”Œ DAG ìƒì„±

`dags/ml_pipeline_dag.py`:

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from airflow.utils.dates import days_ago
from datetime import timedelta
import mlflow

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    'ml_pipeline',
    default_args=default_args,
    description='End-to-end ML Pipeline with MLflow',
    schedule_interval='0 2 * * *',  # ë§¤ì¼ 02:00
    start_date=days_ago(1),
    catchup=False,
    tags=['ml', 'lakehouse', 'mlflow'],
) as dag:

    def log_to_mlflow(**context):
        """MLflowì— ë©”íŠ¸ë¦­ ë¡œê¹…"""
        mlflow.set_tracking_uri("http://mlflow:5000")
        mlflow.set_experiment("lakehouse-tick-pipeline")

        with mlflow.start_run(run_name=context['task_instance'].task_id):
            mlflow.log_param("dag_id", context['dag'].dag_id)
            mlflow.log_param("execution_date", str(context['execution_date']))
            mlflow.log_metric("test_metric", 100)
            print(f"âœ… Logged to MLflow: {context['task_instance'].task_id}")

    # Task 1: Bronze Layer ì²˜ë¦¬
    bronze_task = PythonOperator(
        task_id='bronze_ingestion',
        python_callable=log_to_mlflow,
    )

    # Task 2: Silver Layer ì²˜ë¦¬
    silver_task = PythonOperator(
        task_id='silver_cleaning',
        python_callable=log_to_mlflow,
    )

    # Task 3: Gold Layer ì²˜ë¦¬
    gold_task = PythonOperator(
        task_id='gold_aggregation',
        python_callable=log_to_mlflow,
    )

    # Task 4: Feature Engineering
    feature_task = PythonOperator(
        task_id='feature_engineering',
        python_callable=log_to_mlflow,
    )

    # Task 5: Model Training
    train_task = PythonOperator(
        task_id='model_training',
        python_callable=log_to_mlflow,
    )

    # ì˜ì¡´ì„± ì •ì˜
    bronze_task >> silver_task >> gold_task >> feature_task >> train_task
```

#### 7.2 DAG íŒŒì¼ ë°°í¬

```bash
# DAG íŒŒì¼ì„ dags/ ë””ë ‰í† ë¦¬ì— ë³µì‚¬
cp ml_pipeline_dag.py /home/i/work/ai/lakehouse-tick/dags/

# ê¶Œí•œ ì„¤ì •
sudo chown 50000:50000 /home/i/work/ai/lakehouse-tick/dags/ml_pipeline_dag.py

# Airflowì—ì„œ DAG ì¸ì‹ í™•ì¸ (ì•½ 30ì´ˆ ì†Œìš”)
docker compose -f docker-compose-mlops.yml logs airflow-scheduler | grep ml_pipeline
```

#### 7.3 Airflow UIì—ì„œ DAG í™œì„±í™”

1. http://localhost:8080 ì ‘ì†
2. Login: admin / admin
3. DAGs í˜ì´ì§€ì—ì„œ `ml_pipeline` ì°¾ê¸°
4. Toggle ìŠ¤ìœ„ì¹˜ í´ë¦­í•˜ì—¬ í™œì„±í™”
5. "Trigger DAG" ë²„íŠ¼ìœ¼ë¡œ ìˆ˜ë™ ì‹¤í–‰

---

### 8ï¸âƒ£ ì¤‘ì§€ ë° ì œê±°

#### 8.1 ì„œë¹„ìŠ¤ ì¤‘ì§€

```bash
# MLOps ìŠ¤íƒë§Œ ì¤‘ì§€ (ë°ì´í„° ìœ ì§€)
docker compose -f docker-compose-mlops.yml stop

# Lakehouse ì¸í”„ë¼ëŠ” ê³„ì† ì‹¤í–‰ ìƒíƒœ ìœ ì§€
docker compose ps
```

#### 8.2 MLOps ìŠ¤íƒ ì™„ì „ ì œê±°

```bash
# ì»¨í…Œì´ë„ˆ + ë„¤íŠ¸ì›Œí¬ ì œê±° (ë³¼ë¥¨ ìœ ì§€)
docker compose -f docker-compose-mlops.yml down

# ì»¨í…Œì´ë„ˆ + ë„¤íŠ¸ì›Œí¬ + ë³¼ë¥¨ ëª¨ë‘ ì œê±° (ì£¼ì˜: ë°ì´í„° ì‚­ì œ)
docker compose -f docker-compose-mlops.yml down -v
```

#### 8.3 ì „ì²´ ìŠ¤íƒ ì œê±°

```bash
# 1ë‹¨ê³„: MLOps ìŠ¤íƒ ì œê±°
docker compose -f docker-compose-mlops.yml down -v

# 2ë‹¨ê³„: Lakehouse ì¸í”„ë¼ ì œê±°
docker compose down -v

# 3ë‹¨ê³„: External ë„¤íŠ¸ì›Œí¬ ì œê±°
docker network rm lakehouse-net
```

---

### 9ï¸âƒ£ Health Check ë° ëª¨ë‹ˆí„°ë§

#### 9.1 ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸

```bash
# MLOps ìŠ¤íƒ í—¬ìŠ¤ì²´í¬
docker compose -f docker-compose-mlops.yml ps

# ê°œë³„ ì„œë¹„ìŠ¤ í—¬ìŠ¤ í™•ì¸
curl -f http://localhost:5000/health        # MLflow
curl -f http://localhost:8080/health        # Airflow
curl -f http://localhost:6379               # Redis (ì—°ê²° í…ŒìŠ¤íŠ¸)

# PostgreSQL í™•ì¸
docker exec airflow-postgres pg_isready -U airflow -d airflow
```

#### 9.2 ë¡œê·¸ í™•ì¸

```bash
# MLflow ë¡œê·¸
docker compose -f docker-compose-mlops.yml logs -f mlflow

# Airflow Webserver ë¡œê·¸
docker compose -f docker-compose-mlops.yml logs -f airflow-webserver

# Airflow Scheduler ë¡œê·¸
docker compose -f docker-compose-mlops.yml logs -f airflow-scheduler

# ì „ì²´ ë¡œê·¸
docker compose -f docker-compose-mlops.yml logs -f
```

#### 9.3 ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§

```bash
# ì‹¤ì‹œê°„ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
docker stats mlflow airflow-webserver airflow-scheduler airflow-postgres airflow-redis

# ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ í™•ì¸
docker system df -v | grep -E 'mlflow|airflow'
```

---

### ğŸ”Ÿ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ

#### ë¬¸ì œ 1: External network not found

**ì¦ìƒ**:
```
Error response from daemon: network lakehouse-net declared as external, but could not be found
```

**í•´ê²°**:
```bash
# ë„¤íŠ¸ì›Œí¬ ìˆ˜ë™ ìƒì„±
docker network create lakehouse-net

# ë˜ëŠ” ê¸°ì¡´ docker-compose.yml ë¨¼ì € ì‹œì‘
docker compose up -d
```

---

#### ë¬¸ì œ 2: Airflow ì´ˆê¸°í™” ì‹¤íŒ¨

**ì¦ìƒ**:
```
airflow.exceptions.AirflowConfigException: error: cannot use sqlite with the CeleryExecutor
```

**í•´ê²°**:
```bash
# PostgreSQL í—¬ìŠ¤ì²´í¬ í™•ì¸
docker compose -f docker-compose-mlops.yml ps airflow-postgres

# DB ì´ˆê¸°í™” ì¬ì‹œë„
docker compose -f docker-compose-mlops.yml run --rm airflow-webserver airflow db migrate
```

---

#### ë¬¸ì œ 3: MLflow S3 ì—°ê²° ì‹¤íŒ¨

**ì¦ìƒ**:
```
botocore.exceptions.EndpointConnectionError: Could not connect to the endpoint URL
```

**í•´ê²°**:
```bash
# SeaweedFS S3 ìƒíƒœ í™•ì¸
docker compose ps seaweedfs-s3

# MLflow ì»¨í…Œì´ë„ˆì—ì„œ ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸
docker exec mlflow curl -f http://seaweedfs-s3:8333

# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
docker exec mlflow env | grep AWS
```

---

#### ë¬¸ì œ 4: DAG íŒŒì¼ì´ ì¸ì‹ë˜ì§€ ì•ŠìŒ

**ì¦ìƒ**: Airflow UIì— DAGê°€ í‘œì‹œë˜ì§€ ì•ŠìŒ

**í•´ê²°**:
```bash
# 1. íŒŒì¼ ê¶Œí•œ í™•ì¸
ls -l /home/i/work/ai/lakehouse-tick/dags/

# 2. ê¶Œí•œ ìˆ˜ì •
sudo chown -R 50000:50000 /home/i/work/ai/lakehouse-tick/dags/

# 3. Python ë¬¸ë²• ì—ëŸ¬ í™•ì¸
docker exec airflow-scheduler python /opt/airflow/dags/ml_pipeline_dag.py

# 4. Scheduler ì¬ì‹œì‘
docker compose -f docker-compose-mlops.yml restart airflow-scheduler

# 5. ë¡œê·¸ í™•ì¸
docker compose -f docker-compose-mlops.yml logs airflow-scheduler | tail -50
```

---

#### ë¬¸ì œ 5: ë©”ëª¨ë¦¬ ë¶€ì¡±

**ì¦ìƒ**: Airflow ì»¨í…Œì´ë„ˆê°€ ìì£¼ ì¬ì‹œì‘ë¨

**í•´ê²°**:
```bash
# 1. í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
docker stats --no-stream

# 2. docker-compose-mlops.ymlì—ì„œ ë¦¬ì†ŒìŠ¤ ì œì•½ ì¡°ì •
# deploy.resources.limits.memoryë¥¼ 2G â†’ 4Gë¡œ ì¦ê°€

# 3. ë¶ˆí•„ìš”í•œ ì»¨í…Œì´ë„ˆ ì¤‘ì§€
docker compose -f docker-compose-mlops.yml stop airflow-worker  # Celery Worker ì‚¬ìš© ì•ˆ í•  ê²½ìš°
```

---

### 1ï¸âƒ£1ï¸âƒ£ ìš´ì˜ íŒ

#### 11.1 ìë™ ì‹œì‘ ì„¤ì •

```bash
# docker-compose-mlops.ymlì˜ ëª¨ë“  ì„œë¹„ìŠ¤ì— ì¶”ê°€
restart: unless-stopped
```

#### 11.2 ë°±ì—… ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# backup-mlops.sh

BACKUP_DIR="/backups/mlops-$(date +%Y%m%d)"
mkdir -p $BACKUP_DIR

# Airflow DB ë°±ì—…
docker exec airflow-postgres pg_dump -U airflow airflow > $BACKUP_DIR/airflow-db.sql

# MLflow ë°ì´í„° ë°±ì—…
docker exec mlflow tar -czf - /mlflow > $BACKUP_DIR/mlflow-data.tar.gz

# DAG íŒŒì¼ ë°±ì—…
tar -czf $BACKUP_DIR/dags.tar.gz /home/i/work/ai/lakehouse-tick/dags/

echo "âœ… Backup completed: $BACKUP_DIR"
```

#### 11.3 ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ì¶”ê°€

Grafanaì— Airflow ë©”íŠ¸ë¦­ ì¶”ê°€:

```yaml
# docker-compose-mlops.ymlì— Prometheus Exporter ì¶”ê°€
airflow-exporter:
  image: pbweb/airflow-prometheus-exporter:latest
  container_name: airflow-exporter
  environment:
    AIRFLOW_PROMETHEUS_DATABASE_BACKEND: postgres
    AIRFLOW_PROMETHEUS_DATABASE_HOST: airflow-postgres
    AIRFLOW_PROMETHEUS_DATABASE_PORT: 5432
    AIRFLOW_PROMETHEUS_DATABASE_USER: airflow
    AIRFLOW_PROMETHEUS_DATABASE_PASSWORD: airflow
    AIRFLOW_PROMETHEUS_DATABASE_NAME: airflow
  ports:
    - "9112:9112"
  networks:
    - lakehouse-net
  depends_on:
    - airflow-postgres
```

---

### 1ï¸âƒ£2ï¸âƒ£ ì„±ëŠ¥ ìµœì í™”

#### 12.1 Airflow ë™ì‹œ ì‹¤í–‰ Task ìˆ˜ ì¦ê°€

`docker-compose-mlops.yml`ì˜ í™˜ê²½ ë³€ìˆ˜ ì¶”ê°€:

```yaml
environment:
  AIRFLOW__CORE__PARALLELISM: 32           # ì „ì²´ ë™ì‹œ ì‹¤í–‰ Task ìˆ˜
  AIRFLOW__CORE__DAG_CONCURRENCY: 16       # DAGë‹¹ ë™ì‹œ ì‹¤í–‰ Task ìˆ˜
  AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: 3  # DAGë‹¹ ìµœëŒ€ í™œì„± Run ìˆ˜
```

#### 12.2 MLflow ì„±ëŠ¥ íŠœë‹

Production í™˜ê²½ì—ì„œëŠ” SQLite ëŒ€ì‹  PostgreSQL ì‚¬ìš©:

```yaml
mlflow:
  command: >
    mlflow server
    --host 0.0.0.0
    --port 5000
    --backend-store-uri postgresql://mlflow:mlflow@mlflow-postgres:5432/mlflow
    --default-artifact-root s3://lakehouse/mlflow
```

---

### 1ï¸âƒ£3ï¸âƒ£ ì²´í¬ë¦¬ìŠ¤íŠ¸

#### ë°°í¬ ì „ í™•ì¸

- [ ] `docker-compose.yml`ì—ì„œ ë„¤íŠ¸ì›Œí¬ë¥¼ externalë¡œ ë³€ê²½
- [ ] `docker-compose-mlops.yml` íŒŒì¼ ìƒì„±
- [ ] `.env` íŒŒì¼ì— MLOps í™˜ê²½ ë³€ìˆ˜ ì¶”ê°€
- [ ] `dags/`, `logs/`, `plugins/` ë””ë ‰í† ë¦¬ ìƒì„±
- [ ] ë””ë ‰í† ë¦¬ ê¶Œí•œ ì„¤ì • (UID 50000)
- [ ] External ë„¤íŠ¸ì›Œí¬ `lakehouse-net` ì¡´ì¬ í™•ì¸

#### ë°°í¬ ì¤‘ í™•ì¸

- [ ] ê¸°ì¡´ Lakehouse ì¸í”„ë¼ ì •ìƒ ì‹¤í–‰ (`docker compose ps`)
- [ ] MLOps ìŠ¤íƒ ì‹œì‘ (`docker compose -f docker-compose-mlops.yml up -d`)
- [ ] Airflow DB ì´ˆê¸°í™” (`airflow db migrate`)
- [ ] Admin ì‚¬ìš©ì ìƒì„±
- [ ] MLflow UI ì ‘ì† í™•ì¸ (http://localhost:5000)
- [ ] Airflow UI ì ‘ì† í™•ì¸ (http://localhost:8080)

#### ë°°í¬ í›„ í™•ì¸

- [ ] ìƒ˜í”Œ DAG íŒŒì¼ ì‘ì„± ë° ë°°í¬
- [ ] Airflow UIì—ì„œ DAG ì¸ì‹ í™•ì¸
- [ ] DAG ìˆ˜ë™ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
- [ ] MLflowì—ì„œ ì‹¤í—˜ ë¡œê·¸ í™•ì¸
- [ ] ë„¤íŠ¸ì›Œí¬ ì—°ê²° í…ŒìŠ¤íŠ¸ (Airflow â†’ MLflow, Airflow â†’ Trino)
- [ ] ë¡œê·¸ íŒŒì¼ ì •ìƒ ìƒì„± í™•ì¸

---

### 1ï¸âƒ£4ï¸âƒ£ ë‹¤ìŒ ë‹¨ê³„

1. **DAG íŒŒì¼ í™•ì¥**: ì‹¤ì œ Spark Job ì—°ë™ (`SparkSubmitOperator`)
2. **Slack ì•Œë¦¼ ì„¤ì •**: Task ì‹¤íŒ¨ ì‹œ Slack ì•Œë¦¼
3. **ëª¨ë‹ˆí„°ë§ ê°•í™”**: Prometheus + Grafana ëŒ€ì‹œë³´ë“œ ì¶”ê°€
4. **ë³´ì•ˆ ê°•í™”**: RBAC ì„¤ì •, SSL/TLS ì ìš©
5. **ë°±ì—… ìë™í™”**: Cronìœ¼ë¡œ ì •ê¸° ë°±ì—… ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰

---

**ì‘ì„±**: 2025-12-25
**ë²„ì „**: 1.1 (ì˜µì…˜ 2 êµ¬í˜„ ê°€ì´ë“œ ì¶”ê°€)
**ë‹¤ìŒ ë‹¨ê³„**: [GETTING_STARTED.md](../../GETTING_STARTED.md) - ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ
