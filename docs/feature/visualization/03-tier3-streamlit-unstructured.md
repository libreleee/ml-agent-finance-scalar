# Tier 3: ë¹„ì •í˜• ë°ì´í„° ì‹œê°í™” (Streamlit + PyIceberg)

## ğŸ–¼ï¸ ê°œìš”

**ëŒ€ìƒ ë°ì´í„°**: `s3a://lakehouse/raw/images/` + `hive_prod.media_db.image_metadata` (Iceberg)
**ì‚¬ìš© ë„êµ¬**: Streamlit + PyIceberg + boto3
**í•µì‹¬ ì½”ë“œ**: `python/fspark_raw_examples.py` (ë¼ì¸ 92-121)
**ì£¼ìš” ê¸°ëŠ¥**: ì´ë¯¸ì§€ ê°¤ëŸ¬ë¦¬, ë©”íƒ€ë°ì´í„° ê²€ìƒ‰, í†µê³„
**ì‚¬ìš©ì**: ë°ì´í„° ê³¼í•™ì, ë¶„ì„ê°€

---

## ğŸ¯ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ë‹¹ì‹ ì˜ ì½”ë“œ ì‹¤í–‰ ê²°ê³¼    â”‚
â”‚ (fspark_raw_examples.py) â”‚
â”‚                          â”‚
â”‚ S3 ì´ë¯¸ì§€ ì—…ë¡œë“œ         â”‚
â”‚ (ë¼ì¸ 92-121)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    S3 ì €ì¥ì†Œ             â”‚
â”‚ (lakehouse/raw/images/)  â”‚
â”‚                          â”‚
â”‚ ğŸ“¸ 2025-12-25/image1.png â”‚
â”‚ ğŸ“¸ 2025-12-25/image2.png â”‚
â”‚ ...                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚                      â”‚
             â†“                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Iceberg ë©”íƒ€ë°ì´í„°      â”‚ â”‚    boto3 (S3 ì ‘ê·¼)       â”‚
â”‚  image_metadata í…Œì´ë¸”   â”‚ â”‚                          â”‚
â”‚                          â”‚ â”‚ ì´ë¯¸ì§€ ë°”ì´íŠ¸ ë¡œë“œ       â”‚
â”‚ - image_id              â”‚ â”‚                          â”‚
â”‚ - s3_path               â”‚ â”‚ PIL ì´ë¯¸ì§€ ë³€í™˜          â”‚
â”‚ - upload_time           â”‚ â”‚                          â”‚
â”‚ - file_size             â”‚ â”‚ Streamlit ë Œë”ë§         â”‚
â”‚ - tag                   â”‚ â”‚                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                           â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚     Streamlit App        â”‚
                â”‚    (Port: 8501)          â”‚
                â”‚                          â”‚
                â”‚ ğŸ“¸ Image Gallery         â”‚
                â”‚ ğŸ” Metadata Search       â”‚
                â”‚ ğŸ“Š Statistics Dashboard  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                   ğŸ‘¤ Data Scientists
                   (Image exploration)
```

---

## ğŸ’¡ í•µì‹¬ ê°œë…: ë‹¹ì‹ ì˜ ì½”ë“œ ë¶„ì„

### ì„ íƒëœ ì½”ë“œ (ë¼ì¸ 92-105)

```python
# ë¼ì¸ 92: ë‚ ì§œë³„ íŒŒí‹°ì…”ë‹ ê²½ë¡œ ìƒì„±
image_s3_path = "s3a://lakehouse/raw/images/{date}/sample.txt".format(
    date=datetime.utcnow().strftime('%Y-%m-%d')
)

# ë¼ì¸ 96-98: Hadoop FileSystem ì´ˆê¸°í™”
jconf = spark._jsc.hadoopConfiguration()
fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(
    spark._jvm.java.net.URI(image_s3_path), jconf
)
path = spark._jvm.org.apache.hadoop.fs.Path(image_s3_path)

# ë¼ì¸ 101-103: ë°”ì´ë„ˆë¦¬ ë°ì´í„° ì“°ê¸°
out = fs.create(path, True)
out.write(bytearray(sample_bytes))  # â† í•µì‹¬ íŒ¨í„´
out.close()
```

### íŒ¨í„´ ë¶„ì„

| ìš”ì†Œ | ì„¤ëª… | Streamlitì—ì„œì˜ í™œìš© |
|------|------|-------------------|
| **ë‚ ì§œ íŒŒí‹°ì…”ë‹** | `{date}` ë””ë ‰í† ë¦¬ | ë‚ ì§œ í•„í„°ë¡œ S3 ê²½ë¡œ ìë™ ìƒì„± |
| **Hadoop API** | FileSystem ì§ì ‘ ì ‘ê·¼ | S3 ë©”íƒ€ë°ì´í„° ì¿¼ë¦¬ ëŒ€ì‹  ì§ì ‘ ì ‘ê·¼ |
| **ë°”ì´ë„ˆë¦¬ ì²˜ë¦¬** | `bytearray()` ë³€í™˜ | PIL Imageë¡œ ë³€í™˜ í›„ ë Œë”ë§ |
| **ê²½ë¡œ ê²€ì¦** | íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ | Iceberg ë©”íƒ€ë°ì´í„°ë¡œ ê²€ì¦ |

---

## ğŸ“ ë°ì´í„° êµ¬ì¡°

### Iceberg ë©”íƒ€ë°ì´í„° í…Œì´ë¸”

```sql
CREATE TABLE hive_prod.media_db.image_metadata (
    image_id STRING NOT NULL,                  -- UUID
    s3_path STRING NOT NULL,                   -- s3a://lakehouse/raw/images/2025-12-25/image1.png
    file_size BIGINT,                          -- ë°”ì´íŠ¸
    mime_type STRING,                          -- image/png
    upload_time TIMESTAMP,                     -- ì—…ë¡œë“œ ì‹œê°
    source_system STRING,                      -- 'manual', 'batch'
    tag STRING,                                -- 'product', 'user', 'analytics'
    width INT,                                 -- í”½ì…€
    height INT,                                -- í”½ì…€
    checksum STRING,                           -- MD5
    is_indexed BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP,
    updated_at TIMESTAMP
)
USING iceberg
PARTITIONED BY (days(upload_time), tag)
```

### DDL ì‹¤í–‰

```sql
-- Trinoì—ì„œ í…Œì´ë¸” ìƒì„±
CREATE SCHEMA IF NOT EXISTS hive_prod.media_db;

CREATE TABLE hive_prod.media_db.image_metadata (
    image_id STRING NOT NULL,
    s3_path STRING NOT NULL,
    file_size BIGINT,
    mime_type STRING,
    upload_time TIMESTAMP,
    source_system STRING,
    tag STRING,
    width INT,
    height INT,
    checksum STRING,
    is_indexed BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP,
    updated_at TIMESTAMP
)
USING iceberg
PARTITIONED BY (days(upload_time), tag)
TBLPROPERTIES (
    'write.format.default' = 'parquet',
    'write.metadata.compression-codec' = 'gzip'
);
```

---

## ğŸš€ êµ¬í˜„ ë‹¨ê³„

### Step 1: ì´ë¯¸ì§€ ì—…ë¡œë“œ (ë‹¹ì‹ ì˜ ì½”ë“œ ì‹¤í–‰)

```bash
cd /home/i/work/ai/lakehouse-tick/python
python fspark_raw_examples.py

# ì¶œë ¥:
# ë¹„ì •í˜•(ë°”ì´ë„ˆë¦¬) íŒŒì¼ ì €ì¥ ì™„ë£Œ -> s3a://lakehouse/raw/images/2025-12-25/sample.txt
# ë¡œì»¬ ì´ë¯¸ì§€ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ -> s3a://lakehouse/raw/images/2025-12-25/image1.png
# raw/ ê²½ë¡œì— ìˆëŠ” í•­ëª© ìˆ˜: 2
```

### Step 2: ë©”íƒ€ë°ì´í„° í…Œì´ë¸” ìƒì„±

```bash
# Trino CLI ì ‘ì†
docker exec -it trino trino --server localhost:8080 --catalog hive_prod

# DDL ì‹¤í–‰ (ìœ„ì˜ SQL ë³µì‚¬)
```

### Step 3: ìƒ˜í”Œ ë©”íƒ€ë°ì´í„° INSERT

```sql
INSERT INTO hive_prod.media_db.image_metadata VALUES
('img-001', 's3a://lakehouse/raw/images/2025-12-25/image1.png', 102400, 'image/png',
 TIMESTAMP '2025-12-25 10:00:00', 'manual', 'product', 800, 600,
 'abc123def456', FALSE, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP);
```

### Step 4: Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜ íŒŒì¼ ìƒì„±

#### ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
streamlit-app/
â”œâ”€â”€ app.py
â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ 01_Gallery.py
â”‚   â”œâ”€â”€ 02_ğŸ”_Metadata_Search.py
â”‚   â””â”€â”€ 03_Statistics.py
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ iceberg_connector.py
â”‚   â”œâ”€â”€ s3_utils.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ .streamlit/
    â””â”€â”€ config.toml
```

#### `requirements.txt`

```txt
streamlit==1.30.0
pyiceberg==0.5.1
pandas==2.1.4
boto3==1.34.0
Pillow==10.1.0
pyarrow==14.0.0
python-dotenv==1.0.0
```

#### `modules/iceberg_connector.py`

```python
from pyiceberg.catalog import load_catalog
import os

def get_iceberg_table(table_name):
    """Iceberg í…Œì´ë¸” ë¡œë“œ"""
    catalog = load_catalog("default", **{
        "type": "hive",
        "uri": os.getenv("HIVE_METASTORE_URI", "thrift://hive-metastore:9083"),
        "s3.endpoint": os.getenv("AWS_ENDPOINT_URL_S3", "http://seaweedfs-s3:8333"),
        "s3.access-key-id": os.getenv("AWS_ACCESS_KEY_ID", "seaweedfs_access_key"),
        "s3.secret-access-key": os.getenv("AWS_SECRET_ACCESS_KEY", "seaweedfs_secret_key"),
        "s3.path-style-access": "true"
    })

    return catalog.load_table(table_name)
```

#### `modules/s3_utils.py`

```python
import boto3
import os

def get_s3_client():
    """SeaweedFS S3 í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    return boto3.client(
        's3',
        endpoint_url=os.getenv('AWS_ENDPOINT_URL_S3', 'http://seaweedfs-s3:8333'),
        aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID', 'seaweedfs_access_key'),
        aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY', 'seaweedfs_secret_key'),
        region_name='us-east-1'
    )
```

#### `app.py` (ë©”ì¸ ë„¤ë¹„ê²Œì´ì…˜)

```python
import streamlit as st
from modules.iceberg_connector import get_iceberg_table
from modules.s3_utils import get_s3_client

st.set_page_config(
    page_title="Lakehouse Unstructured Data Explorer",
    page_icon="ğŸ–¼ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.title("ğŸ–¼ï¸ Lakehouse Unstructured Data Explorer")
st.markdown("""
ë¹„ì •í˜• ë°ì´í„°(ì´ë¯¸ì§€, ë¹„ë””ì˜¤, ì˜¤ë””ì˜¤)ë¥¼ íƒìƒ‰í•˜ê³  ê´€ë¦¬í•˜ëŠ” í†µí•© ë„êµ¬ì…ë‹ˆë‹¤.

**ê¸°ëŠ¥**:
- ğŸ“¸ **ì´ë¯¸ì§€ ê°¤ëŸ¬ë¦¬**: S3ì— ì €ì¥ëœ ì´ë¯¸ì§€ë¥¼ ë‚ ì§œ/íƒœê·¸ë³„ë¡œ í•„í„°ë§í•˜ì—¬ í™•ì¸
- ğŸ” **ë©”íƒ€ë°ì´í„° ê²€ìƒ‰**: ì´ë¯¸ì§€ ì†ì„±ìœ¼ë¡œ ê²€ìƒ‰
- ğŸ“Š **í†µê³„ ëŒ€ì‹œë³´ë“œ**: ì €ì¥ì†Œ ì‚¬ìš©ëŸ‰, íŒŒì¼ í˜•ì‹ ë¶„í¬ ë“±

**ë°ì´í„° ì†ŒìŠ¤**:
- **S3**: `s3a://lakehouse/raw/images/`
- **ë©”íƒ€ë°ì´í„°**: `hive_prod.media_db.image_metadata` (Iceberg)
""")

st.markdown("---")

# ì—°ê²° ìƒíƒœ í™•ì¸
with st.sidebar:
    st.header("Connection Status")
    try:
        table = get_iceberg_table("hive_prod.media_db.image_metadata")
        df = table.scan().limit(1).to_pandas()
        st.success("âœ… Iceberg ì—°ê²° ì„±ê³µ")
    except Exception as e:
        st.error(f"âŒ Iceberg ì—°ê²° ì‹¤íŒ¨: {e}")

    try:
        s3 = get_s3_client()
        s3.list_objects_v2(Bucket='lakehouse', Prefix='raw/images/', MaxKeys=1)
        st.success("âœ… S3 ì—°ê²° ì„±ê³µ")
    except Exception as e:
        st.error(f"âŒ S3 ì—°ê²° ì‹¤íŒ¨: {e}")
```

#### `pages/01_Gallery.py` (ì´ë¯¸ì§€ ê°¤ëŸ¬ë¦¬)

```python
import streamlit as st
import pandas as pd
from modules.iceberg_connector import get_iceberg_table
from modules.s3_utils import get_s3_client
from io import BytesIO
from PIL import Image

st.set_page_config(page_title="Image Gallery", page_icon="ğŸ–¼ï¸", layout="wide")
st.title("ğŸ–¼ï¸ Image Gallery")

# í•„í„°
with st.sidebar:
    st.header("Filters")
    selected_tag = st.selectbox("Tag", ['all', 'product', 'user', 'analytics'])
    date_range = st.date_input("Upload Date Range", [])
    size_range = st.slider("File Size (KB)", 0, 10000, (0, 10000))

# ë©”íƒ€ë°ì´í„° ë¡œë“œ
@st.cache_data(ttl=300)
def load_metadata(tag, date_range, size_range):
    table = get_iceberg_table("hive_prod.media_db.image_metadata")
    df = table.scan().to_pandas()

    if tag != 'all':
        df = df[df['tag'] == tag]
    if len(date_range) == 2:
        df = df[(df['upload_time'] >= pd.Timestamp(date_range[0])) &
                (df['upload_time'] <= pd.Timestamp(date_range[1]))]
    df = df[(df['file_size'] >= size_range[0] * 1024) &
            (df['file_size'] <= size_range[1] * 1024)]

    return df.sort_values('upload_time', ascending=False)

df = load_metadata(selected_tag, date_range, size_range)

# í†µê³„
col1, col2, col3, col4 = st.columns(4)
with col1:
    st.metric("Total Images", len(df))
with col2:
    st.metric("Total Size (MB)", f"{df['file_size'].sum() / 1024 / 1024:.2f}")
with col3:
    st.metric("Avg Size (KB)", f"{df['file_size'].mean() / 1024:.2f}")
with col4:
    st.metric("Unique Tags", df['tag'].nunique())

st.markdown("---")

# í˜ì´ì§€ë„¤ì´ì…˜
items_per_page = 20
total_pages = (len(df) - 1) // items_per_page + 1
page_number = st.selectbox("Page", range(1, total_pages + 1)) if total_pages > 1 else 1

start_idx = (page_number - 1) * items_per_page
end_idx = min(start_idx + items_per_page, len(df))

# ê°¤ëŸ¬ë¦¬ ë Œë”ë§ (4ì—´ ê·¸ë¦¬ë“œ)
st.subheader(f"Showing {start_idx + 1}-{end_idx} of {len(df)} images")
cols = st.columns(4)

s3_client = get_s3_client()

for idx, (_, row) in enumerate(df.iloc[start_idx:end_idx].iterrows()):
    col = cols[idx % 4]
    with col:
        try:
            # S3ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ
            s3_path = row['s3_path'].replace('s3a://', '')
            bucket, key = s3_path.split('/', 1)

            response = s3_client.get_object(Bucket=bucket, Key=key)
            image_bytes = response['Body'].read()

            image = Image.open(BytesIO(image_bytes))
            st.image(image, use_container_width=True)
            st.caption(f"**{row['image_id']}**")

            with st.expander("ğŸ“‹ Metadata"):
                st.json({
                    "ID": row['image_id'],
                    "Size": f"{row['file_size'] / 1024:.2f} KB",
                    "Type": row['mime_type'],
                    "Dimensions": f"{row['width']}x{row['height']}",
                    "Upload Time": str(row['upload_time']),
                    "Tag": row['tag']
                })
        except Exception as e:
            st.error(f"Failed to load {row['image_id']}: {e}")
```

---

## ğŸ–¼ï¸ ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤

### ì‹œë‚˜ë¦¬ì˜¤: ë°ì´í„° ê³¼í•™ì

```
Streamlit ì ‘ì† (http://localhost:8501)
  â†“
ğŸ“¸ "Image Gallery" í˜ì´ì§€ í´ë¦­
  â†“
í•„í„° ì„¤ì •:
  - Tag: "product"
  - Date: ìµœê·¼ 7ì¼
  - File Size: 0-1000 KB
  â†“
âœ… "ë‹¹ì‹ ì˜ ì½”ë“œ" íŒ¨í„´ìœ¼ë¡œ ì—…ë¡œë“œëœ ì´ë¯¸ì§€ 5ê°œ í‘œì‹œ
  â†“
ğŸ“Š ê° ì´ë¯¸ì§€ í´ë¦­ â†’ ë©”íƒ€ë°ì´í„° í™•ì¸
  (íŒŒì¼ëª…, í¬ê¸°, ìƒì„±ì¼, íƒœê·¸)
  â†“
ğŸ“Š "Statistics" í˜ì´ì§€ë¡œ ì´ë™
  â†“
ğŸ“ˆ íƒœê·¸ë³„ ì´ë¯¸ì§€ ê°œìˆ˜ ë¶„í¬ í™•ì¸
  â†“
ğŸ’¾ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ìœ¼ë¡œ ì €ì¥
  â†“
âœ… ë¶„ì„ ì‹œì‘
```

---

## ğŸ”§ Docker ë°°í¬

### docker-compose.yml ì¶”ê°€

```yaml
streamlit:
  image: python:3.11-slim
  container_name: streamlit-app
  working_dir: /app
  depends_on:
    hive-metastore:
      condition: service_healthy
    seaweedfs-s3:
      condition: service_healthy
  ports:
    - "8501:8501"
  environment:
    AWS_ACCESS_KEY_ID: seaweedfs_access_key
    AWS_SECRET_ACCESS_KEY: seaweedfs_secret_key
    AWS_ENDPOINT_URL_S3: http://seaweedfs-s3:8333
    HIVE_METASTORE_URI: thrift://hive-metastore:9083
  volumes:
    - ./streamlit-app:/app
    - ./logs/streamlit:/app/logs
  networks:
    - default
  command: >
    bash -c "
    pip install --no-cache-dir -r requirements.txt &&
    streamlit run app.py --server.port=8501
    "
```

### ì‹¤í–‰

```bash
docker-compose up -d streamlit

# ì ‘ì†
# http://localhost:8501
```

---

## âš™ï¸ ì„±ëŠ¥ ìµœì í™”

### 1. ìºì‹± (5ë¶„)

```python
@st.cache_data(ttl=300)
def load_metadata(...):
    ...
```

### 2. í˜ì´ì§€ë„¤ì´ì…˜ (20ê°œì”©)

```python
items_per_page = 20
df.iloc[start_idx:end_idx]
```

### 3. ì¸ë„¤ì¼ ì €ì¥ (S3)

```python
# ë‹¹ì‹ ì˜ ì½”ë“œ íŒ¨í„´ í™•ì¥
thumb_s3_path = "s3a://lakehouse/raw/thumbnails/{date}/thumb_{filename}"
# ... Hadoop FileSystem íŒ¨í„´ìœ¼ë¡œ ì—…ë¡œë“œ
```

---

## ğŸ“š ë‹¤ìŒ ë‹¨ê³„

1. âœ… Tier 1 ì™„ë£Œ: [Superset + Trino](./01-tier1-superset-trino-structured.md)
2. âœ… Tier 2 ì™„ë£Œ: [Grafana + OpenSearch](./02-tier2-grafana-opensearch-semistructured.md)
3. âœ… Tier 3 ì™„ë£Œ: í˜„ì¬ ë¬¸ì„œ
4. ğŸ‘‰ [ì „ì²´ í†µí•© ê°€ì´ë“œ](./README.md)ë¡œ ì´ë™

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸ (15ê°œ í•­ëª©)

- [ ] Iceberg ë©”íƒ€ë°ì´í„° í…Œì´ë¸” ìƒì„±
- [ ] ìƒ˜í”Œ ë©”íƒ€ë°ì´í„° INSERT
- [ ] Python ë² ì´ìŠ¤ ì´ë¯¸ì§€ ì„ íƒ
- [ ] requirements.txt ì‘ì„±
- [ ] Docker ì»¨í…Œì´ë„ˆ ì„¤ì •
- [ ] í¬íŠ¸ ë§¤í•‘ (8501)
- [ ] ë³¼ë¥¨ ë§ˆìš´íŠ¸
- [ ] `app.py` ì‘ì„±
- [ ] `pages/01_Gallery.py` ì‘ì„±
- [ ] `modules/iceberg_connector.py` ì‘ì„±
- [ ] `modules/s3_utils.py` ì‘ì„±
- [ ] PyIceberg ì—°ê²° í…ŒìŠ¤íŠ¸
- [ ] S3 ì—°ê²° í…ŒìŠ¤íŠ¸
- [ ] ì´ë¯¸ì§€ ê°¤ëŸ¬ë¦¬ ë Œë”ë§ í™•ì¸
- [ ] ë©”íƒ€ë°ì´í„° í•„í„°ë§ í™•ì¸

---

**ì¶•í•˜í•©ë‹ˆë‹¤!** ì´ì œ ë¹„ì •í˜• ë°ì´í„° ì‹œê°í™” ê³„ì¸µì´ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ğŸ‰

ë‹¹ì‹ ì´ ì„ íƒí•œ ì½”ë“œ(`fspark_raw_examples.py:92-121`)ê°€ ì™„ë²½í•˜ê²Œ í†µí•©ë˜ì–´ Streamlit ê°¤ëŸ¬ë¦¬ì—ì„œ ì‘ë™í•©ë‹ˆë‹¤! ğŸ–¼ï¸
