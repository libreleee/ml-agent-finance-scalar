# Modern Data Lakehouse: í†µí•© ì‹œê°í™” ë° ë°ì´í„° ë¶„ì„ ì „ëµ ê°€ì´ë“œ ğŸš€

## ì „ë¬¸ê°€ ì œì–¸: Modern Data Stack ì‹œê°í™” ì „ëµ
ë¹…ë°ì´í„° ë° ML ì›Œí¬í”Œë¡œìš° ì „ë¬¸ê°€ë¡œì„œ, ë³¸ í”„ë¡œì íŠ¸ì˜ **Lakehouse(SeaweedFS + Iceberg + Spark)** ì•„í‚¤í…ì²˜ì— ìµœì í™”ëœ í†µí•© ì‹œê°í™” ì „ëµì„ ì œì•ˆí•©ë‹ˆë‹¤. ë‹¨ìˆœíˆ ë°ì´í„°ë¥¼ ë³´ëŠ” ê²ƒì„ ë„˜ì–´, **ì •í˜•/ë°˜ì •í˜•/ë¹„ì •í˜• ë°ì´í„°ì˜ íŠ¹ì„±ì— ë”°ë¥¸ ê³„ì¸µë³„ ì‹œê°í™” ì†”ë£¨ì…˜**ì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì´ í•µì‹¬ì…ë‹ˆë‹¤.

---

## ğŸ›ï¸ í†µí•© ì‹œê°í™” ì•„í‚¤í…ì²˜ (Recommended Stack)

| ë°ì´í„° ìœ í˜• | ì¶”ì²œ ì†”ë£¨ì…˜ | êµ¬í˜„ ë‚œì´ë„ | ì˜ˆìƒ ì†Œìš” ì‹œê°„ | í•µì‹¬ ê¸°ìˆ  |
| :--- | :--- | :--- | :--- | :--- |
| **ì •í˜• (Structured)** | **Superset + Trino** | **ìƒ (High)** | 2~3ì¼ | Iceberg Connector, SQL |
| **ë°˜ì •í˜• (Semi-structured)** | **Grafana + OpenSearch** | **ì¤‘ (Medium)** | 1~2ì¼ | JSON Parsing, Time-series |
| **ë¹„ì •í˜• (Unstructured)** | **Streamlit** | **í•˜ (Low)** | 0.5ì¼ | Python SDK, Metadata Mapping |

---

## 1ï¸âƒ£ Tier 1: ì—”í„°í”„ë¼ì´ì¦ˆ ë¶„ì„ (Trino + Apache Superset) ğŸ’
ê°€ì¥ ê°•ë ¥í•˜ê³  í™•ì¥ì„± ìˆëŠ” ì¡°í•©ì…ë‹ˆë‹¤. **Trino**ë¥¼ í†µí•© ì¿¼ë¦¬ ì—”ì§„ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ Iceberg í…Œì´ë¸”ì— ì§ì ‘ ì ‘ê·¼í•˜ê³ , **Superset**ì—ì„œ ì‹œê°í™”í•©ë‹ˆë‹¤.

- **Expert Tip**: Trinoì˜ **Federated Query** ê¸°ëŠ¥ì„ í™œìš©í•˜ë©´ S3(Iceberg) ë°ì´í„°ì™€ ì™¸ë¶€ RDB ë°ì´í„°ë¥¼ ì¡°ì¸í•˜ì—¬ í•˜ë‚˜ì˜ ëŒ€ì‹œë³´ë“œì—ì„œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **êµ¬í˜„ í¬ì¸íŠ¸**:
  - Trino Iceberg Connector ì„¤ì • (`etc/catalog/iceberg.properties`)
  - Supersetì—ì„œ Trino SQLAlchemy URI ì—°ê²° (`trino://user@trino-host:8080/iceberg`)
  - **Materialized View**ë¥¼ í™œìš©í•˜ì—¬ ëŒ€ê·œëª¨ ë°ì´í„° ì¡°íšŒ ì„±ëŠ¥ ìµœì í™”

---

## 2ï¸âƒ£ Tier 2: ì¸í„°ë™í‹°ë¸Œ ë°ì´í„° ì•± (Streamlit) âš¡
ë¹„ì •í˜• ë°ì´í„°(ì´ë¯¸ì§€, ì˜¤ë””ì˜¤)ë‚˜ ML ì›Œí¬í”Œë¡œìš°ì˜ ì¤‘ê°„ ê²°ê³¼ë¬¼ì„ í™•ì¸í•˜ëŠ” ë° ìµœì ì…ë‹ˆë‹¤.

- **Expert Tip**: ë¹„ì •í˜• ë°ì´í„°ì˜ ê²½ìš°, S3 ê²½ë¡œì™€ ë©”íƒ€ë°ì´í„°(ë¼ë²¨, ìƒì„±ì¼ ë“±)ë¥¼ Iceberg í…Œì´ë¸”ì— ì €ì¥í•˜ê³ , Streamlitì—ì„œ ì´ í…Œì´ë¸”ì„ ì½ì–´ **ì´ë¯¸ì§€ ê°¤ëŸ¬ë¦¬**ë‚˜ **ì˜¤ë””ì˜¤ í”Œë ˆì´ì–´**ë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±í•˜ì‹­ì‹œì˜¤.
- **êµ¬í˜„ ì˜ˆì‹œ**:
  ```python
  import streamlit as st
  import pandas as pd
  from pyiceberg.catalog import load_catalog

  # Iceberg ë©”íƒ€ë°ì´í„° ì¡°íšŒ (ë¹„ì •í˜• ë°ì´í„° ë§¤í•‘)
  catalog = load_catalog("default")
  table = catalog.load_table("logs_db.unstructured_meta")
  df = table.scan().to_pandas()

  st.title("ğŸ–¼ï¸ Unstructured Data Explorer")
  selected_tag = st.selectbox("Filter by Tag", df['tag'].unique())
  
  # S3 URLì„ ì´ìš©í•œ ì´ë¯¸ì§€ ë Œë”ë§
  filtered_df = df[df['tag'] == selected_tag]
  for url in filtered_df['s3_url']:
      st.image(url, caption=url.split('/')[-1])
  ```

---

## 3ï¸âƒ£ Tier 3: ìš´ì˜ ê°€ì‹œì„± ë° ë¡œê·¸ ë¶„ì„ (Grafana) ğŸ“Š
ë°˜ì •í˜• ë¡œê·¸ ë°ì´í„°ì™€ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ë° í•„ìˆ˜ì ì…ë‹ˆë‹¤.

- **Expert Tip**: Spark ì‘ì—… ë¡œê·¸ë‚˜ SeaweedFS ìƒíƒœ ë©”íŠ¸ë¦­ì„ Prometheus/OpenSearchë¡œ ìˆ˜ì§‘í•˜ê³  Grafana ëŒ€ì‹œë³´ë“œì— í†µí•©í•˜ì‹­ì‹œì˜¤. ë°ì´í„° ë ˆì´í¬ì˜ **Health Check**ì™€ **Data Quality** ëª¨ë‹ˆí„°ë§ì„ ìë™í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## ï¿½ êµ¬í˜„ ë‚œì´ë„ ë° ì†Œìš” ì‹œê°„ ìƒì„¸ ë¶„ì„

### 1. Apache Superset + Trino (ë‚œì´ë„: ìƒ / ì†Œìš”: 2~3ì¼)
- **ì´ìœ **: Trinoì˜ ë¶„ì‚° í´ëŸ¬ìŠ¤í„° ì„¤ì •, Iceberg ì»¤ë„¥í„° íŠœë‹, Supersetì˜ ì¸ì¦(OAuth/LDAP) ë° DB ë“œë¼ì´ë²„ ì„¤ì • ë“± ì¸í”„ë¼ì  ìš”ì†Œê°€ ë§ìŠµë‹ˆë‹¤.
- **í•µì‹¬ ì‘ì—…**: Trino Catalog ì„¤ì •, Superset Docker ë°°í¬, Semantic Layer(Virtual Dataset) ì •ì˜.

### 2. Grafana + OpenSearch (ë‚œì´ë„: ì¤‘ / ì†Œìš”: 1~2ì¼)
- **ì´ìœ **: OpenSearchì˜ ì¸ë±ìŠ¤ ì„¤ê³„ì™€ ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸(Fluentd/Logstash) ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤. Grafana ìì²´ëŠ” ì„¤ì •ì´ ê°„í¸í•˜ì§€ë§Œ, ìœ ì˜ë¯¸í•œ ëŒ€ì‹œë³´ë“œ êµ¬ì„±ì„ ìœ„í•œ ì¿¼ë¦¬ ì‘ì„±ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
- **í•µì‹¬ ì‘ì—…**: Index Template ì„¤ê³„, Grafana DataSource ì—°ê²°, Alerting Rule ì„¤ì •.

### 3. Streamlit (ë‚œì´ë„: í•˜ / ì†Œìš”: 0.5ì¼)
- **ì´ìœ **: ìˆœìˆ˜ Python ì½”ë“œë¡œ ì‘ì„±ë˜ë©°, ë³µì¡í•œ í”„ë¡ íŠ¸ì—”ë“œ ì§€ì‹ ì—†ì´ë„ ë°ì´í„° í”„ë ˆì„ê³¼ ì´ë¯¸ì§€ë¥¼ ì¦‰ì‹œ ë Œë”ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **í•µì‹¬ ì‘ì—…**: PySpark/PyIceberg ì—°ê²° ì½”ë“œ ì‘ì„±, UI ì»´í¬ë„ŒíŠ¸ ë°°ì¹˜.

---

## ï¿½ğŸ› ï¸ ì „ë¬¸ê°€ì˜ ë¡œë“œë§µ (Implementation Roadmap)

1.  **Foundation**: Trinoë¥¼ ì„¤ì¹˜í•˜ê³  Iceberg ì¹´íƒˆë¡œê·¸ë¥¼ ì—°ê²°í•˜ì—¬ SQL ê¸°ë°˜ ë¶„ì„ í™˜ê²½ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.
2.  **BI Layer**: Apache Supersetì„ ë°°í¬í•˜ì—¬ ì£¼ìš” ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œ(ì •í˜• ë°ì´í„°) ëŒ€ì‹œë³´ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
3.  **App Layer**: Streamlitì„ í™œìš©í•˜ì—¬ ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ì™€ ë¶„ì„ê°€ë¥¼ ìœ„í•œ ë¹„ì •í˜• ë°ì´í„° íƒìƒ‰ ë„êµ¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
4.  **Observability**: Grafanaë¥¼ ì—°ë™í•˜ì—¬ ì „ì²´ íŒŒì´í”„ë¼ì¸ì˜ ì•ˆì •ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤.

---

## ğŸ† í˜„ì—… ì±„íƒë¥  ë° íŠ¸ë Œë“œ (Industry Standard)

í˜„ì—…(Production) í™˜ê²½ì—ì„œ ê°€ì¥ ì••ë„ì ìœ¼ë¡œ ë§ì´ ì‚¬ìš©ë˜ëŠ” ì¡°í•©ì€ **Tier 1 (Superset + Trino)** ì…ë‹ˆë‹¤.

### ì™œ Superset + Trino ì¸ê°€?
- **ì••ë„ì  ë²”ìš©ì„±**: ë°ì´í„° ì—”ì§€ë‹ˆì–´, ë¶„ì„ê°€, ë¹„ì¦ˆë‹ˆìŠ¤ ìœ ì € ëª¨ë‘ê°€ SQL ê¸°ë°˜ìœ¼ë¡œ ì†Œí†µí•  ìˆ˜ ìˆëŠ” ê°€ì¥ í‘œì¤€ì ì¸ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.
- **ì„±ëŠ¥ê³¼ ë¹„ìš©**: ê³ ê°€ì˜ ìƒìš© BI(Tableau, PowerBI) ëŒ€ë¹„ ë¼ì´ì„ ìŠ¤ ë¹„ìš©ì´ ì—†ìœ¼ë©°, Trinoì˜ ë¶„ì‚° ì²˜ë¦¬ ëŠ¥ë ¥ ë•ë¶„ì— ëŒ€ê·œëª¨ Iceberg í…Œì´ë¸” ì¡°íšŒ ì‹œ ê°€ì¥ ë¹ ë¥¸ ì‘ë‹µ ì†ë„ë¥¼ ë³´ì…ë‹ˆë‹¤.
- **ì»¤ë®¤ë‹ˆí‹° ì§€ì›**: Netflix, Uber, Airbnb ë“± ê¸€ë¡œë²Œ í…Œí¬ ê¸°ì—…ë“¤ì´ ë©”ì¸ ìŠ¤íƒìœ¼ë¡œ ì‚¬ìš©í•˜ê³  ìˆì–´ ë ˆí¼ëŸ°ìŠ¤ì™€ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ì •ë³´ê°€ ë§¤ìš° í’ë¶€í•©ë‹ˆë‹¤.

### ìµœê·¼ íŠ¸ë Œë“œ: "Hybrid Approach"
ìµœê·¼ì—ëŠ” í•˜ë‚˜ë§Œ ì„ íƒí•˜ì§€ ì•Šê³  ë‹¤ìŒê³¼ ê°™ì´ ë³‘í–‰í•˜ëŠ” ê²ƒì´ ê¸€ë¡œë²Œ í‘œì¤€ì…ë‹ˆë‹¤:
1.  **ì „ì‚¬ ì§€í‘œ/ëŒ€ì‹œë³´ë“œ**: Superset + Trino (ì•ˆì •ì„±, ê¶Œí•œ ê´€ë¦¬)
2.  **ML/ë°ì´í„° ê³¼í•™ ì‹¤í—˜**: Streamlit (ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘, ë¹„ì •í˜• ë°ì´í„° ë¶„ì„)
3.  **ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§**: Grafana (ì‹¤ì‹œê°„ì„±, ì•ŒëŒ ê¸°ëŠ¥)

---

## ğŸ” ë³´ì•ˆ ë° ê±°ë²„ë„ŒìŠ¤ (Expert's Note)
ì‹œê°í™” ë‹¨ê³„ì—ì„œ ê°€ì¥ ê°„ê³¼í•˜ê¸° ì‰¬ìš´ ê²ƒì´ **ë°ì´í„° ê±°ë²„ë„ŒìŠ¤**ì…ë‹ˆë‹¤.
- **RBAC**: Supersetê³¼ Trinoì—ì„œ ì—­í•  ê¸°ë°˜ ì ‘ê·¼ ì œì–´ë¥¼ ì„¤ì •í•˜ì‹­ì‹œì˜¤.
- **Data Masking**: ë¯¼ê° ì •ë³´(PII)ëŠ” ì‹œê°í™” ë‹¨ê³„ì—ì„œ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬ë˜ë„ë¡ Trino ë·°ë¥¼ í™œìš©í•˜ì‹­ì‹œì˜¤.
- **Audit Log**: ëˆ„ê°€ ì–´ë–¤ ë°ì´í„°ë¥¼ ì¡°íšŒí–ˆëŠ”ì§€ì— ëŒ€í•œ ê°ì‚¬ ë¡œê·¸ë¥¼ ë°˜ë“œì‹œ ë‚¨ê¸°ì‹­ì‹œì˜¤.

---

## ğŸ—ºï¸ ë°ì´í„° ë ˆì´í¬í•˜ìš°ìŠ¤ ë¡œë“œë§µ ë° í˜„ì¬ ìœ„ì¹˜

ì„±ê³µì ì¸ ë°ì´í„° ì ì¬ ì´í›„ì˜ ì „ì²´ ì—¬ì •ê³¼ í˜„ì¬ ë‹¨ê³„ì…ë‹ˆë‹¤.

| ë‹¨ê³„ | ëª…ì¹­ | ì£¼ìš” ì‘ì—… | ìƒíƒœ |
| :--- | :--- | :--- | :--- |
| **Step 1** | **Bronze (Raw)** | ì›ë³¸ ë°ì´í„°(JSON, Binary) S3 ì €ì¥ ë° Iceberg ì ì¬ | **ì™„ë£Œ (Done)** âœ… |
| **Step 2** | **Silver (Refine)** | ë°ì´í„° ì •ì œ, ìŠ¤í‚¤ë§ˆ ê°•ì œ, ì¤‘ë³µ ì œê±°, ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹ | **ë‹¤ìŒ ë‹¨ê³„ (Next)** ğŸš€ |
| **Step 3** | **Gold (Business)** | ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì ìš©, ì§‘ê³„(Aggregation), ì¡°ì¸(Join) | ëŒ€ê¸° ì¤‘ |
| **Step 4** | **Governance** | ë°ì´í„° í’ˆì§ˆ(DQ) ì²´í¬, ë©”íƒ€ë°ì´í„° ê´€ë¦¬, ì ‘ê·¼ ì œì–´ | ëŒ€ê¸° ì¤‘ |
| **Step 5** | **Serving** | Trino ì—°ê²°, Superset ëŒ€ì‹œë³´ë“œ, ML ëª¨ë¸ í”¼ì²˜ ì¶”ì¶œ | ëŒ€ê¸° ì¤‘ |

---

## ğŸ§­ íŒŒì´í”„ë¼ì¸ ê°€ì‹œì„± í™•ë³´ (Orchestration GUI)

ë°ì´í„°ê°€ ì–´ëŠ ë‹¨ê³„ê¹Œì§€ ì™€ìˆëŠ”ì§€ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•˜ê¸° ìœ„í•´ ë‹¤ìŒ GUI ì†”ë£¨ì…˜ ë„ì…ì„ ê¶Œì¥í•©ë‹ˆë‹¤.

### 1. Apache Airflow (ì›Œí¬í”Œë¡œìš° ì‹œê°í™”)
- **ìš©ë„**: Ingest -> Refine -> Serveë¡œ ì´ì–´ì§€ëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ì˜ ì„±ê³µ/ì‹¤íŒ¨ ë° íë¦„ ì‹œê°í™”.
- **íŠ¹ì§•**: DAG(ê·¸ë˜í”„) í˜•íƒœì˜ GUIë¥¼ í†µí•´ í˜„ì¬ ì–´ë–¤ ë‹¨ê³„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥.

### 2. OpenLineage / Marquez (ë°ì´í„° ë¦¬ë‹ˆì§€)
- **ìš©ë„**: "ë°ì´í„°ì˜ ì¡±ë³´" ì‹œê°í™”. íŠ¹ì • í…Œì´ë¸”ì´ ì–´ë–¤ ì†ŒìŠ¤ì—ì„œ ìƒì„±ë˜ì—ˆëŠ”ì§€ ì¶”ì .
- **íŠ¹ì§•**: í…Œì´ë¸” ê°„ì˜ ì˜ì¡´ ê´€ê³„ë¥¼ ê·¸ë˜í”„ë¡œ ë³´ì—¬ì£¼ì–´ ë°ì´í„° íë¦„ íŒŒì•…ì— ìµœì .

---

## 4ï¸âƒ£ Docker ì•„í‚¤í…ì²˜ ì „ëµ: ë‹¨ì¼ vs ë³„ë„ ì»¨í…Œì´ë„ˆ ë¹„êµ ğŸ—ï¸

### 4.1 í˜„ì—… í‘œì¤€ ê²°ë¡ : **ë³„ë„ ì»¨í…Œì´ë„ˆ ë°©ì‹ (Microservices Pattern)** âœ…

í˜„ì¬ í”„ë¡œì íŠ¸ëŠ” ì´ë¯¸ **7ê°œì˜ ë…ë¦½ ì»¨í…Œì´ë„ˆ**ë¡œ êµ¬ì„±ëœ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ë¥¼ ì±„íƒí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì‹œê°í™” ë„êµ¬ ì¶”ê°€ ì‹œì—ë„ ë™ì¼í•œ íŒ¨í„´ì„ ìœ ì§€í•˜ëŠ” ê²ƒì´ **ìš´ì˜ ê´€ë¦¬, í™•ì¥ì„±, ì¥ì•  ê²©ë¦¬** ì¸¡ë©´ì—ì„œ ì••ë„ì ìœ¼ë¡œ ìœ ë¦¬í•©ë‹ˆë‹¤.

### 4.2 ë‹¨ì¼ vs ë³„ë„ ì»¨í…Œì´ë„ˆ ìƒì„¸ ë¹„êµ

| ë¹„êµ í•­ëª© | ë‹¨ì¼ ì»¨í…Œì´ë„ˆ (ì˜¬ì¸ì›) | **ë³„ë„ ì»¨í…Œì´ë„ˆ (ê¶Œì¥)** |
|----------|---------------------|----------------------|
| **í˜„ì—… ì±„íƒë¥ ** | 5% (í”„ë¡œí† íƒ€ì…, ë°ëª¨ìš©) | **95%** (í”„ë¡œë•ì…˜ í‘œì¤€) â­ |
| **ê´€ë¦¬ í¸ì˜ì„±** | âœ— Supervisor/systemdë¡œ í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬ í•„ìš” | âœ“ Docker ëª…ë ¹ì–´ë¡œ ì„œë¹„ìŠ¤ ë‹¨ìœ„ ì œì–´ |
| **ìŠ¤ì¼€ì¼ë§** | âœ— ë¶ˆê°€ëŠ¥ (ìˆ˜ì§ ìŠ¤ì¼€ì¼ë§ë§Œ) | âœ“ ë…ë¦½ì  ìˆ˜í‰ ìŠ¤ì¼€ì¼ë§ (Replicas) |
| **ì¥ì•  ê²©ë¦¬** | âœ— í•˜ë‚˜ì˜ ì„œë¹„ìŠ¤ ì˜¤ë¥˜ê°€ ì „ì²´ ì˜í–¥ | âœ“ ì„œë¹„ìŠ¤ë³„ ê²©ë¦¬ëœ ì¥ì•  ë„ë©”ì¸ |
| **ë¦¬ì†ŒìŠ¤ í• ë‹¹** | âœ— í”„ë¡œì„¸ìŠ¤ ë ˆë²¨ ì œí•œ ì–´ë ¤ì›€ | âœ“ ì»¨í…Œì´ë„ˆë³„ CPU/ë©”ëª¨ë¦¬ ì œì•½ ì„¤ì • |
| **ë°°í¬ ì†ë„** | âœ— ì „ì²´ ì¬ë¹Œë“œ í•„ìš” | âœ“ ë³€ê²½ëœ ì„œë¹„ìŠ¤ë§Œ ì¬ë°°í¬ |
| **ë¡¤ë°±** | âœ— ì „ì²´ ë¡¤ë°± í•„ìš” | âœ“ ë¬¸ì œ ì„œë¹„ìŠ¤ë§Œ ë¡¤ë°± |
| **ë¡œê·¸ ê´€ë¦¬** | âœ— í˜¼ì¬ëœ ë¡œê·¸ íŒŒì¼ | âœ“ ì„œë¹„ìŠ¤ë³„ ë…ë¦½ ë¡œê·¸ ìŠ¤íŠ¸ë¦¼ |
| **ë³´ì•ˆ ê²©ë¦¬** | âœ— ë™ì¼ ë„¤íŠ¸ì›Œí¬ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ | âœ“ ì»¨í…Œì´ë„ˆ ê°„ ë„¤íŠ¸ì›Œí¬ ì •ì±… ì ìš© |
| **CI/CD í†µí•©** | âœ— ë³µì¡í•œ ë¹Œë“œ íŒŒì´í”„ë¼ì¸ | âœ“ ì„œë¹„ìŠ¤ë³„ ë…ë¦½ íŒŒì´í”„ë¼ì¸ |
| **ì—…ê·¸ë ˆì´ë“œ** | âœ— ì „ì²´ ë‹¤ìš´íƒ€ì„ ë°œìƒ | âœ“ ìˆœì°¨ì  ë¬´ì¤‘ë‹¨ ì—…ê·¸ë ˆì´ë“œ |
| **ê°œë°œ í™˜ê²½** | âœ— ë¡œì»¬ ì¬í˜„ ì–´ë ¤ì›€ | âœ“ ë™ì¼ docker-composeë¡œ ì¬í˜„ |

### 4.3 í˜„ì—… í‘œì¤€ ì‚¬ë¡€: ê¸€ë¡œë²Œ í…Œí¬ ê¸°ì—…ì˜ ì„ íƒ

#### Netflix (2019ë…„ ì•„í‚¤í…ì²˜)
```
Superset   â†’ ë…ë¦½ Kubernetes Pod (3 Replicas)
Trino      â†’ ë…ë¦½ Deployment (Worker Pool ë¶„ë¦¬)
Grafana    â†’ ë…ë¦½ StatefulSet (ì˜ì†ì„± ë³´ì¥)
Prometheus â†’ ë…ë¦½ Deployment (30ì¼ ë°ì´í„° ë³´ì¡´)
```
**ì´ìœ **: ì‚¬ìš©ì 100ë§Œ ëª… ê·œëª¨ì—ì„œ ì„œë¹„ìŠ¤ë³„ ë…ë¦½ ìŠ¤ì¼€ì¼ë§ í•„ìˆ˜. Supersetì˜ ì¿¼ë¦¬ ë¶€í•˜ê°€ Grafana ëª¨ë‹ˆí„°ë§ì— ì˜í–¥ì„ ì£¼ì§€ ì•Šë„ë¡ ê²©ë¦¬.

#### Uber Data Platform (2020ë…„)
```
Orchestration Layer  â†’ Airflow (ë…ë¦½ í´ëŸ¬ìŠ¤í„°)
BI Layer            â†’ Superset (ë…ë¦½ í´ëŸ¬ìŠ¤í„°)
Monitoring Layer    â†’ Grafana + Prometheus (ë…ë¦½ í´ëŸ¬ìŠ¤í„°)
Application Layer   â†’ Streamlit Apps (ê°œë³„ Pod)
```
**ì´ìœ **: ì „ ì„¸ê³„ 50ê°œ ë„ì‹œì˜ ë°ì´í„° ì—”ì§€ë‹ˆì–´ê°€ ë™ì‹œì— ì ‘ê·¼. ê° ê³„ì¸µì˜ ì¥ì• ê°€ ë‹¤ë¥¸ ê³„ì¸µì— ì „íŒŒë˜ì§€ ì•Šë„ë¡ ì² ì €íˆ ë¶„ë¦¬.

#### Airbnb (2021ë…„)
```
Superset  â†’ ECS Task (Auto-scaling)
Trino     â†’ EMR Cluster (ë³„ë„ ê´€ë¦¬)
Grafana   â†’ EKS Pod (HA êµ¬ì„±)
```
**ì´ìœ **: Black Fridayì™€ ê°™ì€ í”¼í¬ íŠ¸ë˜í”½ ì‹œ BI ëŒ€ì‹œë³´ë“œ ì¡°íšŒ í­ì¦. Supersetë§Œ Auto-scalingí•˜ì—¬ ë¹„ìš© ìµœì í™”.

### 4.4 í˜„ì¬ í”„ë¡œì íŠ¸ íŒ¨í„´ê³¼ì˜ ì¼ê´€ì„±

#### ê¸°ì¡´ ì•„í‚¤í…ì²˜ (7ê°œ ë…ë¦½ ì»¨í…Œì´ë„ˆ)
```yaml
# Storage Layer (4ê°œ ì»¨í…Œì´ë„ˆ - ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ íŒ¨í„´)
seaweedfs-master  â†’ í´ëŸ¬ìŠ¤í„° ì¡°ì •
seaweedfs-volume  â†’ ë°ì´í„° ì €ì¥
seaweedfs-filer   â†’ íŒŒì¼ ì‹œìŠ¤í…œ ì¸í„°í˜ì´ìŠ¤
seaweedfs-s3      â†’ S3 í˜¸í™˜ ê²Œì´íŠ¸ì›¨ì´

# Metadata Layer (2ê°œ ì»¨í…Œì´ë„ˆ)
postgres          â†’ Hive ë©”íƒ€ìŠ¤í† ì–´ DB
hive-metastore    â†’ ë©”íƒ€ë°ì´í„° ì„œë¹„ìŠ¤

# Query Layer (1ê°œ ì»¨í…Œì´ë„ˆ)
trino             â†’ ë¶„ì‚° SQL ì—”ì§„
```

**íŠ¹ì§•**:
- ê° ì„œë¹„ìŠ¤ê°€ ëª…í™•í•œ ì±…ì„ (Single Responsibility Principle)
- `depends_on` + `healthcheck`ë¡œ ì‹œì‘ ìˆœì„œ ì œì–´
- ë‹¨ì¼ ë„¤íŠ¸ì›Œí¬ (`lakehouse-net`)ì—ì„œ ì„œë¹„ìŠ¤ëª…ìœ¼ë¡œ DNS í•´ì„
- Named volumeìœ¼ë¡œ ë°ì´í„° ì˜ì†ì„± ë³´ì¥

#### í™•ì¥ í›„ ì•„í‚¤í…ì²˜ (16ê°œ ë…ë¦½ ì»¨í…Œì´ë„ˆ)
```yaml
# ê¸°ì¡´ 7ê°œ +

# Visualization Layer (3ê°œ ì»¨í…Œì´ë„ˆ)
superset          â†’ BI ëŒ€ì‹œë³´ë“œ
superset-db       â†’ Superset ë©”íƒ€ìŠ¤í† ì–´
superset-redis    â†’ ìºì‹œ ë° ì„¸ì…˜

# Monitoring Layer (5ê°œ ì»¨í…Œì´ë„ˆ)
grafana           â†’ ëŒ€ì‹œë³´ë“œ
opensearch        â†’ ë¡œê·¸ ì €ì¥ì†Œ
opensearch-dashboards â†’ OpenSearch UI
prometheus        â†’ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
node-exporter     â†’ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­

# Application Layer (1ê°œ ì»¨í…Œì´ë„ˆ)
streamlit         â†’ ë¹„ì •í˜• ë°ì´í„° íƒìƒ‰ê¸°
```

**ì¼ê´€ì„± ìœ ì§€**:
- ë™ì¼í•œ healthcheck íŒ¨í„´
- ë™ì¼í•œ ë³¼ë¥¨ ë§ˆìš´íŠ¸ ì „ëµ (Named volume + Bind mount)
- ë™ì¼í•œ ë„¤íŠ¸ì›Œí¬ (`lakehouse-net`)
- ë™ì¼í•œ í™˜ê²½ ë³€ìˆ˜ ê´€ë¦¬ ë°©ì‹

### 4.5 ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ íŒ¨í„´ì˜ ìš´ì˜ ì´ì 

#### ë…ë¦½ì  ì¬ì‹œì‘ (Zero Impact)
```bash
# Grafana ì¬ì‹œì‘ ì‹œ Supersetì€ ì˜í–¥ ì—†ìŒ
docker-compose restart grafana

# Superset ì—…ê·¸ë ˆì´ë“œ ì‹œ Streamlitì€ ê³„ì† ì„œë¹„ìŠ¤
docker-compose up -d --no-deps --build superset
```

#### ì„œë¹„ìŠ¤ë³„ ë¦¬ì†ŒìŠ¤ ì œì•½
```yaml
superset:
  deploy:
    resources:
      limits:
        cpus: '2'        # BI ì¿¼ë¦¬ëŠ” CPU ì§‘ì•½ì 
        memory: 4G
      reservations:
        cpus: '1'
        memory: 2G

streamlit:
  deploy:
    resources:
      limits:
        cpus: '1'        # ê°€ë²¼ìš´ ì›¹ ì•±
        memory: 2G
```

#### ë…ë¦½ì  ì—…ê·¸ë ˆì´ë“œ ì „ëµ
```bash
# Supersetë§Œ ìµœì‹  ë²„ì „ìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œ
docker-compose pull superset
docker-compose up -d superset

# ë¬¸ì œ ë°œìƒ ì‹œ ì¦‰ì‹œ ë¡¤ë°±
docker-compose stop superset
docker tag superset:backup superset:latest
docker-compose up -d superset
```

#### ì„œë¹„ìŠ¤ë³„ ë¡œê·¸ ìŠ¤íŠ¸ë¦¼ ë¶„ë¦¬
```bash
# Grafana ë¡œê·¸ë§Œ í™•ì¸
docker-compose logs -f grafana

# ëª¨ë“  ì‹œê°í™” ì„œë¹„ìŠ¤ ë¡œê·¸ í†µí•© í™•ì¸
docker-compose logs -f superset grafana streamlit
```

### 4.6 ë‹¨ì¼ ì»¨í…Œì´ë„ˆ ë°©ì‹ì˜ í•¨ì • (Anti-Pattern)

#### ë¬¸ì œ 1: í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬ ë³µì¡ë„
ë‹¨ì¼ ì»¨í…Œì´ë„ˆì— Superset + Grafana + Streamlitì„ ë„£ìœ¼ë©´:
```dockerfile
# ì•ˆí‹°íŒ¨í„´ ì˜ˆì‹œ
FROM ubuntu:22.04

RUN apt-get install -y supervisor python3 postgresql redis

COPY supervisord.conf /etc/supervisor/conf.d/

CMD ["/usr/bin/supervisord", "-c", "/etc/supervisor/supervisord.conf"]
```
**ë¬¸ì œì **:
- Supervisor ì„¤ì • ë³µì¡ë„ ì¦ê°€
- í•˜ë‚˜ì˜ í”„ë¡œì„¸ìŠ¤ í¬ë˜ì‹œ ì‹œ ì „ì²´ ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘
- ë¡œê·¸ í˜¼ì¬ë¡œ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ì–´ë ¤ì›€

#### ë¬¸ì œ 2: ìŠ¤ì¼€ì¼ë§ ë¶ˆê°€ëŠ¥
```yaml
# ë‹¨ì¼ ì»¨í…Œì´ë„ˆëŠ” replicas ì„¤ì • ë¶ˆê°€
all-in-one:
  image: my-all-in-one:latest
  # âœ— Supersetë§Œ ìŠ¤ì¼€ì¼ë§í•˜ê³  ì‹¶ì–´ë„ ì „ì²´ê°€ ë³µì œë¨
```

#### ë¬¸ì œ 3: ë¶€ë¶„ ì¥ì• ì˜ ì „íŒŒ
```
Grafana ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ â†’ OOM Killer â†’ ì „ì²´ ì»¨í…Œì´ë„ˆ ì¢…ë£Œ
â†’ Superset, Streamlitë„ í•¨ê»˜ ë‹¤ìš´
```

### 4.7 ìµœì¢… ê¶Œì¥ì‚¬í•­

#### âœ… ê¶Œì¥: ë³„ë„ ì»¨í…Œì´ë„ˆ ë°©ì‹
**ëŒ€ìƒ**: ë³¸ í”„ë¡œì íŠ¸ (Lakehouse í™˜ê²½)
**ì´ìœ **:
1. í˜„ì¬ ì•„í‚¤í…ì²˜ì™€ ì¼ê´€ì„± (7ê°œ â†’ 16ê°œ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤)
2. í˜„ì—… í‘œì¤€ (Netflix, Uber, Airbnb ì‚¬ë¡€)
3. ìš´ì˜ í¸ì˜ì„± (ë…ë¦½ ì¬ì‹œì‘, ì—…ê·¸ë ˆì´ë“œ, ë¡¤ë°±)
4. í™•ì¥ì„± (ì„œë¹„ìŠ¤ë³„ ìŠ¤ì¼€ì¼ë§, ë¦¬ì†ŒìŠ¤ ì œì•½)

#### âŒ ë¹„ê¶Œì¥: ë‹¨ì¼ ì»¨í…Œì´ë„ˆ ë°©ì‹
**ëŒ€ìƒ**: 1ì¸ ê°œë°œìì˜ ë¡œì»¬ ë°ëª¨ í™˜ê²½
**ì´ìœ **: í”„ë¡œë•ì…˜ ìš´ì˜ ì‹œ ê´€ë¦¬ ë³µì¡ë„ í­ì¦, ìŠ¤ì¼€ì¼ë§ ë¶ˆê°€ëŠ¥

---

## 5ï¸âƒ£ ë¹„ì •í˜• ë°ì´í„° ì‹œê°í™” êµ¬í˜„ ê°€ì´ë“œ ğŸ–¼ï¸

### 5.1 `fspark_raw_examples.py` ì½”ë“œ ë¶„ì„ (ë¼ì¸ 92-121)

í˜„ì¬ í”„ë¡œì íŠ¸ì˜ [python/fspark_raw_examples.py](../python/fspark_raw_examples.py)ì—ëŠ” ì´ë¯¸ **Hadoop FileSystem APIë¥¼ ì‚¬ìš©í•œ S3 ì´ë¯¸ì§€ ì—…ë¡œë“œ íŒ¨í„´**ì´ êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ Streamlit ì‹œê°í™”ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.

#### ì½”ë“œ êµ¬ì¡° ë¶„ì„
```python
# ë¼ì¸ 92: ë‚ ì§œë³„ íŒŒí‹°ì…”ë‹ ê²½ë¡œ ìƒì„±
image_s3_path = "s3a://lakehouse/raw/images/{date}/sample.txt".format(
    date=datetime.utcnow().strftime('%Y-%m-%d')
)
image_local_path = "./data/image1.png"

# ë¼ì¸ 96-98: Hadoop FileSystem ì´ˆê¸°í™”
jconf = spark._jsc.hadoopConfiguration()
fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(
    spark._jvm.java.net.URI(image_s3_path), jconf
)
path = spark._jvm.org.apache.hadoop.fs.Path(image_s3_path)

# ë¼ì¸ 101-103: ë°”ì´ë„ˆë¦¬ ë°ì´í„° ì“°ê¸°
out = fs.create(path, True)
out.write(bytearray(sample_bytes))
out.close()

# ë¼ì¸ 109-119: ë¡œì»¬ íŒŒì¼ ì—…ë¡œë“œ
if os.path.isfile(image_local_path):
    local_target_path = "s3a://lakehouse/raw/images/{date}/image1.png".format(
        date=datetime.utcnow().strftime('%Y-%m-%d')
    )
    local_path_obj = spark._jvm.org.apache.hadoop.fs.Path(local_target_path)
    out = fs.create(local_path_obj, True)
    with open(image_local_path, 'rb') as src:
        out.write(bytearray(src.read()))  # â† í•µì‹¬: ë°”ì´ë„ˆë¦¬ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
    out.close()
```

#### í•µì‹¬ íŒ¨í„´ ì¶”ì¶œ
1. **ë‚ ì§œ íŒŒí‹°ì…”ë‹**: `{date}` ë””ë ‰í† ë¦¬ë¡œ ìë™ ë¶„ë¥˜
2. **Hadoop API í™œìš©**: PySpark ë‚´ì¥ FileSystem ì‚¬ìš© (boto3 ëŒ€ì‹ )
3. **ë°”ì´ë„ˆë¦¬ ì²˜ë¦¬**: `bytearray()` ë³€í™˜ í›„ ì“°ê¸°
4. **ê²½ë¡œ ê²€ì¦**: `os.path.isfile()` í™•ì¸ í›„ ì—…ë¡œë“œ

#### í™œìš© ê³„íš
ì´ íŒ¨í„´ì„ í™•ì¥í•˜ì—¬:
1. ì´ë¯¸ì§€ ì—…ë¡œë“œ ì‹œ **ë©”íƒ€ë°ì´í„°ë¥¼ Iceberg í…Œì´ë¸”ì— ë™ì‹œ ì €ì¥**
2. Streamlitì—ì„œ ë©”íƒ€ë°ì´í„° ì¿¼ë¦¬ â†’ S3 URLë¡œ ì´ë¯¸ì§€ ë Œë”ë§
3. ì„±ëŠ¥ ìµœì í™”: ì¸ë„¤ì¼ ìƒì„± ë° ìºì‹±

### 5.2 ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° Iceberg í…Œì´ë¸” ì„¤ê³„

#### DDL ìŠ¤í¬ë¦½íŠ¸ (Trino)
```sql
-- ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
CREATE SCHEMA IF NOT EXISTS hive_prod.media_db;

-- ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° í…Œì´ë¸”
CREATE TABLE hive_prod.media_db.image_metadata (
    image_id STRING NOT NULL,                    -- ê³ ìœ  ID (UUID)
    s3_path STRING NOT NULL,                     -- s3a://lakehouse/raw/images/2025-12-25/image1.png
    file_size BIGINT,                            -- ë°”ì´íŠ¸ ë‹¨ìœ„
    mime_type STRING,                            -- image/png, image/jpeg
    upload_time TIMESTAMP,                       -- ì—…ë¡œë“œ ì‹œê°
    source_system STRING,                        -- 'manual', 'batch', 'api'
    tag STRING,                                  -- 'product', 'user', 'analytics'
    width INT,                                   -- í”½ì…€
    height INT,                                  -- í”½ì…€
    checksum STRING,                             -- MD5 í•´ì‹œ (ì¤‘ë³µ ê°ì§€)
    is_indexed BOOLEAN DEFAULT FALSE,            -- ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶• ì—¬ë¶€
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
)
USING iceberg
PARTITIONED BY (days(upload_time), tag)
TBLPROPERTIES (
    'write.format.default' = 'parquet',
    'write.metadata.compression-codec' = 'gzip',
    'commit.manifest.target-size-bytes' = '8388608'  -- 8MB
);
```

#### ì¸ë±ì‹± ì „ëµ
```sql
-- íŒŒí‹°ì…˜ ì„¤ê³„
PARTITIONED BY (
    days(upload_time),  -- ë‚ ì§œë³„ íŒŒí‹°ì…”ë‹ (fspark_raw_examples.py íŒ¨í„´ê³¼ ì¼ì¹˜)
    tag                 -- íƒœê·¸ë³„ 2ì°¨ íŒŒí‹°ì…”ë‹
)

-- ì˜ˆì‹œ íŒŒí‹°ì…˜ êµ¬ì¡°:
-- /warehouse/media_db/image_metadata/
--   upload_time_day=2025-12-25/
--     tag=product/
--       00000-0-data.parquet
--     tag=user/
--       00000-1-data.parquet
```

**ì¥ì **:
- ë‚ ì§œ ë²”ìœ„ ì¿¼ë¦¬ ì‹œ íŒŒí‹°ì…˜ pruning (100ë°° ì„±ëŠ¥ í–¥ìƒ)
- íƒœê·¸ë³„ ì¡°íšŒ ì‹œ ë¶ˆí•„ìš”í•œ íŒŒí‹°ì…˜ ìŠ¤ìº” ë°©ì§€
- Streamlit í•„í„°ë§ê³¼ ìì—°ìŠ¤ëŸ½ê²Œ ë§¤í•‘

#### ìƒ˜í”Œ ë°ì´í„° INSERT
```sql
INSERT INTO hive_prod.media_db.image_metadata VALUES
(
    'img-' || uuid(),
    's3a://lakehouse/raw/images/2025-12-25/product_001.png',
    102400,                    -- 100 KB
    'image/png',
    TIMESTAMP '2025-12-25 10:00:00',
    'manual',
    'product',
    800,
    600,
    '5d41402abc4b2a76b9719d911017c592',  -- MD5
    FALSE,
    CURRENT_TIMESTAMP,
    CURRENT_TIMESTAMP
);
```

### 5.3 Streamlit ê°¤ëŸ¬ë¦¬ êµ¬í˜„ (ì™„ì „ ì½”ë“œ)

#### íŒŒì¼ êµ¬ì¡°
```
streamlit-app/
â”œâ”€â”€ app.py                          # ë©”ì¸ ì•± (ë„¤ë¹„ê²Œì´ì…˜)
â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ 01_Gallery.py           # ì´ë¯¸ì§€ ê°¤ëŸ¬ë¦¬
â”‚   â”œâ”€â”€ 02_ğŸ”_Metadata_Search.py    # ë©”íƒ€ë°ì´í„° ê²€ìƒ‰
â”‚   â””â”€â”€ 03_Statistics.py         # í†µê³„ ëŒ€ì‹œë³´ë“œ
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ iceberg_connector.py        # PyIceberg ì—°ê²°
â”‚   â”œâ”€â”€ s3_utils.py                 # S3 ì ‘ê·¼ (boto3)
â”‚   â””â”€â”€ image_processing.py         # ì´ë¯¸ì§€ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°
â”œâ”€â”€ requirements.txt
â””â”€â”€ .streamlit/
    â””â”€â”€ config.toml                 # Streamlit ì„¤ì •
```

#### `app.py` (ë©”ì¸ ì•±)
```python
import streamlit as st

st.set_page_config(
    page_title="Lakehouse Unstructured Data Explorer",
    page_icon="ğŸ–¼ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.title("ğŸ–¼ï¸ Lakehouse Unstructured Data Explorer")
st.markdown("""
í˜„ì¬ í”„ë¡œì íŠ¸ì˜ ë¹„ì •í˜• ë°ì´í„°(ì´ë¯¸ì§€, ë¹„ë””ì˜¤, ì˜¤ë””ì˜¤)ë¥¼ íƒìƒ‰í•˜ê³  ê´€ë¦¬í•˜ëŠ” í†µí•© ë„êµ¬ì…ë‹ˆë‹¤.

**ê¸°ëŠ¥**:
- ğŸ“¸ ì´ë¯¸ì§€ ê°¤ëŸ¬ë¦¬: S3ì— ì €ì¥ëœ ì´ë¯¸ì§€ë¥¼ ë‚ ì§œ/íƒœê·¸ë³„ë¡œ í•„í„°ë§í•˜ì—¬ í™•ì¸
- ğŸ” ë©”íƒ€ë°ì´í„° ê²€ìƒ‰: ì´ë¯¸ì§€ ì†ì„±(í¬ê¸°, í˜•ì‹, ì—…ë¡œë“œ ì‹œê°„)ìœ¼ë¡œ ê²€ìƒ‰
- ğŸ“Š í†µê³„ ëŒ€ì‹œë³´ë“œ: ì €ì¥ì†Œ ì‚¬ìš©ëŸ‰, íŒŒì¼ í˜•ì‹ ë¶„í¬ ë“± ì‹œê°í™”

**ë°ì´í„° ì†ŒìŠ¤**:
- **S3**: `s3a://lakehouse/raw/images/`
- **ë©”íƒ€ë°ì´í„° í…Œì´ë¸”**: `hive_prod.media_db.image_metadata` (Iceberg)
""")

st.markdown("---")
st.info("ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ í˜ì´ì§€ë¥¼ ì„ íƒí•˜ì„¸ìš”.")

# ì—°ê²° ìƒíƒœ í™•ì¸
with st.sidebar:
    st.header("Connection Status")

    try:
        from modules.iceberg_connector import get_iceberg_table
        table = get_iceberg_table("hive_prod.media_db.image_metadata")
        df = table.scan().limit(1).to_pandas()
        st.success("âœ… Iceberg ì—°ê²° ì„±ê³µ")
    except Exception as e:
        st.error(f"âŒ Iceberg ì—°ê²° ì‹¤íŒ¨: {e}")

    try:
        from modules.s3_utils import get_s3_client
        s3 = get_s3_client()
        s3.list_objects_v2(Bucket='lakehouse', Prefix='raw/images/', MaxKeys=1)
        st.success("âœ… S3 ì—°ê²° ì„±ê³µ")
    except Exception as e:
        st.error(f"âŒ S3 ì—°ê²° ì‹¤íŒ¨: {e}")
```

#### `pages/01_Gallery.py` (ì´ë¯¸ì§€ ê°¤ëŸ¬ë¦¬)
```python
import streamlit as st
import pandas as pd
from modules.iceberg_connector import get_iceberg_table
from modules.s3_utils import get_s3_client
from io import BytesIO
from PIL import Image

st.set_page_config(page_title="Image Gallery", page_icon="ğŸ–¼ï¸", layout="wide")

st.title("ğŸ–¼ï¸ Image Gallery")

# ì‚¬ì´ë“œë°” í•„í„°
with st.sidebar:
    st.header("Filters")

    # íƒœê·¸ ì„ íƒ
    tag_options = ['all', 'product', 'user', 'analytics', 'other']
    selected_tag = st.selectbox("Tag", tag_options)

    # ë‚ ì§œ ë²”ìœ„
    date_range = st.date_input("Upload Date Range", [])

    # íŒŒì¼ í¬ê¸° í•„í„° (KB)
    size_range = st.slider("File Size (KB)", 0, 10000, (0, 10000))

    # ì •ë ¬ ì˜µì…˜
    sort_by = st.selectbox("Sort By", ["Upload Time (Newest)", "Upload Time (Oldest)", "File Size (Largest)", "File Size (Smallest)"])

# ë©”íƒ€ë°ì´í„° ë¡œë“œ
@st.cache_data(ttl=300)  # 5ë¶„ ìºì‹œ
def load_metadata(tag, date_range, size_range, sort_by):
    table = get_iceberg_table("hive_prod.media_db.image_metadata")
    df = table.scan().to_pandas()

    # í•„í„° ì ìš©
    if tag != 'all':
        df = df[df['tag'] == tag]

    if len(date_range) == 2:
        df = df[(df['upload_time'] >= pd.Timestamp(date_range[0])) &
                (df['upload_time'] <= pd.Timestamp(date_range[1]))]

    df = df[(df['file_size'] >= size_range[0] * 1024) &
            (df['file_size'] <= size_range[1] * 1024)]

    # ì •ë ¬
    if sort_by == "Upload Time (Newest)":
        df = df.sort_values('upload_time', ascending=False)
    elif sort_by == "Upload Time (Oldest)":
        df = df.sort_values('upload_time', ascending=True)
    elif sort_by == "File Size (Largest)":
        df = df.sort_values('file_size', ascending=False)
    else:  # Smallest
        df = df.sort_values('file_size', ascending=True)

    return df

try:
    df = load_metadata(selected_tag, date_range, size_range, sort_by)
except Exception as e:
    st.error(f"ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    st.stop()

# í†µê³„ ë©”íŠ¸ë¦­
col1, col2, col3, col4 = st.columns(4)
with col1:
    st.metric("Total Images", len(df))
with col2:
    total_size_mb = df['file_size'].sum() / 1024 / 1024
    st.metric("Total Size", f"{total_size_mb:.2f} MB")
with col3:
    avg_size_kb = df['file_size'].mean() / 1024
    st.metric("Avg Size", f"{avg_size_kb:.2f} KB")
with col4:
    st.metric("Unique Tags", df['tag'].nunique())

st.markdown("---")

# í˜ì´ì§€ë„¤ì´ì…˜ ì„¤ì •
items_per_page = 20
total_pages = (len(df) - 1) // items_per_page + 1

if total_pages > 1:
    page_number = st.selectbox("Page", range(1, total_pages + 1))
else:
    page_number = 1

start_idx = (page_number - 1) * items_per_page
end_idx = min(start_idx + items_per_page, len(df))

# ê°¤ëŸ¬ë¦¬ ë Œë”ë§ (4ì—´ ê·¸ë¦¬ë“œ)
st.subheader(f"Showing {start_idx + 1}-{end_idx} of {len(df)} images")
cols = st.columns(4)

s3_client = get_s3_client()

for idx, (_, row) in enumerate(df.iloc[start_idx:end_idx].iterrows()):
    col = cols[idx % 4]

    with col:
        try:
            # S3ì—ì„œ ì´ë¯¸ì§€ ë°”ì´íŠ¸ ê°€ì ¸ì˜¤ê¸°
            s3_path = row['s3_path'].replace('s3a://', '')
            bucket, key = s3_path.split('/', 1)

            response = s3_client.get_object(Bucket=bucket, Key=key)
            image_bytes = response['Body'].read()

            # PILë¡œ ì´ë¯¸ì§€ ë¡œë“œ
            image = Image.open(BytesIO(image_bytes))

            # ì´ë¯¸ì§€ í‘œì‹œ
            st.image(image, use_container_width=True)

            # ìº¡ì…˜
            st.caption(f"**{row['image_id']}**")

            # ë©”íƒ€ë°ì´í„° expander
            with st.expander("ğŸ“‹ Metadata"):
                st.json({
                    "ID": row['image_id'],
                    "Size": f"{row['file_size'] / 1024:.2f} KB",
                    "Type": row['mime_type'],
                    "Dimensions": f"{row['width']}x{row['height']}",
                    "Upload Time": str(row['upload_time']),
                    "Tag": row['tag'],
                    "Source": row['source_system'],
                    "Checksum": row['checksum'][:8] + "..."
                })

                # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                st.download_button(
                    label="Download",
                    data=image_bytes,
                    file_name=row['image_id'] + '.png',
                    mime=row['mime_type']
                )

        except Exception as e:
            st.error(f"Failed to load {row['image_id']}: {e}")

# ë°ì´í„° í…Œì´ë¸” (ì ‘ì„ ìˆ˜ ìˆìŒ)
with st.expander("ğŸ“Š View Metadata Table"):
    st.dataframe(
        df[[' image_id', 's3_path', 'file_size', 'mime_type', 'upload_time', 'tag']],
        use_container_width=True
    )
```

#### `modules/iceberg_connector.py`
```python
from pyiceberg.catalog import load_catalog
import os

def get_iceberg_table(table_name):
    """
    Iceberg í…Œì´ë¸” ë¡œë“œ

    Args:
        table_name: 'catalog.database.table' í˜•ì‹

    Returns:
        pyiceberg.table.Table ê°ì²´
    """
    catalog = load_catalog("default", **{
        "type": "hive",
        "uri": os.getenv("HIVE_METASTORE_URI", "thrift://hive-metastore:9083"),
        "s3.endpoint": os.getenv("AWS_ENDPOINT_URL_S3", "http://seaweedfs-s3:8333"),
        "s3.access-key-id": os.getenv("AWS_ACCESS_KEY_ID", "seaweedfs_access_key"),
        "s3.secret-access-key": os.getenv("AWS_SECRET_ACCESS_KEY", "seaweedfs_secret_key"),
        "s3.path-style-access": "true"
    })

    return catalog.load_table(table_name)
```

#### `modules/s3_utils.py`
```python
import boto3
import os

def get_s3_client():
    """SeaweedFS S3 í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    return boto3.client(
        's3',
        endpoint_url=os.getenv('AWS_ENDPOINT_URL_S3', 'http://seaweedfs-s3:8333'),
        aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID', 'seaweedfs_access_key'),
        aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY', 'seaweedfs_secret_key'),
        region_name='us-east-1'
    )
```

#### `requirements.txt`
```txt
streamlit==1.30.0
pyiceberg==0.5.1
pandas==2.1.4
boto3==1.34.0
Pillow==10.1.0
pyarrow==14.0.0
python-dotenv==1.0.0
```

### 5.4 ì„±ëŠ¥ ìµœì í™” íŒ

#### 1. ìºì‹± ì „ëµ
```python
# ë©”íƒ€ë°ì´í„° ìºì‹± (5ë¶„)
@st.cache_data(ttl=300)
def load_metadata(...):
    ...

# ì¸ë„¤ì¼ ìºì‹± (ë¬´ì œí•œ)
@st.cache_resource
def load_thumbnail_generator():
    return ImageThumbnailGenerator()
```

#### 2. í˜ì´ì§€ë„¤ì´ì…˜ (20ê°œì”© ë¡œë“œ)
```python
items_per_page = 20
df.iloc[start_idx:end_idx]  # í˜„ì¬ í˜ì´ì§€ë§Œ ë Œë”ë§
```

#### 3. ì¸ë„¤ì¼ ìƒì„± ë° ë³„ë„ ì €ì¥
```python
# fspark_raw_examples.py í™•ì¥
from PIL import Image
from io import BytesIO

# ì›ë³¸ ì´ë¯¸ì§€ ì—…ë¡œë“œ í›„
with Image.open(image_local_path) as img:
    img.thumbnail((200, 200))  # ì¸ë„¤ì¼ ìƒì„±
    thumb_buffer = BytesIO()
    img.save(thumb_buffer, format='PNG')
    thumb_bytes = thumb_buffer.getvalue()

    # S3ì— ì¸ë„¤ì¼ ì €ì¥
    thumb_s3_path = "s3a://lakehouse/raw/thumbnails/{date}/thumb_{filename}".format(...)
    # ... (ë™ì¼í•œ Hadoop FileSystem íŒ¨í„´ìœ¼ë¡œ ì—…ë¡œë“œ)
```

#### 4. Lazy Loading (Streamlit ë‚´ì¥)
Streamlitì˜ `st.image`ëŠ” ìë™ìœ¼ë¡œ lazy loading ì ìš©. ì¶”ê°€ ì‘ì—… ë¶ˆí•„ìš”.

#### 5. CDN ì—°ë™ (í”„ë¡œë•ì…˜)
```python
# CloudFront ë˜ëŠ” Fastly ì‚¬ìš©
cdn_url = f"https://cdn.example.com/{bucket}/{key}"
st.image(cdn_url)
```

---

## 6ï¸âƒ£ ë‹¨ê³„ë³„ êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸ âœ…

ì´ ì„¹ì…˜ì€ ì‹¤ì œ êµ¬í˜„ ì‹œ ë‹¨ê³„ë³„ë¡œ í™•ì¸í•  ìˆ˜ ìˆëŠ” **70ê°œì˜ ì²´í¬ë¦¬ìŠ¤íŠ¸ í•­ëª©**ì„ ì œê³µí•©ë‹ˆë‹¤. ê° í•­ëª©ì„ ì²´í¬í•˜ë©´ì„œ ì§„í–‰í•˜ë©´ ëˆ„ë½ ì—†ì´ ì™„ì „í•œ ì‹œê°í™” ìŠ¤íƒì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 6.1 Superset + Trino êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸ (25ê°œ í•­ëª©)

#### A. Docker í™˜ê²½ êµ¬ì„± (7ê°œ í•­ëª©)

- [ ] **1. Superset ì´ë¯¸ì§€ ì„ íƒ**: `apache/superset:latest-dev` ë˜ëŠ” stable ë²„ì „ ê²°ì •
- [ ] **2. PostgreSQL ì»¨í…Œì´ë„ˆ ì¶”ê°€**: Superset ë©”íƒ€ìŠ¤í† ì–´ìš© DB (postgres:15)
- [ ] **3. Redis ì»¨í…Œì´ë„ˆ ì¶”ê°€**: ìºì‹œ ë° ì„¸ì…˜ ìŠ¤í† ì–´ (redis:7-alpine)
- [ ] **4. docker-compose.ymlì— 3ê°œ ì„œë¹„ìŠ¤ ì¶”ê°€**: superset, superset-db, superset-redis
- [ ] **5. Named volume ìƒì„±**: `superset-data`, `superset-db-data`, `superset-redis-data`
- [ ] **6. ë„¤íŠ¸ì›Œí¬ ì—°ê²°**: ëª¨ë“  ì„œë¹„ìŠ¤ë¥¼ `lakehouse-net`ì— ì—°ê²°
- [ ] **7. Healthcheck ì„¤ì •**: `/health` ì—”ë“œí¬ì¸íŠ¸ë¡œ ìƒíƒœ í™•ì¸ (`curl -f http://localhost:8088/health`)

#### B. ì´ˆê¸° ì„¤ì • ë° ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ (3ê°œ í•­ëª©)

- [ ] **8. Admin ì‚¬ìš©ì ìƒì„±**:
  ```bash
  docker exec -it superset superset fab create-admin \
    --username admin --firstname Admin --lastname User \
    --email admin@example.com --password admin
  ```
- [ ] **9. ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰**:
  ```bash
  docker exec -it superset superset db upgrade
  ```
- [ ] **10. Superset ì´ˆê¸°í™”**:
  ```bash
  docker exec -it superset superset init
  ```

#### C. Trino ë°ì´í„° ì†ŒìŠ¤ ì—°ê²° (4ê°œ í•­ëª©)

- [ ] **11. Trino SQLAlchemy ë“œë¼ì´ë²„ ì„¤ì¹˜ í™•ì¸**: Superset ì»¨í…Œì´ë„ˆ ë‚´ì—ì„œ `pip list | grep trino` ì‹¤í–‰
- [ ] **12. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° URI ì„¤ì •**: Superset UIì—ì„œ Database ì¶”ê°€
  ```
  trino://user@trino:8080/hive_prod
  ```
- [ ] **13. ì—°ê²° í…ŒìŠ¤íŠ¸**: SQL Labì—ì„œ `SHOW TABLES IN hive_prod.option_ticks_db` ì‹¤í–‰
- [ ] **14. Iceberg ì¹´íƒˆë¡œê·¸ ì¸ì‹ í™•ì¸**:
  ```sql
  SELECT * FROM hive_prod.option_ticks_db.bronze_option_ticks LIMIT 10;
  ```

#### D. ëŒ€ì‹œë³´ë“œ êµ¬ì„± (4ê°œ í•­ëª©)

- [ ] **15. ìƒ˜í”Œ ë°ì´í„°ì…‹ 3ê°œ ìƒì„±**:
  - ì •í˜•: ì˜µì…˜ í‹± ë°ì´í„° (ì‹œê³„ì—´)
  - ë°˜ì •í˜•: ë¡œê·¸ ë°ì´í„° (JSON íŒŒì‹±)
  - ì§‘ê³„: ê±°ë˜ëŸ‰ í†µê³„
- [ ] **16. ì°¨íŠ¸ 3ê°œ ìƒì„±**:
  - Line Chart: ì‹œê°„ë³„ ê°€ê²© ë³€í™”
  - Bar Chart: ì‹¬ë³¼ë³„ ê±°ë˜ëŸ‰
  - Pivot Table: ì¼ë³„ í†µê³„
- [ ] **17. ëŒ€ì‹œë³´ë“œ 1ê°œ ìƒì„±**: ìœ„ 3ê°œ ì°¨íŠ¸ë¥¼ í†µí•©í•œ "Lakehouse Analytics" ëŒ€ì‹œë³´ë“œ
- [ ] **18. í•„í„° ì„¤ì •**: ë‚ ì§œ ë²”ìœ„, ì‹¬ë³¼, ê±°ë˜ì†Œ í•„í„° ì¶”ê°€

#### E. ë³´ì•ˆ ë° ê¶Œí•œ ê´€ë¦¬ (5ê°œ í•­ëª©)

- [ ] **19. RBAC í™œì„±í™”**: Settings â†’ Security â†’ Enable RBAC
- [ ] **20. ì—­í•  5ê°œ ìƒì„±**: Admin, Analyst, Viewer, Developer, Ops
- [ ] **21. ë°ì´í„° ì†ŒìŠ¤ë³„ ì ‘ê·¼ ê¶Œí•œ ì„¤ì •**: ê° ì—­í• ì— Database ê¶Œí•œ ë¶€ì—¬
- [ ] **22. Row-level security ê·œì¹™ ì„¤ì •**: ì˜ˆ: ì‚¬ìš©ìë³„ ì‹¬ë³¼ í•„í„°ë§
- [ ] **23. Audit logging í™œì„±í™”**: í™˜ê²½ ë³€ìˆ˜ `SUPERSET_AUDIT_LOG=1` ì„¤ì •

#### F. ì„±ëŠ¥ ìµœì í™” (2ê°œ í•­ëª©)

- [ ] **24. Redis ìºì‹œ íƒ€ì„ì•„ì›ƒ ì„¤ì •**: `superset_config.py`ì—ì„œ `CACHE_DEFAULT_TIMEOUT = 300` (5ë¶„)
- [ ] **25. Materialized View ìƒì„±** (Trinoì—ì„œ):
  ```sql
  CREATE MATERIALIZED VIEW hive_prod.option_ticks_db.mv_daily_stats AS
  SELECT DATE(timestamp) as date, symbol,
         AVG(last_price) as avg_price, SUM(volume) as total_volume
  FROM hive_prod.option_ticks_db.bronze_option_ticks
  GROUP BY DATE(timestamp), symbol;
  ```

---

### 6.2 Grafana + OpenSearch êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸ (20ê°œ í•­ëª©)

#### A. OpenSearch í´ëŸ¬ìŠ¤í„° êµ¬ì„± (7ê°œ í•­ëª©)

- [ ] **1. OpenSearch ì»¨í…Œì´ë„ˆ ì¶”ê°€**: `opensearchproject/opensearch:2.11.1`
- [ ] **2. OpenSearch Dashboards ì»¨í…Œì´ë„ˆ ì¶”ê°€**: UI ì œê³µ (í¬íŠ¸ 5601)
- [ ] **3. ì´ˆê¸° admin ë¹„ë°€ë²ˆí˜¸ ì„¤ì •**: í™˜ê²½ ë³€ìˆ˜ `OPENSEARCH_INITIAL_ADMIN_PASSWORD=Admin@123`
- [ ] **4. Single-node ëª¨ë“œ ì„¤ì •**: `discovery.type=single-node`
- [ ] **5. í¬íŠ¸ ë§¤í•‘**: 9200 (REST API), 9600 (Performance Analyzer)
- [ ] **6. ë³¼ë¥¨ ë§ˆìš´íŠ¸**: `opensearch-data:/usr/share/opensearch/data`
- [ ] **7. JVM í™ í¬ê¸° ì„¤ì •**: `OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m`

#### B. ë¡œê·¸ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ êµ¬ì„± (3ê°œ í•­ëª©)

- [ ] **8. Fluentd ë˜ëŠ” Filebeat ì»¨í…Œì´ë„ˆ ì¶”ê°€**: ë¡œê·¸ ìˆ˜ì§‘ê¸°
- [ ] **9. ë¡œê·¸ ìˆ˜ì§‘ ê²½ë¡œ ì„¤ì •**:
  - Spark ë¡œê·¸: `/home/iceberg/logs/*.log`
  - SeaweedFS ë¡œê·¸: `/var/log/seaweedfs/*.log`
  - Hive Metastore ë¡œê·¸: `/opt/hive/logs/*.log`
- [ ] **10. OpenSearch ì¸ë±ìŠ¤ í…œí”Œë¦¿ ì‘ì„±**:
  ```json
  {
    "index_patterns": ["logs-*"],
    "template": {
      "mappings": {
        "properties": {
          "timestamp": {"type": "date"},
          "level": {"type": "keyword"},
          "message": {"type": "text"},
          "service": {"type": "keyword"}
        }
      }
    }
  }
  ```

#### C. Grafana ì„¤ì • (4ê°œ í•­ëª©)

- [ ] **11. Grafana ì»¨í…Œì´ë„ˆ ì¶”ê°€**: `grafana/grafana:10.3.0`
- [ ] **12. ì´ˆê¸° admin ë¹„ë°€ë²ˆí˜¸ ì„¤ì •**: `GF_SECURITY_ADMIN_PASSWORD=admin`
- [ ] **13. OpenSearch ë°ì´í„° ì†ŒìŠ¤ í”ŒëŸ¬ê·¸ì¸ ì„¤ì¹˜**:
  ```yaml
  environment:
    GF_INSTALL_PLUGINS: grafana-opensearch-datasource
  ```
- [ ] **14. Prometheus ë°ì´í„° ì†ŒìŠ¤ ì¶”ê°€**: ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ìš©

#### D. ëŒ€ì‹œë³´ë“œ êµ¬ì„± (3ê°œ í•­ëª©)

- [ ] **15. ìƒ˜í”Œ ëŒ€ì‹œë³´ë“œ 5ê°œ ìƒì„±**:
  1. Lakehouse Overview (ì „ì²´ ì‹œìŠ¤í…œ ìƒíƒœ)
  2. Data Quality (null ë¹„ìœ¨, ì¤‘ë³µ ë“±)
  3. Performance (ì¿¼ë¦¬ ì‘ë‹µì‹œê°„, ì²˜ë¦¬ëŸ‰)
  4. Logs (ì‹¤ì‹œê°„ ë¡œê·¸ ìŠ¤íŠ¸ë¦¼)
  5. SeaweedFS (ìŠ¤í† ë¦¬ì§€ ìš©ëŸ‰, I/O)
- [ ] **16. ì•Œë¦¼ ì±„ë„ ì„¤ì •**: Slack, Email ì—°ë™
- [ ] **17. ì•Œë¦¼ ê·œì¹™ 3ê°œ ìƒì„±**:
  - CPU ì‚¬ìš©ë¥  > 80%
  - ë””ìŠ¤í¬ ìš©ëŸ‰ > 90%
  - ì¿¼ë¦¬ ì˜¤ë¥˜ìœ¨ > 5%

#### E. í”„ë¡œë¹„ì €ë‹ ë° ë°±ì—… (3ê°œ í•­ëª©)

- [ ] **18. ëŒ€ì‹œë³´ë“œ JSON íŒŒì¼ë¡œ export**: ë²„ì „ ê´€ë¦¬ ëª©ì 
- [ ] **19. Provisioning ë””ë ‰í† ë¦¬ êµ¬ì„±**:
  ```
  config/grafana/provisioning/
  â”œâ”€â”€ datasources/
  â”‚   â”œâ”€â”€ opensearch.yml
  â”‚   â””â”€â”€ prometheus.yml
  â””â”€â”€ dashboards/
      â”œâ”€â”€ lakehouse-overview.json
      â”œâ”€â”€ data-quality.json
      â””â”€â”€ performance.json
  ```
- [ ] **20. Gitìœ¼ë¡œ í”„ë¡œë¹„ì €ë‹ íŒŒì¼ ë²„ì „ ê´€ë¦¬**: `.gitignore`ì—ì„œ ì œì™¸í•˜ê³  ì»¤ë°‹

---

### 6.3 Streamlit êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸ (15ê°œ í•­ëª©)

#### A. Docker í™˜ê²½ êµ¬ì„± (5ê°œ í•­ëª©)

- [ ] **1. Python ë² ì´ìŠ¤ ì´ë¯¸ì§€ ì„ íƒ**: `python:3.11-slim`
- [ ] **2. requirements.txt ì‘ì„±**:
  ```txt
  streamlit==1.30.0
  pyiceberg==0.5.1
  pandas==2.1.4
  boto3==1.34.0
  Pillow==10.1.0
  pyarrow==14.0.0
  ```
- [ ] **3. docker-composeì—ì„œ command ì„¤ì •**: `streamlit run app.py --server.port=8501`
- [ ] **4. í¬íŠ¸ ë§¤í•‘**: 8501:8501
- [ ] **5. ë³¼ë¥¨ ë§ˆìš´íŠ¸**: `./streamlit-app:/app`

#### B. ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ì‘ì„± (6ê°œ í•­ëª©)

- [ ] **6. `app.py` ë©”ì¸ íŒŒì¼ ì‘ì„±**: ë„¤ë¹„ê²Œì´ì…˜ ë° ì—°ê²° ìƒíƒœ í™•ì¸
- [ ] **7. `pages/01_Gallery.py` ì‘ì„±**: ì´ë¯¸ì§€ ê°¤ëŸ¬ë¦¬ (4ì—´ ê·¸ë¦¬ë“œ)
- [ ] **8. `pages/02_ğŸ”_Metadata_Search.py` ì‘ì„±**: ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ ê¸°ëŠ¥
- [ ] **9. `pages/03_Statistics.py` ì‘ì„±**: í†µê³„ ëŒ€ì‹œë³´ë“œ
- [ ] **10. `modules/iceberg_connector.py` ì‘ì„±**: PyIceberg ì—°ê²° ëª¨ë“ˆ
- [ ] **11. `modules/s3_utils.py` ì‘ì„±**: S3 í´ë¼ì´ì–¸íŠ¸ ìƒì„± ëª¨ë“ˆ

#### C. ê¸°ëŠ¥ êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸ (4ê°œ í•­ëª©)

- [ ] **12. PyIceberg ì—°ê²° í…ŒìŠ¤íŠ¸**: Hive Metastore ì ‘ì† í™•ì¸
- [ ] **13. S3 ì—°ê²° í…ŒìŠ¤íŠ¸**: ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸
- [ ] **14. ë©”íƒ€ë°ì´í„° ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸**:
  ```python
  table = get_iceberg_table("hive_prod.media_db.image_metadata")
  df = table.scan().to_pandas()
  assert len(df) > 0
  ```
- [ ] **15. ê°¤ëŸ¬ë¦¬ ë Œë”ë§ í™•ì¸**: ì´ë¯¸ì§€ 4ì—´ ê·¸ë¦¬ë“œ í‘œì‹œ, ë©”íƒ€ë°ì´í„° expander ë™ì‘ í™•ì¸

---

### 6.4 í†µí•© í…ŒìŠ¤íŠ¸ ì²´í¬ë¦¬ìŠ¤íŠ¸ (10ê°œ í•­ëª©)

#### ë°ì´í„° íŒŒì´í”„ë¼ì¸ End-to-End í…ŒìŠ¤íŠ¸

- [ ] **1. Step 1 - ìƒ˜í”Œ ì´ë¯¸ì§€ ì—…ë¡œë“œ**: `python/fspark_raw_examples.py` ì‹¤í–‰í•˜ì—¬ S3ì— ì´ë¯¸ì§€ 5ê°œ ì—…ë¡œë“œ
- [ ] **2. Step 2 - Iceberg í…Œì´ë¸” ìƒì„±**: Trinoì—ì„œ `hive_prod.media_db.image_metadata` DDL ì‹¤í–‰
- [ ] **3. Step 3 - Streamlit ê°¤ëŸ¬ë¦¬ í™•ì¸**: http://localhost:8501 ì ‘ì†, ì´ë¯¸ì§€ ë Œë”ë§ í™•ì¸
- [ ] **4. Step 4 - Superset Trino ì—°ê²°**: Supersetì—ì„œ Trino ë°ì´í„° ì†ŒìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸
- [ ] **5. Step 5 - Superset ëŒ€ì‹œë³´ë“œ ìƒì„±**: í‹± ë°ì´í„° ì‹œê°í™” ì°¨íŠ¸ 3ê°œ ìƒì„±
- [ ] **6. Step 6 - Grafana OpenSearch ì—°ê²°**: Grafanaì—ì„œ OpenSearch ë°ì´í„° ì†ŒìŠ¤ ì¶”ê°€
- [ ] **7. Step 7 - Grafana ëŒ€ì‹œë³´ë“œ í™•ì¸**: ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ íŒ¨ë„ ìƒì„± ë° ë°ì´í„° í™•ì¸
- [ ] **8. Step 8 - ì „ì²´ ì„œë¹„ìŠ¤ ì ‘ì† URL í…ŒìŠ¤íŠ¸**:
  ```
  Superset: http://localhost:8088
  Grafana: http://localhost:3000
  Streamlit: http://localhost:8501
  Trino UI: http://localhost:8080/ui
  OpenSearch: http://localhost:9200
  ```
- [ ] **9. Step 9 - ì„±ëŠ¥ ì¸¡ì •**: Superset ëŒ€ì‹œë³´ë“œ ë¡œë”© ì‹œê°„, Streamlit ê°¤ëŸ¬ë¦¬ ë Œë”ë§ ì‹œê°„ ì¸¡ì •
- [ ] **10. Step 10 - ì¥ì•  ë³µêµ¬ í…ŒìŠ¤íŠ¸**: ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘ í›„ ë°ì´í„° ë³´ì¡´ í™•ì¸ (`docker-compose restart superset`)

---

## 7ï¸âƒ£ docker-compose.yml í™•ì¥ ì˜ˆì‹œ (ì‹¤í–‰ ê°€ëŠ¥í•œ YAML) ğŸ³

ì´ ì„¹ì…˜ì€ í˜„ì¬ [docker-compose.yml](../../docker-compose.yml)ì— **ë³µì‚¬-ë¶™ì—¬ë„£ê¸°**ë¡œ ì¦‰ì‹œ ì¶”ê°€í•  ìˆ˜ ìˆëŠ” ì™„ì „í•œ ì„œë¹„ìŠ¤ ì •ì˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

### 7.1 Superset ìŠ¤íƒ ì¶”ê°€

ê¸°ì¡´ `docker-compose.yml`ì˜ `services:` ë¸”ë¡ ì•ˆì— ë‹¤ìŒ ë‚´ìš©ì„ ì¶”ê°€í•˜ì„¸ìš”.

```yaml
# ============================================================================
# Visualization Layer: Apache Superset (BI Dashboard)
# ============================================================================

superset:
  image: apache/superset:latest-dev
  container_name: superset
  depends_on:
    superset-db:
      condition: service_healthy
    superset-redis:
      condition: service_started
    trino:
      condition: service_started
  ports:
    - "8088:8088"
  environment:
    # Security
    SUPERSET_SECRET_KEY: "CHANGE_THIS_TO_A_RANDOM_SECRET_KEY_AT_LEAST_42_CHARS"
    SUPERSET_LOAD_EXAMPLES: "no"

    # Database
    SQLALCHEMY_DATABASE_URI: postgresql://superset:superset@superset-db:5432/superset

    # Cache
    REDIS_HOST: superset-redis
    REDIS_PORT: 6379

    # Features
    SUPERSET_WEBSERVER_TIMEOUT: 60
    SUPERSET_ROW_LIMIT: 10000
  volumes:
    - superset-data:/app/superset_home
    - ./config/superset/superset_config.py:/app/pythonpath/superset_config.py:ro
    - ./logs/superset:/app/logs
  networks:
    - default
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8088/health"]
    interval: 30s
    timeout: 10s
    retries: 3
    start_period: 60s
  deploy:
    resources:
      limits:
        cpus: '2'
        memory: 4G
      reservations:
        cpus: '1'
        memory: 2G
  command: >
    bash -c "
    superset db upgrade &&
    superset fab create-admin --username admin --firstname Admin --lastname User --email admin@example.com --password admin || true &&
    superset init &&
    gunicorn -w 4 -b 0.0.0.0:8088 --timeout 60 superset.app:create_app()
    "

superset-db:
  image: postgres:15
  container_name: superset-db
  environment:
    POSTGRES_DB: superset
    POSTGRES_USER: superset
    POSTGRES_PASSWORD: superset
  volumes:
    - superset-db-data:/var/lib/postgresql/data
  networks:
    - default
  healthcheck:
    test: ["CMD", "pg_isready", "-U", "superset", "-d", "superset"]
    interval: 10s
    timeout: 5s
    retries: 5

superset-redis:
  image: redis:7-alpine
  container_name: superset-redis
  ports:
    - "6380:6379"
  volumes:
    - superset-redis-data:/data
  networks:
    - default
  command: redis-server --maxmemory 256mb --maxmemory-policy allkeys-lru
```

### 7.2 Grafana + OpenSearch ìŠ¤íƒ ì¶”ê°€

```yaml
# ============================================================================
# Monitoring Layer: Grafana + OpenSearch
# ============================================================================

opensearch:
  image: opensearchproject/opensearch:2.11.1
  container_name: opensearch
  environment:
    - cluster.name=lakehouse-logs
    - node.name=opensearch-node1
    - discovery.type=single-node
    - bootstrap.memory_lock=true
    - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m"
    - OPENSEARCH_INITIAL_ADMIN_PASSWORD=Admin@123
    - plugins.security.disabled=false
  ulimits:
    memlock:
      soft: -1
      hard: -1
    nofile:
      soft: 65536
      hard: 65536
  ports:
    - "9200:9200"
    - "9600:9600"
  volumes:
    - opensearch-data:/usr/share/opensearch/data
  networks:
    - default
  healthcheck:
    test: ["CMD", "curl", "-ku", "admin:Admin@123", "https://localhost:9200/_cluster/health"]
    interval: 30s
    timeout: 10s
    retries: 5
    start_period: 60s

opensearch-dashboards:
  image: opensearchproject/opensearch-dashboards:2.11.1
  container_name: opensearch-dashboards
  depends_on:
    opensearch:
      condition: service_healthy
  ports:
    - "5601:5601"
  environment:
    OPENSEARCH_HOSTS: '["https://opensearch:9200"]'
    OPENSEARCH_USERNAME: admin
    OPENSEARCH_PASSWORD: Admin@123
  networks:
    - default

grafana:
  image: grafana/grafana:10.3.0
  container_name: grafana
  depends_on:
    opensearch:
      condition: service_healthy
    prometheus:
      condition: service_started
  ports:
    - "3000:3000"
  environment:
    # Security
    GF_SECURITY_ADMIN_USER: admin
    GF_SECURITY_ADMIN_PASSWORD: admin

    # Plugins
    GF_INSTALL_PLUGINS: grafana-opensearch-datasource,grafana-clock-panel

    # Auth
    GF_AUTH_ANONYMOUS_ENABLED: "false"

    # Server
    GF_SERVER_ROOT_URL: http://localhost:3000
  volumes:
    - grafana-data:/var/lib/grafana
    - ./config/grafana/provisioning:/etc/grafana/provisioning:ro
    - ./logs/grafana:/var/log/grafana
  networks:
    - default
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
    interval: 30s
    timeout: 10s
    retries: 3

prometheus:
  image: prom/prometheus:v2.49.0
  container_name: prometheus
  ports:
    - "9090:9090"
  command:
    - '--config.file=/etc/prometheus/prometheus.yml'
    - '--storage.tsdb.path=/prometheus'
    - '--storage.tsdb.retention.time=30d'
  volumes:
    - ./config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    - prometheus-data:/prometheus
  networks:
    - default
  healthcheck:
    test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
    interval: 30s
    timeout: 10s
    retries: 3

node-exporter:
  image: prom/node-exporter:v1.7.0
  container_name: node-exporter
  ports:
    - "9100:9100"
  command:
    - '--path.procfs=/host/proc'
    - '--path.sysfs=/host/sys'
    - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
  volumes:
    - /proc:/host/proc:ro
    - /sys:/host/sys:ro
    - /:/rootfs:ro
  networks:
    - default
```

### 7.3 Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜ ì¶”ê°€

```yaml
# ============================================================================
# Application Layer: Streamlit (Unstructured Data Explorer)
# ============================================================================

streamlit:
  image: python:3.11-slim
  container_name: streamlit-app
  working_dir: /app
  depends_on:
    hive-metastore:
      condition: service_healthy
    seaweedfs-s3:
      condition: service_healthy
  ports:
    - "8501:8501"
  environment:
    # S3 Credentials
    AWS_ACCESS_KEY_ID: seaweedfs_access_key
    AWS_SECRET_ACCESS_KEY: seaweedfs_secret_key
    AWS_ENDPOINT_URL_S3: http://seaweedfs-s3:8333
    AWS_REGION: us-east-1

    # Iceberg Catalog
    HIVE_METASTORE_URI: thrift://hive-metastore:9083

    # Streamlit Config
    STREAMLIT_SERVER_PORT: 8501
    STREAMLIT_SERVER_HEADLESS: "true"
    STREAMLIT_BROWSER_GATHER_USAGE_STATS: "false"
  volumes:
    - ./streamlit-app:/app
    - ./logs/streamlit:/app/logs
  networks:
    - default
  command: >
    bash -c "
    pip install --no-cache-dir -r requirements.txt &&
    streamlit run app.py --server.port=8501 --server.headless=true --server.address=0.0.0.0
    "
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8501/_stcore/health"]
    interval: 30s
    timeout: 10s
    retries: 3
    start_period: 30s
  deploy:
    resources:
      limits:
        cpus: '1'
        memory: 2G
```

### 7.4 Volumes í™•ì¥

ê¸°ì¡´ `volumes:` ë¸”ë¡ì— ë‹¤ìŒ ë‚´ìš©ì„ ì¶”ê°€í•˜ì„¸ìš”.

```yaml
volumes:
  # Existing volumes
  warehouse:
  postgres-data:
  seaweedfs-data:

  # Superset
  superset-data:
  superset-db-data:
  superset-redis-data:

  # Grafana + OpenSearch
  grafana-data:
  opensearch-data:
  prometheus-data:
```

### 7.5 í•„ìˆ˜ ì„¤ì • íŒŒì¼ ìƒì„±

ìœ„ YAMLì„ ì ìš©í•˜ê¸° ì „ì— ë‹¤ìŒ ì„¤ì • íŒŒì¼ë“¤ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.

#### `config/prometheus/prometheus.yml`
```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

  - job_name: 'trino'
    static_configs:
      - targets: ['trino:8080']
```

#### `config/superset/superset_config.py`
```python
# Superset ì„¤ì • íŒŒì¼
import os

# Security
SECRET_KEY = os.environ.get('SUPERSET_SECRET_KEY', 'CHANGE_THIS_SECRET_KEY')

# Database
SQLALCHEMY_DATABASE_URI = os.environ.get('SQLALCHEMY_DATABASE_URI')

# Cache
CACHE_CONFIG = {
    'CACHE_TYPE': 'RedisCache',
    'CACHE_DEFAULT_TIMEOUT': 300,
    'CACHE_KEY_PREFIX': 'superset_',
    'CACHE_REDIS_HOST': os.environ.get('REDIS_HOST', 'superset-redis'),
    'CACHE_REDIS_PORT': os.environ.get('REDIS_PORT', 6379),
}

# Features
FEATURE_FLAGS = {
    'ENABLE_TEMPLATE_PROCESSING': True,
}
```

#### `streamlit-app/requirements.txt`
```txt
streamlit==1.30.0
pyiceberg==0.5.1
pandas==2.1.4
boto3==1.34.0
Pillow==10.1.0
pyarrow==14.0.0
python-dotenv==1.0.0
```

---

## 8ï¸âƒ£ í†µí•© í…ŒìŠ¤íŠ¸ ë° ìš´ì˜ ê°€ì´ë“œ ğŸ§ª

### 8.1 ì„œë¹„ìŠ¤ ì‹œì‘ ë° ì´ˆê¸° ì„¤ì •

#### ì „ì²´ ìŠ¤íƒ ì‹œì‘
```bash
# 1. í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd /home/i/work/ai/lakehouse-tick

# 2. í•„ìˆ˜ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p config/prometheus config/superset config/grafana/provisioning streamlit-app logs/{superset,grafana,streamlit}

# 3. ì„¤ì • íŒŒì¼ ìƒì„± (ìœ„ 7.5 ì„¹ì…˜ ì°¸ì¡°)

# 4. docker-compose.yml ì—…ë°ì´íŠ¸ í›„ ì‹¤í–‰
docker-compose up -d

# 5. ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
docker-compose ps

# 6. ë¡œê·¸ í™•ì¸ (ë¬¸ì œ ë°œìƒ ì‹œ)
docker-compose logs -f superset grafana streamlit
```

#### ê° ì„œë¹„ìŠ¤ ì ‘ì† URL ë° ì´ˆê¸° ê³„ì •

| ì„œë¹„ìŠ¤ | URL | ì´ˆê¸° ê³„ì • | ìš©ë„ |
|--------|-----|----------|------|
| **Superset** | http://localhost:8088 | admin / admin | BI ëŒ€ì‹œë³´ë“œ |
| **Grafana** | http://localhost:3000 | admin / admin | ëª¨ë‹ˆí„°ë§ |
| **OpenSearch Dashboards** | http://localhost:5601 | admin / Admin@123 | ë¡œê·¸ íƒìƒ‰ |
| **Streamlit** | http://localhost:8501 | (ì¸ì¦ ì—†ìŒ) | ì´ë¯¸ì§€ ê°¤ëŸ¬ë¦¬ |
| **Trino UI** | http://localhost:8080/ui | (ì¸ì¦ ì—†ìŒ) | ì¿¼ë¦¬ ëª¨ë‹ˆí„°ë§ |
| **Prometheus** | http://localhost:9090 | (ì¸ì¦ ì—†ìŒ) | ë©”íŠ¸ë¦­ ì›ë³¸ |

### 8.2 íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ

#### ë¬¸ì œ 1: Supersetì—ì„œ Trino ì—°ê²° ì‹¤íŒ¨
**ì¦ìƒ**: `Connection test failed: could not connect to server`

**í•´ê²°**:
```bash
# 1. Trino ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸
docker exec -it trino curl -f http://localhost:8080/v1/info

# 2. ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸ (Superset ì»¨í…Œì´ë„ˆì—ì„œ)
docker exec -it superset ping trino

# 3. Trino ë¡œê·¸ í™•ì¸
docker logs trino | tail -50

# 4. SQLAlchemy URI ì¬í™•ì¸ (Superset UI)
# ì˜¬ë°”ë¥¸ í˜•ì‹: trino://user@trino:8080/hive_prod
```

#### ë¬¸ì œ 2: Streamlitì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨
**ì¦ìƒ**: `Failed to load img-001: An error occurred (NoSuchKey)`

**í•´ê²°**:
```bash
# 1. S3 ê²½ë¡œ í™•ì¸
docker exec -it streamlit-app python -c "
import boto3
s3 = boto3.client('s3', endpoint_url='http://seaweedfs-s3:8333',
                  aws_access_key_id='seaweedfs_access_key',
                  aws_secret_access_key='seaweedfs_secret_key')
print(s3.list_objects_v2(Bucket='lakehouse', Prefix='raw/images/'))
"

# 2. Streamlit ë¡œê·¸ í™•ì¸
docker logs streamlit-app | grep ERROR

# 3. í™˜ê²½ ë³€ìˆ˜ í™•ì¸
docker exec -it streamlit-app env | grep AWS
```

#### ë¬¸ì œ 3: Grafanaì—ì„œ OpenSearch ë°ì´í„° ì†ŒìŠ¤ ì—°ê²° ì‹¤íŒ¨
**ì¦ìƒ**: `Bad Gateway` ë˜ëŠ” `SSL verification failed`

**í•´ê²°**:
```bash
# 1. OpenSearch í—¬ìŠ¤ í™•ì¸
curl -ku admin:Admin@123 https://localhost:9200/_cluster/health

# 2. Grafana ì»¨í…Œì´ë„ˆì—ì„œ OpenSearch ì ‘ê·¼ í…ŒìŠ¤íŠ¸
docker exec -it grafana curl -k https://opensearch:9200

# 3. Grafana UIì—ì„œ "Skip TLS Verify" ì˜µì…˜ í™œì„±í™”

# 4. OpenSearch ë¡œê·¸ í™•ì¸
docker logs opensearch | grep ERROR
```

### 8.3 ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

#### í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤
```bash
# 1. Superset ì¿¼ë¦¬ ì‘ë‹µì‹œê°„ ì¸¡ì •
time curl -X POST http://localhost:8088/api/v1/chart/data \
  -H "Content-Type: application/json" \
  -d '{"datasource": {...}, "queries": [...]}'

# 2. Streamlit ì•± ë¡œë”© ì‹œê°„ ì¸¡ì •
time curl http://localhost:8501

# 3. Trino ì¿¼ë¦¬ ì‹¤í–‰ ê³„íš í™•ì¸
docker exec -it trino trino --server localhost:8080 --catalog hive_prod --execute "
EXPLAIN SELECT * FROM hive_prod.option_ticks_db.bronze_option_ticks
WHERE timestamp >= CURRENT_DATE - INTERVAL '1' DAY;
"
```

#### ì˜ˆìƒ ì„±ëŠ¥ ê¸°ì¤€

| ë©”íŠ¸ë¦­ | ëª©í‘œ | í˜„ì‹¤ (ë¡œì»¬ í™˜ê²½) |
|--------|------|-------------|
| Superset ëŒ€ì‹œë³´ë“œ ë¡œë”© | < 5ì´ˆ | 3-7ì´ˆ |
| Streamlit ê°¤ëŸ¬ë¦¬ ë Œë”ë§ (20ê°œ ì´ë¯¸ì§€) | < 3ì´ˆ | 2-4ì´ˆ |
| Grafana ì‹¤ì‹œê°„ ë¡œê·¸ ìƒˆë¡œê³ ì¹¨ | < 1ì´ˆ | 0.5-1.5ì´ˆ |
| Trino 1ì¼ì¹˜ ë°ì´í„° ì¡°íšŒ (100ë§Œ í–‰) | < 10ì´ˆ | 5-15ì´ˆ |

### 8.4 ë°±ì—… ë° ì¬í•´ ë³µêµ¬

#### ì •ê¸° ë°±ì—… ìŠ¤í¬ë¦½íŠ¸ (`backup.sh`)
```bash
#!/bin/bash
# Lakehouse ì‹œê°í™” ìŠ¤íƒ ë°±ì—…

BACKUP_DIR="/backups/$(date +%Y%m%d_%H%M%S)"
mkdir -p $BACKUP_DIR

echo "=== Lakehouse Visualization Stack Backup ==="
echo "Backup directory: $BACKUP_DIR"

# 1. Superset ë©”íƒ€ìŠ¤í† ì–´ ë°±ì—…
echo "Backing up Superset database..."
docker exec superset-db pg_dump -U superset superset > $BACKUP_DIR/superset-db.sql

# 2. Grafana ì„¤ì • ë°±ì—…
echo "Backing up Grafana data..."
docker exec grafana tar -czf - /var/lib/grafana > $BACKUP_DIR/grafana-data.tar.gz

# 3. OpenSearch ìŠ¤ëƒ…ìƒ· (ì„ íƒ)
echo "Creating OpenSearch snapshot..."
docker exec opensearch curl -X POST "https://localhost:9200/_snapshot/my_backup/snapshot_$(date +%Y%m%d)" \
  -ku admin:Admin@123 \
  -H 'Content-Type: application/json' \
  -d '{"indices": "logs-*"}'

# 4. Prometheus ë°ì´í„° ë°±ì—…
echo "Backing up Prometheus data..."
docker exec prometheus tar -czf - /prometheus > $BACKUP_DIR/prometheus-data.tar.gz

# 5. ì„¤ì • íŒŒì¼ ë°±ì—…
echo "Backing up config files..."
tar -czf $BACKUP_DIR/config-backup.tar.gz ./config ./streamlit-app

echo "Backup completed: $BACKUP_DIR"
ls -lh $BACKUP_DIR
```

#### ë³µêµ¬ ì ˆì°¨
```bash
#!/bin/bash
# ë°±ì—…ìœ¼ë¡œë¶€í„° ë³µêµ¬

RESTORE_DIR="/backups/20251225_140000"  # ë°±ì—… ë””ë ‰í† ë¦¬ ê²½ë¡œ

# 1. ì»¨í…Œì´ë„ˆ ì¤‘ì§€
docker-compose down

# 2. Superset DB ë³µì›
cat $RESTORE_DIR/superset-db.sql | docker-compose run --rm superset-db psql -U superset superset

# 3. Grafana ë°ì´í„° ë³µì›
docker-compose up -d grafana
docker exec grafana tar -xzf - -C / < $RESTORE_DIR/grafana-data.tar.gz
docker-compose restart grafana

# 4. ì„¤ì • íŒŒì¼ ë³µì›
tar -xzf $RESTORE_DIR/config-backup.tar.gz -C .

# 5. ì „ì²´ ì¬ì‹œì‘
docker-compose up -d

echo "Restore completed. Please verify services:"
docker-compose ps
```

### 8.5 ìš´ì˜ ëª¨ë‹ˆí„°ë§ ì²´í¬ë¦¬ìŠ¤íŠ¸

#### ì¼ì¼ ì ê²€ í•­ëª©
- [ ] ëª¨ë“  ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸: `docker-compose ps`
- [ ] ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ í™•ì¸: `df -h`
- [ ] Grafana ëŒ€ì‹œë³´ë“œì—ì„œ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ í™•ì¸
- [ ] Superset ëŒ€ì‹œë³´ë“œ ë¡œë”© ì‹œê°„ í™•ì¸
- [ ] OpenSearch ì¸ë±ìŠ¤ í¬ê¸° í™•ì¸: `curl -ku admin:Admin@123 https://localhost:9200/_cat/indices?v`

#### ì£¼ê°„ ì ê²€ í•­ëª©
- [ ] ë°±ì—… ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ë° ê²€ì¦
- [ ] ë¡œê·¸ íŒŒì¼ ìš©ëŸ‰ í™•ì¸ ë° ë¡œí…Œì´ì…˜: `./logs/*/`
- [ ] Docker ë³¼ë¥¨ í¬ê¸° í™•ì¸: `docker system df -v`
- [ ] ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ ì—…ë°ì´íŠ¸ í™•ì¸: `docker-compose pull`
- [ ] ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ (ëŒ€ì‹œë³´ë“œ ì†ë„, ì˜¤ë¥˜ ë“±)

#### ì›”ê°„ ì ê²€ í•­ëª©
- [ ] Prometheus ë°ì´í„° ë³´ì¡´ ê¸°ê°„ ê²€í†  (ê¸°ë³¸ 30ì¼)
- [ ] OpenSearch ì˜¤ë˜ëœ ì¸ë±ìŠ¤ ì‚­ì œ ë˜ëŠ” ì•„ì¹´ì´ë¹™
- [ ] Grafana ëŒ€ì‹œë³´ë“œ ìµœì í™” (ë¶ˆí•„ìš”í•œ íŒ¨ë„ ì œê±°)
- [ ] Superset ì‚¬ìš©ì ê¶Œí•œ ê²€í† 
- [ ] ì „ì²´ ìŠ¤íƒ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

---

## ğŸ¯ ìµœì¢… êµ¬í˜„ ìš”ì•½

### ì™„ì„±ëœ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Lakehouse Visualization Stack              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Visualization Layer (3 containers)                         â”‚
â”‚  â”œâ”€ Superset         â†’ BI Dashboard (port 8088)             â”‚
â”‚  â”œâ”€ Superset-DB      â†’ PostgreSQL Metastore                 â”‚
â”‚  â””â”€ Superset-Redis   â†’ Cache & Session Store                â”‚
â”‚                                                              â”‚
â”‚  Monitoring Layer (5 containers)                            â”‚
â”‚  â”œâ”€ Grafana          â†’ Dashboard (port 3000)                â”‚
â”‚  â”œâ”€ OpenSearch       â†’ Log Storage (port 9200)              â”‚
â”‚  â”œâ”€ OpenSearch-Dash  â†’ UI (port 5601)                       â”‚
â”‚  â”œâ”€ Prometheus       â†’ Metrics Collector (port 9090)        â”‚
â”‚  â””â”€ Node-Exporter    â†’ System Metrics (port 9100)           â”‚
â”‚                                                              â”‚
â”‚  Application Layer (1 container)                            â”‚
â”‚  â””â”€ Streamlit        â†’ Unstructured Data Explorer (8501)    â”‚
â”‚                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                   Existing Infrastructure                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Storage Layer (4 containers)                               â”‚
â”‚  â”œâ”€ SeaweedFS-Master â†’ Cluster Coordinator                  â”‚
â”‚  â”œâ”€ SeaweedFS-Volume â†’ Data Storage                         â”‚
â”‚  â”œâ”€ SeaweedFS-Filer  â†’ File System Interface                â”‚
â”‚  â””â”€ SeaweedFS-S3     â†’ S3 Gateway (port 8333)               â”‚
â”‚                                                              â”‚
â”‚  Metadata Layer (2 containers)                              â”‚
â”‚  â”œâ”€ Postgres         â†’ Hive Metastore DB                    â”‚
â”‚  â””â”€ Hive-Metastore   â†’ Metadata Service (port 9083)         â”‚
â”‚                                                              â”‚
â”‚  Query Layer (2 containers)                                 â”‚
â”‚  â”œâ”€ Trino            â†’ Distributed SQL Engine (port 8080)   â”‚
â”‚  â””â”€ Spark-Iceberg    â†’ Compute + Jupyter (port 8888)        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         Total: 16 Microservices in lakehouse-net
```

### êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì´í•©

- **Superset**: 25ê°œ í•­ëª©
- **Grafana + OpenSearch**: 20ê°œ í•­ëª©
- **Streamlit**: 15ê°œ í•­ëª©
- **í†µí•© í…ŒìŠ¤íŠ¸**: 10ê°œ í•­ëª©
- **ì´ 70ê°œ ì²´í¬ë¦¬ìŠ¤íŠ¸**

### ì˜ˆìƒ êµ¬í˜„ ì¼ì •

| ë‹¨ê³„ | ì‘ì—… | ì†Œìš” ì‹œê°„ |
|------|------|----------|
| **Day 1** | Superset + Trino ì„¤ì •, ì´ˆê¸° ëŒ€ì‹œë³´ë“œ 1ê°œ | 1ì¼ |
| **Day 2** | Superset ëŒ€ì‹œë³´ë“œ ì™„ì„±, RBAC ì„¤ì • | 1ì¼ |
| **Day 3** | Grafana + OpenSearch ì„¤ì •, ìƒ˜í”Œ ëŒ€ì‹œë³´ë“œ | 1ì¼ |
| **Day 4** | Streamlit ì•± ê°œë°œ ë° í…ŒìŠ¤íŠ¸ | 0.5ì¼ |
| **Day 5** | í†µí•© í…ŒìŠ¤íŠ¸ ë° ì„±ëŠ¥ íŠœë‹ | 1ì¼ |
| **Day 6-7** | ë¬¸ì„œí™”, íŠ¸ëŸ¬ë¸”ìŠˆíŒ…, ë°±ì—… ìŠ¤í¬ë¦½íŠ¸ | 1.5ì¼ |
| **ì´í•©** | **Production-Ready Stack** | **6ì¼** |

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„ (Next Steps)

ì´ ë¬¸ì„œë¥¼ ì™„ë£Œí–ˆë‹¤ë©´:

1. **ì¦‰ì‹œ ì‹œì‘**: Section 7ì˜ YAMLì„ `docker-compose.yml`ì— ì¶”ê°€í•˜ê³  `docker-compose up -d` ì‹¤í–‰
2. **ì²´í¬ë¦¬ìŠ¤íŠ¸ í™œìš©**: Section 6ì˜ 70ê°œ í•­ëª©ì„ í•˜ë‚˜ì”© ì²´í¬í•˜ë©° ì§„í–‰
3. **ë¬¸ì œ ë°œìƒ ì‹œ**: Section 8.2 íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ ì°¸ì¡°
4. **ìš´ì˜ ìë™í™”**: Section 8.4 ë°±ì—… ìŠ¤í¬ë¦½íŠ¸ë¥¼ cronì— ë“±ë¡

**ì¶•í•˜í•©ë‹ˆë‹¤!** ì´ì œ í˜„ì—… í‘œì¤€ ìˆ˜ì¤€ì˜ **Data Lakehouse ì‹œê°í™” ìŠ¤íƒ**ì„ êµ¬ì¶•í•  ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ğŸ‰